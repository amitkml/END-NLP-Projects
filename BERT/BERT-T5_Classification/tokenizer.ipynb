{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCt0qj7iiHxn"
      },
      "source": [
        "# State-of-the-art Tokenizer Algorithms\n",
        "\n",
        "A comprehensive guide to understanding state-of-the-art tokenization algorithms. \n",
        "\n",
        "NLP systems have three main components that help machines understand natural language:\n",
        "1. Tokenization\n",
        "2. Embedding\n",
        "3. Model architectures\n",
        "\n",
        "Top Deep Learning models like BERT, GPT-2 or GPT-3 all share the same components but with different architectures that distinguishes one model from another.\n",
        "\n",
        "In this notebook, we are going to focus on the first component of an NLP pipeline which is tokenization. An often overlooked concept but it is a field of research in itself. We have come so far ahead of the traditional NLTK tokenization process.\n",
        "\n",
        "Though we have SOTA algorithms for tokenization but it's always a good practice to understand the evolution trail. How have we reached here.\n",
        "\n",
        "Here's what we'll cover:\n",
        "\n",
        "* What is tokenization?\n",
        "* Why do we need a tokenizer?\n",
        "* Types of tokenization - Word, Character and Subword.\n",
        "* Byte Pair Encoding Algorithm\n",
        "* Unigram Algorithm\n",
        "* WordPiece - BERT transformer\n",
        "* SentencePiece - End-to-End tokenizer system\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdyPE1DFCZN5"
      },
      "source": [
        "## What is Tokenization?\n",
        "\n",
        "Tokenization is the process of representing raw text in smaller units called tokens. These tokens can then be mapped with numbers to further feed to an NLP model.\n",
        "\n",
        "Here's an overly simplified example of what a tokenizer does:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcxbZxrah08u"
      },
      "source": [
        "## read the text and enumerate the tokens in the text\n",
        "text = open('example.txt', 'r').read(). # read a text file\n",
        "words = text.split(\" \")  # split the text on spaces\n",
        "tokens = {v: k  for k, v in enumerate(words)}  # generate a word to index mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTO-oF9MiELZ",
        "outputId": "7bffa6bd-8a45-4a20-a6b9-c86dc95315e7"
      },
      "source": [
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The': 0,\n",
              " 'We': 5,\n",
              " 'and': 13,\n",
              " 'get': 10,\n",
              " 'go': 7,\n",
              " 'is': 2,\n",
              " 'out': 8,\n",
              " 'relax.': 14,\n",
              " 'should': 6,\n",
              " 'some': 11,\n",
              " 'sun': 12,\n",
              " 'sunny': 3,\n",
              " 'to': 9,\n",
              " 'today.': 4,\n",
              " 'weather': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCjU_hNJEuj4"
      },
      "source": [
        "> Here, we have simply mapped every word in the text to a numerical index. Obviously, this is a very simple example and we have not considered grammar, punctuations, compound words(like test, test-ify, test-ing, etc.). \n",
        "\n",
        "Thus, we need a more technical and accurate definition of tokenization. To take into account every punctuation and related words, we need to start working at character level. \n",
        "\n",
        "There are multiple applications of tokenization. One of the use cases comes from compiler design where we need to parse computer programs to convert raw characters into keywords of a programming language.\n",
        "\n",
        "**In deep learning,** tokenization is the process of converting a sequence of characters into a sequence of tokens which further needs to be converted into a sequence of numerical vectors which can be processed by a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Htq3AZKxaY"
      },
      "source": [
        "## Why do we need a Tokenizer?\n",
        "\n",
        "The need for a tokenizer has protruded from the question \"How can we make machines read?\"\n",
        "\n",
        "One of the common ways of processing textual data is by defining a set of rules in a dictionary and then looking up that fixed dictionary of rules. But this method can only go so far and we want machines to learn these rules from the text that is read by it.\n",
        "\n",
        "Now, machines don't know any language, nor do they understand sound or phonetics. They need to be taught from scratch and in such a way that they could read any language possible.\n",
        "\n",
        "Quite a task, right?\n",
        "\n",
        "Humans learn a language by connecting sound to the meaning and then we learn to read and write in that language. Machines can't do that, so they need to be given the most basic units of text to start processing it.\n",
        "\n",
        "That's where tokenization comes into play. Break down the text into smaller units called \"tokens\".\n",
        "\n",
        "And there are different ways of tokenizing text which is what we'll learn now.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zb9iLeyVN_u"
      },
      "source": [
        "## Types of Tokenization\n",
        "\n",
        "To make the deep learning model learn from the text, we need a two-step process:\n",
        "1. Tokenize - decide the algorithm to be used to generate the tokens.\n",
        "2. Encode the tokens to vectors\n",
        "\n",
        "As the first step suggests, we need to decide how to convert text into small tokens. A simple and straight forward method that most of us would propose is the word based tokens, splitting the text by space.\n",
        "\n",
        "### Problems with Word tokenizer\n",
        "* **the risk of missing words in the training data:** with word tokens, your model won't recognize the variants of words that were not part of the data on which the model was trained. So, if your model has seen `foot` and `ball` in the training data but the final text has `football`, the model won't be able to recognise the word and it will be treated with `<UNK>` token. Similarly, punctuations pose another problem, `let` or `let's` will need individual tokens and it is an inefficient solution. This will **require a huge vocabulary** to make sure you've every variant of the word. Even if you add a **lemmatizer** to solve this problem, you're adding an extra step in your processing pipeline.\n",
        "\n",
        "* **Handling slang and abbreviations-** another problem is the use of slang and abbreviations in texts these days such as \"FOMO\", \"LOL\", \"tl;dr\" etc. What do we do for these words?\n",
        "\n",
        "* **What if the language doesn't use space for segmentation:** for a language like Chinese, which doesn't use spaces for word separation, this tokenizer will fail completely.\n",
        "\n",
        "After encountering these problems, researchers looked into another approach which was tokenizing all the characters.\n",
        "\n",
        "## Character-based tokenization\n",
        "\n",
        "To resolve the problems associated with word-based tokenization, an alternative approach of character-by-character tokenization was tried.\n",
        "\n",
        "This did solve the problem of missing words as now we are dealing with characters which can be encoded using ASCII or Unicode and it could generate embedding for any word now. \n",
        "\n",
        "Every character, be it space or apostrophes or colons, can now be assigned a symbol to generate a sequence of vectors.\n",
        "\n",
        "But this approach had its own cons.\n",
        "\n",
        "### Drawbacks of character-based models\n",
        "\n",
        "* **Requirement of more compute:** character-based models will treat each character as tokens and more number of tokens mean more input computations to process each token which in turn requires more compute resources. For a 5-word long sentence, you may need to process 30 tokens instead of 5 word tokens. \n",
        "\n",
        "* **Narrows down the number of NLP tasks and applications:** with long sequences of characters, only a certain type of neural network architectures can be used. This puts a limitation on the type of NLP tasks we can perform. For applications like Entity recognition or text classification, character-based encoding might turn out to be an inefficient approach.\n",
        "\n",
        "* **Risk of learning incorrect semantics:** working with characters could generate incorrect spellings of words. Also, with no inherent meaning, learning with characters is like learning with no meaningful semantics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-2uQgCOztqx"
      },
      "source": [
        "> What's fascinating is that for such a seemingly simple task, mutliple algorithms are written to find the optimal tokenization policy.\n",
        "\n",
        "After understanding the pros and cons of these tokenization methods, it makes sense to look for an approach that offers a middle route.\n",
        "\n",
        "## Subword Tokenizaiton\n",
        "\n",
        "With character-based models, we risk losing the semantic features of the word and with word-based tokenization, we need a very large vocabulary to encompass all the possible variations of every word.\n",
        "\n",
        "So, the goal was to develop an algorithm that could:\n",
        "1. Retain the semantic features of the token i.e. information per token.\n",
        "2. tokenize without demanding a very large vocaublary with a finite set of words.\n",
        "\n",
        "To solve this problem, we could think of breaking down the words based on a set of prefixes and suffixes. For example, we can write a rule-based system to identify subwords like `\"##s\"`, `\"##ing\"`, `\"##ify\"`, `\"un##\"` etc., where the position of double hash denotes prefix and suffixes.\n",
        "\n",
        "So, a word like `\"unhappily\"` is tokenzied using subwords like `\"un##\"`, `\"happ\"`, and `\"##ily\"`.\n",
        "\n",
        "The model only learns a few subwords and then puts them together to create opther words. This solves the problem of memory requirement and effort to create a large vocabulary.\n",
        "\n",
        "### Problems with this algorithm:\n",
        "\n",
        "* Some of the subwords that are created as per the defined rules may never appear in your text to tokenize and may end up occupying extra memory.\n",
        "\n",
        "* Also, for every language, we'll need to define different set of rules to create subwords.\n",
        "\n",
        "To alleviate this problem, in practice, most modern tokenizers have a training phase that identifies the recurring text in the input corpus and create new subwords tokens. For rare patterns, we stick to word-based tokens.\n",
        "\n",
        "Another important factor that plays a vital role in this process is the size of the vocabulary that is set by the user. Large vocabulary size allows for more common words to be tokenized whereas smaller vocabulary requires more subwords to be created to create every word in the text without using the `<UNK>` token.\n",
        "\n",
        "Striking the balance for your application is key here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh2rLpsiHnE8"
      },
      "source": [
        "## Byte Pair Encoding(BPE)\n",
        "\n",
        "BPE was originally a data compression algorithm which is used to find the best way to represent data by identifying the common byte pairs. It is now used in NLP to find the best representation of text using the least number of tokens.\n",
        "\n",
        "Here's how it works:\n",
        "1. Add a </w> at the end of each word to identify the end of a word and then calculate the word frequency in the text.\n",
        "2. Split the word into characters and then calculate the character frequency.\n",
        "3. From the character tokens, for a predefined number of iterations, count the frequency of the consecutive byte pairs and merge the most frequently occurring byte pairing.\n",
        "4. Keep iterating until you have reached the iteration limit(set by you) or if you have reached the token limit.\n",
        "\n",
        "Let's go through each step(in code) for a sample text. For coding this, I have taken help from [Lei Mao's very minimalistic blog on BPE](https://leimao.github.io/blog/Byte-Pair-Encoding/). I encourage you to check it out!\n",
        "\n",
        "Here's our sample text:\n",
        "\n",
        "#### `\"There is an 80% chance of rainfall today. We are pretty sure it is going to rain.\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsPeIYQlRjjs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2438b4-f66a-4a2b-f7a3-c7d4abafdd9d"
      },
      "source": [
        "## define the text first\n",
        "text = \"There is an 80% chance of rainfall today. We are pretty sure it is going to rain.\"\n",
        "\n",
        "## get the word frequency and add the end of word (</w>) token at the end of each word\n",
        "words = text.strip().split(\" \")\n",
        "print(f\"Vocabulary size: {len(words)}\")\n",
        "words\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 17\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There',\n",
              " 'is',\n",
              " 'an',\n",
              " '80%',\n",
              " 'chance',\n",
              " 'of',\n",
              " 'rainfall',\n",
              " 'today.',\n",
              " 'We',\n",
              " 'are',\n",
              " 'pretty',\n",
              " 'sure',\n",
              " 'it',\n",
              " 'is',\n",
              " 'going',\n",
              " 'to',\n",
              " 'rain.']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnjv2FLnX3rr",
        "outputId": "3e559c42-b9b3-4045-b5cb-acb95e867304"
      },
      "source": [
        "import collections\n",
        "word_freq_dict = collections.defaultdict(int)\n",
        "\n",
        "for word in words:\n",
        "    word_freq_dict[' '.join(word) + ' </w>'] += 1\n",
        "\n",
        "word_freq_dict "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'8 0 % </w>': 1,\n",
              "             'T h e r e </w>': 1,\n",
              "             'W e </w>': 1,\n",
              "             'a n </w>': 1,\n",
              "             'a r e </w>': 1,\n",
              "             'c h a n c e </w>': 1,\n",
              "             'g o i n g </w>': 1,\n",
              "             'i s </w>': 2,\n",
              "             'i t </w>': 1,\n",
              "             'o f </w>': 1,\n",
              "             'p r e t t y </w>': 1,\n",
              "             'r a i n . </w>': 1,\n",
              "             'r a i n f a l l </w>': 1,\n",
              "             's u r e </w>': 1,\n",
              "             't o </w>': 1,\n",
              "             't o d a y . </w>': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GETjH-BCyKhJ"
      },
      "source": [
        "## Step 2: Split the word into characters and then calculate the character frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD-tR1FCwQAm",
        "outputId": "7543ada4-259c-4cb5-a25d-64951f5d7613"
      },
      "source": [
        "char_freq_dict = collections.defaultdict(int)\n",
        "for word, freq in word_freq_dict.items():\n",
        "    chars = word.split()\n",
        "    for char in chars:\n",
        "        char_freq_dict[char] += freq\n",
        "\n",
        "char_freq_dict\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'%': 1,\n",
              "             '.': 2,\n",
              "             '0': 1,\n",
              "             '8': 1,\n",
              "             '</w>': 17,\n",
              "             'T': 1,\n",
              "             'W': 1,\n",
              "             'a': 7,\n",
              "             'c': 2,\n",
              "             'd': 1,\n",
              "             'e': 7,\n",
              "             'f': 2,\n",
              "             'g': 2,\n",
              "             'h': 2,\n",
              "             'i': 6,\n",
              "             'l': 2,\n",
              "             'n': 5,\n",
              "             'o': 4,\n",
              "             'p': 1,\n",
              "             'r': 6,\n",
              "             's': 3,\n",
              "             't': 5,\n",
              "             'u': 1,\n",
              "             'y': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "dmm7iQveYQrt",
        "outputId": "5f244806-5886-45e1-8926-d0b4d53fc437"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(char_freq_dict, index=[0]).T\n",
        "df = df.rename(columns={0: 'Count'})\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>h</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>e</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>r</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>&lt;/w&gt;</th>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>%</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>o</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>l</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>t</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>p</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>u</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>g</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Count\n",
              "T         1\n",
              "h         2\n",
              "e         7\n",
              "r         6\n",
              "</w>     17\n",
              "i         6\n",
              "s         3\n",
              "a         7\n",
              "n         5\n",
              "8         1\n",
              "0         1\n",
              "%         1\n",
              "c         2\n",
              "o         4\n",
              "f         2\n",
              "l         2\n",
              "t         5\n",
              "d         1\n",
              "y         2\n",
              ".         2\n",
              "W         1\n",
              "p         1\n",
              "u         1\n",
              "g         2"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n4jfWpI8Z6e"
      },
      "source": [
        "## Step 3: Merge the most frequently occurring consecutive byte pairing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7rGNuyA7rki",
        "outputId": "bda76876-5a2d-44e4-82ff-b66627fde784"
      },
      "source": [
        "import re\n",
        "\n",
        "## create all possible consecutive pairs\n",
        "pairs = collections.defaultdict(int)\n",
        "for word, freq in word_freq_dict.items():\n",
        "    chars = word.split()\n",
        "    for i in range(len(chars)-1):\n",
        "        pairs[chars[i], chars[i+1]] += freq\n",
        "\n",
        "pairs\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('%', '</w>'): 1,\n",
              "             ('.', '</w>'): 2,\n",
              "             ('0', '%'): 1,\n",
              "             ('8', '0'): 1,\n",
              "             ('T', 'h'): 1,\n",
              "             ('W', 'e'): 1,\n",
              "             ('a', 'i'): 2,\n",
              "             ('a', 'l'): 1,\n",
              "             ('a', 'n'): 2,\n",
              "             ('a', 'r'): 1,\n",
              "             ('a', 'y'): 1,\n",
              "             ('c', 'e'): 1,\n",
              "             ('c', 'h'): 1,\n",
              "             ('d', 'a'): 1,\n",
              "             ('e', '</w>'): 5,\n",
              "             ('e', 'r'): 1,\n",
              "             ('e', 't'): 1,\n",
              "             ('f', '</w>'): 1,\n",
              "             ('f', 'a'): 1,\n",
              "             ('g', '</w>'): 1,\n",
              "             ('g', 'o'): 1,\n",
              "             ('h', 'a'): 1,\n",
              "             ('h', 'e'): 1,\n",
              "             ('i', 'n'): 3,\n",
              "             ('i', 's'): 2,\n",
              "             ('i', 't'): 1,\n",
              "             ('l', '</w>'): 1,\n",
              "             ('l', 'l'): 1,\n",
              "             ('n', '.'): 1,\n",
              "             ('n', '</w>'): 1,\n",
              "             ('n', 'c'): 1,\n",
              "             ('n', 'f'): 1,\n",
              "             ('n', 'g'): 1,\n",
              "             ('o', '</w>'): 1,\n",
              "             ('o', 'd'): 1,\n",
              "             ('o', 'f'): 1,\n",
              "             ('o', 'i'): 1,\n",
              "             ('p', 'r'): 1,\n",
              "             ('r', 'a'): 2,\n",
              "             ('r', 'e'): 4,\n",
              "             ('s', '</w>'): 2,\n",
              "             ('s', 'u'): 1,\n",
              "             ('t', '</w>'): 1,\n",
              "             ('t', 'o'): 2,\n",
              "             ('t', 't'): 1,\n",
              "             ('t', 'y'): 1,\n",
              "             ('u', 'r'): 1,\n",
              "             ('y', '.'): 1,\n",
              "             ('y', '</w>'): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unub0kgGbfW3",
        "outputId": "9a7bb5aa-97f7-48fe-c787-636fc2d07b12"
      },
      "source": [
        "max(pairs, key=pairs.get)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', '</w>')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIjJTbCnrKfK"
      },
      "source": [
        "word_freq_dict = collections.defaultdict(int)\n",
        "\n",
        "for word in words:\n",
        "    word_freq_dict[' '.join(word) + ' </w>'] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBmnzQ68ysW9"
      },
      "source": [
        "## Step 4 - Iterate a number of times to find the best(in terms of frequency) pairs to encode and then concatenate them to find the subwords.\n",
        "\n",
        "It is better at this point to turn structure our code into functions. This will require us to perform the following steps:\n",
        "\n",
        "1. Find the most frequently occurring byte pairs in each iteration.\n",
        "2. Merge these tokens. \n",
        "3. Recalculate the character tokens frequency with the new pair encoding added.\n",
        "3. Keep doing it until there is no more pair or you reach the end of the for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-9mvoOhoU_sz",
        "outputId": "7dd117c4-6b72-49ab-a16a-4327c21da237"
      },
      "source": [
        "##find the best pair\n",
        "\n",
        "def get_pairs(word_freq_dict):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in word_freq_dict.items():\n",
        "        chars = word.split()\n",
        "        for i in range(len(chars)-1):\n",
        "            pairs[chars[i], chars[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_byte_pairs(best_pair, word_freq_dict):\n",
        "    print(best_pair)\n",
        "    merged_dict = {}\n",
        "    bigram = re.escape(' '.join(best_pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in word_freq_dict:\n",
        "        # print(word)\n",
        "        w_out = p.sub(''.join(best_pair), word)\n",
        "        merged_dict[w_out] = word_freq_dict[word]\n",
        "    return merged_dict\n",
        "\n",
        "def get_subword_tokens(word_freq_dict):\n",
        "    char_freq_dict = collections.defaultdict(int)\n",
        "    for word, freq in word_freq_dict.items():\n",
        "        chars = word.split()\n",
        "        for char in chars:\n",
        "            char_freq_dict[char] += freq\n",
        "    return char_freq_dict\n",
        "\n",
        "for i in range(100):\n",
        "    pairs = get_pairs(word_freq_dict)\n",
        "    best_pair = max(pairs, key=pairs.get)\n",
        "    print(f\"Iteration {i}: \")\n",
        "    word_freq_dict = merge_byte_pairs(best_pair, word_freq_dict)\n",
        "    # print(word_freq_dict)\n",
        "    subword_tokens = get_subword_tokens(word_freq_dict)\n",
        "    print(subword_tokens)\n",
        "    print(len(subword_tokens))\n",
        "    print(\"--------\")\n",
        "\n",
        "\n",
        "merged_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: \n",
            "('e', '</w>')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 'r': 6, 'e</w>': 5, 'i': 6, 's': 3, '</w>': 12, 'a': 7, 'n': 5, '8': 1, '0': 1, '%': 1, 'c': 2, 'o': 4, 'f': 2, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 'u': 1, 'g': 2})\n",
            "25\n",
            "--------\n",
            "Iteration 1: \n",
            "('r', 'e</w>')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'i': 6, 's': 3, '</w>': 12, 'a': 7, 'n': 5, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 4, 'f': 2, 'r': 3, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 'u': 1, 'g': 2})\n",
            "26\n",
            "--------\n",
            "Iteration 2: \n",
            "('i', 'n')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'i': 3, 's': 3, '</w>': 12, 'a': 7, 'n': 2, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 4, 'f': 2, 'r': 3, 'in': 3, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 'u': 1, 'g': 2})\n",
            "27\n",
            "--------\n",
            "Iteration 3: \n",
            "('i', 's')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'is': 2, '</w>': 12, 'a': 7, 'n': 2, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 4, 'f': 2, 'r': 3, 'in': 3, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 's': 1, 'u': 1, 'i': 1, 'g': 2})\n",
            "28\n",
            "--------\n",
            "Iteration 4: \n",
            "('is', '</w>')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'is</w>': 2, 'a': 7, 'n': 2, '</w>': 10, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 4, 'f': 2, 'r': 3, 'in': 3, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 's': 1, 'u': 1, 'i': 1, 'g': 2})\n",
            "28\n",
            "--------\n",
            "Iteration 5: \n",
            "('a', 'n')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'is</w>': 2, 'an': 2, '</w>': 10, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 4, 'f': 2, 'r': 3, 'a': 5, 'in': 3, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 's': 1, 'u': 1, 'i': 1, 'g': 2})\n",
            "28\n",
            "--------\n",
            "Iteration 6: \n",
            "('r', 'a')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'is</w>': 2, 'an': 2, '</w>': 10, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 4, 'f': 2, 'ra': 2, 'in': 3, 'a': 3, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 'r': 1, 's': 1, 'u': 1, 'i': 1, 'g': 2})\n",
            "29\n",
            "--------\n",
            "Iteration 7: \n",
            "('ra', 'in')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'is</w>': 2, 'an': 2, '</w>': 10, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 4, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 't': 5, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 'r': 1, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "29\n",
            "--------\n",
            "Iteration 8: \n",
            "('t', 'o')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'is</w>': 2, 'an': 2, '</w>': 10, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.': 2, 'W': 1, 'p': 1, 'r': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "30\n",
            "--------\n",
            "Iteration 9: \n",
            "('.', '</w>')\n",
            "defaultdict(<class 'int'>, {'T': 1, 'h': 2, 'e': 2, 're</w>': 3, 'is</w>': 2, 'an': 2, '</w>': 8, '8': 1, '0': 1, '%': 1, 'c': 2, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 'p': 1, 'r': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "30\n",
            "--------\n",
            "Iteration 10: \n",
            "('T', 'h')\n",
            "defaultdict(<class 'int'>, {'Th': 1, 'e': 2, 're</w>': 3, 'is</w>': 2, 'an': 2, '</w>': 8, '8': 1, '0': 1, '%': 1, 'c': 2, 'h': 1, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 'p': 1, 'r': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "30\n",
            "--------\n",
            "Iteration 11: \n",
            "('Th', 'e')\n",
            "defaultdict(<class 'int'>, {'The': 1, 're</w>': 3, 'is</w>': 2, 'an': 2, '</w>': 8, '8': 1, '0': 1, '%': 1, 'c': 2, 'h': 1, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "30\n",
            "--------\n",
            "Iteration 12: \n",
            "('The', 're</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an': 2, '</w>': 8, '8': 1, '0': 1, '%': 1, 'c': 2, 'h': 1, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "30\n",
            "--------\n",
            "Iteration 13: \n",
            "('an', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '8': 1, '0': 1, '%': 1, '</w>': 7, 'c': 2, 'h': 1, 'an': 1, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "31\n",
            "--------\n",
            "Iteration 14: \n",
            "('8', '0')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80': 1, '%': 1, '</w>': 7, 'c': 2, 'h': 1, 'an': 1, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "30\n",
            "--------\n",
            "Iteration 15: \n",
            "('80', '%')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%': 1, '</w>': 7, 'c': 2, 'h': 1, 'an': 1, 'e</w>': 2, 'o': 2, 'f': 2, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "29\n",
            "--------\n",
            "Iteration 16: \n",
            "('80%', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'c': 2, 'h': 1, 'an': 1, 'e</w>': 2, 'o': 2, 'f': 2, '</w>': 6, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "29\n",
            "--------\n",
            "Iteration 17: \n",
            "('c', 'h')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'ch': 1, 'an': 1, 'c': 1, 'e</w>': 2, 'o': 2, 'f': 2, '</w>': 6, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "29\n",
            "--------\n",
            "Iteration 18: \n",
            "('ch', 'an')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chan': 1, 'c': 1, 'e</w>': 2, 'o': 2, 'f': 2, '</w>': 6, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "28\n",
            "--------\n",
            "Iteration 19: \n",
            "('chan', 'c')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chanc': 1, 'e</w>': 2, 'o': 2, 'f': 2, '</w>': 6, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 20: \n",
            "('chanc', 'e</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'o': 2, 'f': 2, '</w>': 6, 'rain': 2, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'in': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 21: \n",
            "('o', 'f')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of': 1, '</w>': 6, 'rain': 2, 'f': 1, 'a': 3, 'l': 2, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1})\n",
            "28\n",
            "--------\n",
            "Iteration 22: \n",
            "('of', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rain': 2, 'f': 1, 'a': 3, 'l': 2, '</w>': 5, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1})\n",
            "28\n",
            "--------\n",
            "Iteration 23: \n",
            "('rain', 'f')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainf': 1, 'a': 3, 'l': 2, '</w>': 5, 'to': 2, 'd': 1, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'rain': 1})\n",
            "28\n",
            "--------\n",
            "Iteration 24: \n",
            "('rainf', 'a')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfa': 1, 'l': 2, '</w>': 5, 'to': 2, 'd': 1, 'a': 2, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'rain': 1})\n",
            "28\n",
            "--------\n",
            "Iteration 25: \n",
            "('rainfa', 'l')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfal': 1, 'l': 1, '</w>': 5, 'to': 2, 'd': 1, 'a': 2, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'rain': 1})\n",
            "28\n",
            "--------\n",
            "Iteration 26: \n",
            "('rainfal', 'l')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall': 1, '</w>': 5, 'to': 2, 'd': 1, 'a': 2, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'rain': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 27: \n",
            "('rainfall', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'to': 2, 'd': 1, 'a': 2, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, '</w>': 4, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'rain': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 28: \n",
            "('to', 'd')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'tod': 1, 'a': 2, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, '</w>': 4, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 29: \n",
            "('tod', 'a')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'toda': 1, 'y': 2, '.</w>': 2, 'W': 1, 'e</w>': 1, 'a': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, '</w>': 4, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 30: \n",
            "('toda', 'y')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today': 1, '.</w>': 2, 'W': 1, 'e</w>': 1, 'a': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 31: \n",
            "('today', '.</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'W': 1, 'e</w>': 1, 'a': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "27\n",
            "--------\n",
            "Iteration 32: \n",
            "('W', 'e</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'a': 1, 're</w>': 2, 'p': 1, 'r': 1, 'e': 1, 't': 3, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "26\n",
            "--------\n",
            "Iteration 33: \n",
            "('a', 're</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'p': 1, 'r': 1, 'e': 1, 't': 3, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 're</w>': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "26\n",
            "--------\n",
            "Iteration 34: \n",
            "('p', 'r')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pr': 1, 'e': 1, 't': 3, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 're</w>': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "25\n",
            "--------\n",
            "Iteration 35: \n",
            "('pr', 'e')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pre': 1, 't': 3, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 're</w>': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "24\n",
            "--------\n",
            "Iteration 36: \n",
            "('pre', 't')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pret': 1, 't': 2, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 're</w>': 1, 'i': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "24\n",
            "--------\n",
            "Iteration 37: \n",
            "('pret', 't')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'prett': 1, 'y': 1, '</w>': 4, 's': 1, 'u': 1, 're</w>': 1, 'i': 1, 't': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "24\n",
            "--------\n",
            "Iteration 38: \n",
            "('prett', 'y')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty': 1, '</w>': 4, 's': 1, 'u': 1, 're</w>': 1, 'i': 1, 't': 1, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "23\n",
            "--------\n",
            "Iteration 39: \n",
            "('pretty', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 's': 1, 'u': 1, 're</w>': 1, 'i': 1, 't': 1, '</w>': 3, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "23\n",
            "--------\n",
            "Iteration 40: \n",
            "('s', 'u')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'su': 1, 're</w>': 1, 'i': 1, 't': 1, '</w>': 3, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "22\n",
            "--------\n",
            "Iteration 41: \n",
            "('su', 're</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'i': 1, 't': 1, '</w>': 3, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "21\n",
            "--------\n",
            "Iteration 42: \n",
            "('i', 't')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it': 1, '</w>': 3, 'g': 2, 'o': 1, 'in': 1, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "20\n",
            "--------\n",
            "Iteration 43: \n",
            "('it', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it</w>': 1, 'g': 2, 'o': 1, 'in': 1, '</w>': 2, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "20\n",
            "--------\n",
            "Iteration 44: \n",
            "('g', 'o')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it</w>': 1, 'go': 1, 'in': 1, 'g': 1, '</w>': 2, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "20\n",
            "--------\n",
            "Iteration 45: \n",
            "('go', 'in')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it</w>': 1, 'goin': 1, 'g': 1, '</w>': 2, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "19\n",
            "--------\n",
            "Iteration 46: \n",
            "('goin', 'g')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it</w>': 1, 'going': 1, '</w>': 2, 'to': 1, 'rain': 1, '.</w>': 1})\n",
            "18\n",
            "--------\n",
            "Iteration 47: \n",
            "('going', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it</w>': 1, 'going</w>': 1, 'to': 1, '</w>': 1, 'rain': 1, '.</w>': 1})\n",
            "18\n",
            "--------\n",
            "Iteration 48: \n",
            "('to', '</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it</w>': 1, 'going</w>': 1, 'to</w>': 1, 'rain': 1, '.</w>': 1})\n",
            "17\n",
            "--------\n",
            "Iteration 49: \n",
            "('rain', '.</w>')\n",
            "defaultdict(<class 'int'>, {'There</w>': 1, 'is</w>': 2, 'an</w>': 1, '80%</w>': 1, 'chance</w>': 1, 'of</w>': 1, 'rainfall</w>': 1, 'today.</w>': 1, 'We</w>': 1, 'are</w>': 1, 'pretty</w>': 1, 'sure</w>': 1, 'it</w>': 1, 'going</w>': 1, 'to</w>': 1, 'rain.</w>': 1})\n",
            "16\n",
            "--------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-7137461ab5f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_freq_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mbest_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {i}: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mword_freq_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_byte_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSBN2fgRymQr"
      },
      "source": [
        "So as we iterate with each best pair, we merge(concatenating) the pair and you can see as we recalculate the frequency, the original character token frequency is reduced and the new paired token frequency pops up in the token dictionary.\n",
        "\n",
        "If you look at the number of tokens created, it first increases because we create new pairings but the number starts to decrease after a number of iterations.\n",
        "\n",
        "Here, we started from 25 tokens, went up till 31 token in the 14th iteration and then came down to 16 tokens in the 50th iteration. Interesting, right?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDabAkjd3Yje"
      },
      "source": [
        "There are pros and cons of the BPE algorithm too.\n",
        "\n",
        "The final tokens will vary depending upon the number of iterations you have run which also causes another problem. We now can have different tokens for a single text and thus different embeddings.\n",
        "\n",
        "To address this issue, multiple solutions were proposed but the one that stood out was a unigram language model that added subword regularization(a new method of subword segmentation) training that calculates the probability for each subword token to choose the best option using a loss function. More on this in the upcoming blogs.\n",
        "\n",
        "## Do they use BPE or the enhanced(unigram) BPE in BERT or GPTs?\n",
        "\n",
        "What you've learned(if at all) in this blog is the main problem and the approach to solving tokenization problems. \n",
        "\n",
        "Models like BERT or GPT-2 use some version of the BPE or the unigram model to tokenize the input text. \n",
        "\n",
        "BERT included a new algorithm called WordPiece which is also similar to the BPE but has an added layer of likelihood calculation to decide whether the merged token will make the final cut.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lZdqsN9X0zc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
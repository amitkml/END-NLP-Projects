{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment Analysis using SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4BKy4zcZ/u7+x/ScYuANY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ljVmH4TqfkZs"},"source":["# SVM based Sentiment Analysis\n","Let's perform a SVM based Sentiment Analysis based on Support a Vector Machine Model on Twitter Sentiments of US Airline passengers.\n","\n","**Fill in the blanks**"]},{"cell_type":"code","metadata":{"id":"tF_pfwune6tY"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vcu9OCGWf8Kr"},"source":["## Import Libraries"]},{"cell_type":"code","metadata":{"id":"xJQkq7NJft9P"},"source":["import numpy as np\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","from nltk.tokenize import TweetTokenizer\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","from sklearn.pipeline import make_pipeline, Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer, accuracy_score, f1_score\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUojZ_-Ef0uO"},"source":["## Import data\n"]},{"cell_type":"code","metadata":{"id":"JX3uxVzUf1bw"},"source":["data = pd.read_csv(\"Tweets_Airline.csv\")\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BZpfoYfgDkQ"},"source":["## We take only the tweets we are very confident with. We use the BeautifulSoup library to process html encoding present in some tweets."]},{"cell_type":"code","metadata":{"id":"LwFYL4JYgET1"},"source":["data_clean = data.copy()\n","data_clean = data_clean[data_clean['airline_sentiment_confidence'] > 0.65]\n","data_clean['text_clean'] = data_clean['text'].apply(lambda x: BeautifulSoup(x, \"lxml\").text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zNpo6rKtgIjz"},"source":["## For simplicity we are going to distinguish two cases: tweets with negative sentiment and tweets with non-negative sentiment"]},{"cell_type":"code","metadata":{"id":"AhzEtW7qgJFZ"},"source":["data_clean['sentiment'] = data_clean['airline_sentiment'].apply(--Fill--) #Hint: Assign 1 to negative class and 0 to rest\n","data_clean = data_clean.loc[:, ['text_clean', 'sentiment']]\n","data_clean.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"goWgpeISgVjb"},"source":["## We split the data into training and testing set:"]},{"cell_type":"code","metadata":{"id":"hb3xJkm1gWEV"},"source":["train, test = train_test_split(data_clean, test_size=0.2, random_state=1)\n","X_train = train['text_clean'].values\n","X_test = test['text_clean'].values\n","y_train = train['sentiment']\n","y_test = test['sentiment']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RUVhKrXJggFe"},"source":["## Preprocessing the Data"]},{"cell_type":"code","metadata":{"id":"bt-UPdxNgg1l"},"source":["def tokenize(text): \n","    tknzr = TweetTokenizer()\n","    return tknzr.tokenize(text)\n","\n","def stem(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))\n","\n","en_stopwords = set(stopwords.words(\"english\")) \n","\n","vectorizer = CountVectorizer(\n","    analyzer = 'word',\n","    tokenizer = tokenize,\n","    lowercase = True,\n","    ngram_range=(1, 1),\n","    stop_words = en_stopwords)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tIubNDdAglWc"},"source":["## We are going to use cross validation and grid search to find good hyperparameters for our SVM model. We need to build a pipeline."]},{"cell_type":"code","metadata":{"id":"10SpNhNlgmVm"},"source":["kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_GbUy4Vgrq3"},"source":["np.random.seed(1)\n","\n","pipeline_svm = make_pipeline(vectorizer, --Fill--(probability=True, \n","                                             kernel= --Fill--, \n","                                             class_weight= --Fill--)) #Hint : Linear kernel with balanced class weights\n","\n","grid_svm = GridSearchCV(pipeline_svm,\n","                    param_grid = {'svc__C': [0.01, 0.1, 1]}, \n","                    cv = kfolds,\n","                    scoring=\"roc_auc\",\n","                    verbose=1,   \n","                    n_jobs=-1) \n","\n","grid_svm.fit(X_train, y_train)\n","grid_svm.score(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NdSXvoPg0s-"},"source":["print(grid_svm.best_params_)\n","print(grid_svm.best_score_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2fifQEugzHr"},"source":["## Let's see how the model (with the best hyperparameters) works on the test data:"]},{"cell_type":"code","metadata":{"id":"j1vY0Ruag382"},"source":["def report_results(model, X, y):\n","    pred_proba = model.predict_proba(X)[:, 1]\n","    pred = model.predict(X)        \n","\n","    auc = roc_auc_score(y, pred_proba)\n","    acc = accuracy_score(y, pred)\n","    f1 = f1_score(y, pred)\n","    prec = precision_score(y, pred)\n","    rec = recall_score(y, pred)\n","    result = {'auc': auc, 'f1': f1, 'acc': acc, 'precision': prec, 'recall': rec}\n","    return result\n","\n","report_results(grid_svm.best_estimator_, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Wk71TaNg7qY"},"source":["## ROC Curve"]},{"cell_type":"code","metadata":{"id":"efe0yTp1g8aw"},"source":["def get_roc_curve(model, X, y):\n","    pred_proba = model.predict_proba(X)[:, 1]\n","    fpr, tpr, _ = roc_curve(y, pred_proba)\n","    return fpr, tpr\n","\n","fpr, tpr = get_roc_curve(grid_svm.best_estimator_, X_test, y_test)\n","plt.figure(figsize=(14,8))\n","plt.plot(fpr, tpr, color=\"red\")\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Roc curve')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cr8-f-N1hAQx"},"source":["## Prediction"]},{"cell_type":"code","metadata":{"id":"pVWXH80shBOb"},"source":["pred = grid_svm.predict([\"flying with @united is always a great experience.\"])\n","print('negative' if pred == np.array([1]) else 'not negative')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chGlA_QmhC9Q"},"source":["pred = grid_svm.predict([\"flying with @united is always a great experience. If you don't lose your luggage\"])\n","print('negative' if pred == np.array([1]) else 'not negative')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AA5lve07hJzf"},"source":["**It easily distinguishes the text based on context!!**"]},{"cell_type":"code","metadata":{"id":"PnnVO1LOhQA7"},"source":[""],"execution_count":null,"outputs":[]}]}
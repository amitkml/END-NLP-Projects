{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS Tagging based on Heuristics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitkml/END-NLP-Projects/blob/main/NLPBasic-Session1/POS_Tagging_based_on_Heuristics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM0Ka5OWjTtn"
      },
      "source": [
        "##Import necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUkH-XafjF18",
        "cellView": "both",
        "outputId": "02d4142c-e374-4828-df59-34ca6563fa07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv2hQOyGFV9_"
      },
      "source": [
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnfJWQfEjn89"
      },
      "source": [
        "## A sentence under consideration for Information Extraction (NER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nOjV-QMjlBF"
      },
      "source": [
        "sentence = 'Virat Kohli is an Indian cricketer who currently captains the India national team. A right-handed top-order batsman, Kohli is regarded as one of the best batsmen in the world'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwc64Wajjv95"
      },
      "source": [
        "## Apply word tokenization and part-of-speech tagging to the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mos0kBAIGUkN"
      },
      "source": [
        "**What is Tokenization?**\n",
        "\n",
        "- Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
        "- These tokens are very useful for finding such patterns as well as is considered as a base step for stemming and lemmatization.\n",
        "- *Tokenization of words*: word_tokenize() to split a sentence into words. The output of word tokenization can be converted to Data Frame for better text understanding in \n",
        "machine learning applications. It can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming. Machine learning models need numeric data to be trained and make a prediction. Word tokenization becomes a crucial part of the text (string) to numeric data conversion. Please read about Bag of Words or [CountVectorizer](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
        "- *Tokenization of Sentences*: why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both sentence tokenization as well as words to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrJPkc2MjtwW"
      },
      "source": [
        "def preprocess(sent):\n",
        "    sent =  sent_tokenize(sent) # TOKENIZE THE SENTENCE\n",
        "    sent = # GET POS TAG OF THE SENTENCE\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKv51GotjyZU"
      },
      "source": [
        "sent = preprocess(sentence)\n",
        "sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZfnCIBOlP32"
      },
      "source": [
        "## Plot a Parse Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SFMBOJhj0kD"
      },
      "source": [
        "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "NPChunker =  # Regex Parse using the pattern\n",
        "result = NPChunker.parse(sent)\n",
        "result.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzfWmM0s3qLQ"
      },
      "source": [
        "## POS Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lUv0-IX0V-v"
      },
      "source": [
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "iob_tagged = tree2conlltags(cs)\n",
        "pprint(iob_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsWTk0dX4DN8"
      },
      "source": [
        "**This is how Information is extracted using heuristics based techniques. Try using another pattern.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGPurTlb3tWd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END_NLP_Assignment_11_Convolutional Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A-H6cCBOLaf"
      },
      "source": [
        "# Convolutional Seq2Seq\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq0.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAw4EP9cOHSB"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PGBbGe9OTR0"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yehZL6jOf-8",
        "outputId": "417dc961-b761-4b12-dac6-0b270967dcd3"
      },
      "source": [
        "!python -m spacy download en\r\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=dd79aa6584656b8f9a9513bda8a34d011d43ba2482c011ae77a5a1f2902b3775\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-40_7w0cg/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2YHL1DPOVKD"
      },
      "source": [
        "spacy_de = spacy.load('de')\r\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdIpGUYDOWu1"
      },
      "source": [
        "def tokenize_de(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes German text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJvJxnt-AEyG"
      },
      "source": [
        "Following snippet has set up the Fields which decides how the data will be processed. By default RNN models in PyTorch require the sequence to be a tensor of shape [sequence length, batch size] so TorchText will, by default, return batches of tensors in the same shape. However in this notebook we are using CNNs which expect the batch dimension to be first. We tell TorchText to have batches be [batch size, sequence length] by setting batch_first = True.\r\n",
        "We also append the start and end of sequence tokens as well as lowercasing all text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6SkU5t4OoJy"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \r\n",
        "            init_token = '<sos>', \r\n",
        "            eos_token = '<eos>', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_en, \r\n",
        "            init_token = '<sos>', \r\n",
        "            eos_token = '<eos>', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOvVFsE-ATL6"
      },
      "source": [
        "Here, we are loading the dataset and then splitting them into source and target. Extension indicate language of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh30BF32Opfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dda673b-11b2-44ef-92b0-a60471939dea"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), \r\n",
        "                                                    fields=(SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 975kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 273kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 265kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjEVkit5AgNO"
      },
      "source": [
        "We build our vocabulary as before, by converting any tokens that appear less than 2 times. This is hyperparameter and as of now, we have assumed as 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQapMRRGOqrE"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\r\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RGcSVJ1Arqt"
      },
      "source": [
        "We are setting device here based on availability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IG349zyOsIG"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bNpbJFbAwUu"
      },
      "source": [
        "Following snippet, defines the itertator based on dataset that we have defined and also our batch size is being set to 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3ltm3BEOtgH"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "     batch_size = BATCH_SIZE,\r\n",
        "     device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7_dEuyGOwHO"
      },
      "source": [
        "## Encoder\r\n",
        "\r\n",
        "The convolutional sequence-to-sequence model is a little different - it gets two context vectors for each token in the input sentence. So, if our input sentence had 6 tokens (as per following example), we would get 12 context vectors, two for each token.\r\n",
        "\r\n",
        "### positional embedding\r\n",
        "Now, lets understand a bit why we need positional embedding. From what I understand, for each word to translate, the input contains both the word itself and its position in the input chain (say, 0, 1, ...m).\r\n",
        "\r\n",
        "Now, encoding such a data with simply having a cell with value pos (in 0..m) would not perform very well (for the same reason we use one-hot vectors to encode words). So, basically, the position will be encoded in a number of input cells, with one-hot representation (or similar, I might think of a binary representation of the position being used).\r\n",
        "\r\n",
        "Then, an embedding layer will be used (just as it is used for word encodings) to transform this sparse and discrete representation into a continuous one.\r\n",
        "\r\n",
        "The representation used in the paper chose to have the same dimension for the word embedding and the position embedding and to simply sum up the two.\r\n",
        "\r\n",
        "- First, the token is passed through a token embedding layer.\r\n",
        "- A second embedding layer where input is the position of the token within the sequence.\r\n",
        "\r\n",
        "- Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position with in the sequence.\r\n",
        "\r\n",
        "- We then have a linear layer which transforms the embedding vector into a vector with the required hidden dimension size.\r\n",
        "\r\n",
        "- The next step is to pass this hidden vector into $N$ convolutional blocks and I will explain what is $N$ convolutional blocks later below.\r\n",
        "\r\n",
        "- Output of $N$ convolutional blocks is then pass through a linear layer which transforms the input into embedding dimension size output. This output is generated per token as shared below and is being called conved output.\r\n",
        "\r\n",
        "- The conved vector is elementwise summed with the embedding vector via a residual connection to get a combined vector for each token. This is called combined vector for each token in the input sequence.\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq1.png)\r\n",
        "\r\n",
        "## Convolutional Blocks\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHOkTNoji1yN"
      },
      "source": [
        "### Encoder Class Implementation Details\r\n",
        "\r\n",
        "- Encoder is simple blocks of convolution operation(nn.Conv1d) followed by nonlinearity on fixed size of input.\r\n",
        "- Kernel size is odd her for symmetry and the validation is being added into Encoder\r\n",
        "- Have defined word embedding (tok_embedding) and positional embedding (pos_embedding)\r\n",
        "- Have used modulelist to stack n_layer no of nn.Conv1d. Here input to the conv1d is hidden layer dimension, padding is being set in a general way so that for kernel size of 3 the padding becomes 1 and for kernel size of 5 the padding becomes 2. **Note: output dimension is set as twice of hidden dimension because it needs to be passed to GLU which will halve the input.**\r\n",
        "- Then a dropout is being added here\r\n",
        "- Now lets understand forward function\r\n",
        "  - Get the batch size and source length\r\n",
        "  - We are creating a position tensor starting from 0 to source length -1 and then adding a new dimension by unsqeeze. Pushing this to cuda device. We ae using torch range command to get and then repeating this for batch_size to ensure position vector is available for all input and multiple batches.\r\n",
        "  - Source and Position input being passed through embedding\r\n",
        "  - adding token and positional embedding. Key point here is that both dimension needs to be same to be added\r\n",
        "  - we are then passing through dropout layer\r\n",
        "  - Now, conv_input is being generated by passing the concatenated embedding to the linear layer.\r\n",
        "  - Our required intout to the conv1d needs to be in [batch size, emb dim, src len] and so we have used conv_input.permute(0, 2, 1) which is further passed to conv1d\r\n",
        "  - Then we are running a for loop to extract convolution 1d layers from nn.modules and passing the conv_input after permute.\r\n",
        "    - Dropout being added into every conv1d layer as regulraizatiion technique.\r\n",
        "    - Output of dropout layer being passed to GLU. GLU can be explained by **Given a tensor, we do two independent convolutions and get two outputs. We further do an additional sigmoid activation for one of the outputs, and element-wise multiply the two outputs together**. Here input to the GLU is double of hidden is input and output will be same size of hidden. we are also passing dimension as 1 to indiate to GLU at which axis it will do\r\n",
        "    - we then add here the output of GLU and output of Linear layer output. **Scaling here is very critical to ensure values does not explode**.\r\n",
        "    - This output then becomes input to the next convolution 1d layer and we continue into the loop\r\n",
        "  \r\n",
        "  - We then further purmute baack final out from conv1d layer by conved.permute(0, 2, 1) to get into [batch size, src len, emb dim] which is the accepted format for linear FC layer. This is called **Convd** in the diagram.\r\n",
        "  - This output after permute is then passed through a fc layer self.hid2emb and then then sum with embedding layer output and this FL layer output which is called **combined** in the diagram. \r\n",
        "  - forward function then retuns **convd** and **combined** which are the output of encoder.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEjxwGUxOun-"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n",
        "        \r\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n",
        "                                              out_channels = 2 * hid_dim, \r\n",
        "                                              kernel_size = kernel_size, \r\n",
        "                                              padding = (kernel_size - 1) // 2)\r\n",
        "                                    for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        #create position tensor\r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [0, 1, 2, 3, ..., src len - 1]\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "        \r\n",
        "        #embed tokens and positions\r\n",
        "        tok_embedded = self.tok_embedding(src)\r\n",
        "        pos_embedded = self.pos_embedding(pos)\r\n",
        "        \r\n",
        "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #combine embeddings by elementwise summing\r\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\r\n",
        "        \r\n",
        "        #embedded = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        \r\n",
        "        #pass embedded through linear layer to convert from emb dim to hid dim\r\n",
        "        conv_input = self.emb2hid(embedded)\r\n",
        "        \r\n",
        "        #conv_input = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        # required conv_input = [batch size, emb dim, src len]\r\n",
        "\r\n",
        "        #permute for convolutional layer\r\n",
        "        conv_input = conv_input.permute(0, 2, 1) \r\n",
        "        \r\n",
        "        #conv_input = [batch size, hid dim, src len]\r\n",
        "        \r\n",
        "        #begin convolutional blocks...\r\n",
        "        \r\n",
        "        for i, conv in enumerate(self.convs):\r\n",
        "        \r\n",
        "            #pass through convolutional layer\r\n",
        "            conved = conv(self.dropout(conv_input))\r\n",
        "\r\n",
        "            #conved = [batch size, 2 * hid dim, src len]\r\n",
        "\r\n",
        "            #pass through GLU activation function\r\n",
        "            conved = F.glu(conved, dim = 1)\r\n",
        "\r\n",
        "            #conved = [batch size, hid dim, src len]\r\n",
        "            \r\n",
        "            #apply residual connection\r\n",
        "            conved = (conved + conv_input) * self.scale\r\n",
        "\r\n",
        "            #conved = [batch size, hid dim, src len]\r\n",
        "            \r\n",
        "            #set conv_input to conved for next loop iteration\r\n",
        "            conv_input = conved\r\n",
        "        \r\n",
        "        #...end convolutional blocks\r\n",
        "        \r\n",
        "        #permute and convert back to emb dim\r\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #conved = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\r\n",
        "        combined = (conved + embedded) * self.scale\r\n",
        "        \r\n",
        "        #combined = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        return conved, combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUEAan-FPIoJ"
      },
      "source": [
        "## Decoder \r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq3.png)\r\n",
        "\r\n",
        "## Decoder Conv Blocks\r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq4.png)\r\n",
        "\r\n",
        "## Incorrect Padding\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMUzsqzYugfw"
      },
      "source": [
        "### Decoder Class Implementation Details\r\n",
        "- Our decoder convolution block have two additional input from encoders which are being shown as **convd** abd **combined**. Then we have output text which is also input to the deocder as shown in the diagram above and that is also being fed to convolution block after going through\r\n",
        "    - token embedding and positional embedding\r\n",
        "    - add of both embedding output and this is directly fed to convolution block\r\n",
        "    - FC Linear layer output which is also fed to convolution block\r\n",
        "    - Embedding does not have residual connection to with linear layer as similar to Encoder. Rather this embedding combined has has feeding connection with convolution block.\r\n",
        "    - Decoder has padding twice in start instad of one padding in front and another has last. So we are going to have padding only at the begining of the sentence with two padding. So this means the decoder will see PAD, PAD and <SOS> and then kernel will prredict \"two\" which is next word in diagram above.\r\n",
        "    - \r\n",
        "\r\n",
        "- Flow\r\n",
        "  - First embedding for target and position being generated\r\n",
        "  - We have defined three linear FC layer which are having input and output as emb dim -> hid dim, hid dim ->emb dim and emb dim -> out dim\r\n",
        "  - Have defined nn.ModuleList to define stack of conv1d layers. **Key point to note here is that input to conv1d layer is hidden_dimension and output is double of that because we have to pass that output through GLU which will halve the final output.**\r\n",
        "  - It also has attention layers defined to understand where decoder is looking for.\r\n",
        "    - We want to focus on embedding-dim->hidden-dim and hidden-dim->embedding-dim and see in which word decoder is focussing on.\r\n",
        "    So this means our first attention need to have input as embedding-dim and output as hidden-dim. 2nd attention layer needs to have input as hidden-dim while its output will be embedding-dim\r\n",
        "    - For attention calculation, we first do a matrix multiplication between encoder convd (after permute) and combined vector.\r\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSyBwu30PH-2"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n",
        "        \r\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\r\n",
        "        \r\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n",
        "                                              out_channels = 2 * hid_dim, \r\n",
        "                                              kernel_size = kernel_size)\r\n",
        "                                    for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "      \r\n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #embedded = [batch size, trg len, emb dim]\r\n",
        "        #conved = [batch size, hid dim, trg len]\r\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #permute and convert back to emb dim\r\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #conved_emb = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        combined = (conved_emb + embedded) * self.scale\r\n",
        "        \r\n",
        "        #combined = [batch size, trg len, emb dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #energy = [batch size, trg len, src len]\r\n",
        "        \r\n",
        "        attention = F.softmax(energy, dim=2)\r\n",
        "        \r\n",
        "        #attention = [batch size, trg len, src len]\r\n",
        "            \r\n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch size, trg len, emd dim]\r\n",
        "        \r\n",
        "        #convert from emb dim -> hid dim\r\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #apply residual connection\r\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\r\n",
        "        \r\n",
        "        #attended_combined = [batch size, hid dim, trg len]\r\n",
        "        \r\n",
        "        return attention, attended_combined\r\n",
        "        \r\n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "            \r\n",
        "        #create position tensor\r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "        \r\n",
        "        #embed tokens and positions\r\n",
        "        tok_embedded = self.tok_embedding(trg)\r\n",
        "        pos_embedded = self.pos_embedding(pos)\r\n",
        "        \r\n",
        "        #tok_embedded = [batch size, trg len, emb dim]\r\n",
        "        #pos_embedded = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        #combine embeddings by elementwise summing\r\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\r\n",
        "        \r\n",
        "        #embedded = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\r\n",
        "        conv_input = self.emb2hid(embedded)\r\n",
        "        \r\n",
        "        #conv_input = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #permute for convolutional layer\r\n",
        "        conv_input = conv_input.permute(0, 2, 1) \r\n",
        "        \r\n",
        "        #conv_input = [batch size, hid dim, trg len]\r\n",
        "        \r\n",
        "        batch_size = conv_input.shape[0]\r\n",
        "        hid_dim = conv_input.shape[1]\r\n",
        "        \r\n",
        "        for i, conv in enumerate(self.convs):\r\n",
        "        \r\n",
        "            #apply dropout\r\n",
        "            conv_input = self.dropout(conv_input)\r\n",
        "        \r\n",
        "            #need to pad so decoder can't \"cheat\"\r\n",
        "            padding = torch.zeros(batch_size, \r\n",
        "                                  hid_dim, \r\n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\r\n",
        "                \r\n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\r\n",
        "        \r\n",
        "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\r\n",
        "        \r\n",
        "            #pass through convolutional layer\r\n",
        "            conved = conv(padded_conv_input)\r\n",
        "\r\n",
        "            #conved = [batch size, 2 * hid dim, trg len]\r\n",
        "            \r\n",
        "            #pass through GLU activation function\r\n",
        "            conved = F.glu(conved, dim = 1)\r\n",
        "\r\n",
        "            #conved = [batch size, hid dim, trg len]\r\n",
        "            \r\n",
        "            #calculate attention\r\n",
        "            attention, conved = self.calculate_attention(embedded, \r\n",
        "                                                         conved, \r\n",
        "                                                         encoder_conved, \r\n",
        "                                                         encoder_combined)\r\n",
        "            \r\n",
        "            #attention = [batch size, trg len, src len]\r\n",
        "            \r\n",
        "            #apply residual connection\r\n",
        "            conved = (conved + conv_input) * self.scale\r\n",
        "            \r\n",
        "            #conved = [batch size, hid dim, trg len]\r\n",
        "            \r\n",
        "            #set conv_input to conved for next loop iteration\r\n",
        "            conv_input = conved\r\n",
        "            \r\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n",
        "         \r\n",
        "        #conved = [batch size, trg len, emb dim]\r\n",
        "            \r\n",
        "        output = self.fc_out(self.dropout(conved))\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H56wQXdqPb1_"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        \r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\r\n",
        "           \r\n",
        "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\r\n",
        "        #encoder_conved is output from final encoder conv. block\r\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \r\n",
        "        #  positional embeddings \r\n",
        "        encoder_conved, encoder_combined = self.encoder(src)\r\n",
        "            \r\n",
        "        #encoder_conved = [batch size, src len, emb dim]\r\n",
        "        #encoder_combined = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #calculate predictions of next words\r\n",
        "        #output is a batch of predictions for each word in the trg sentence\r\n",
        "        #attention a batch of attention scores across the src sentence for \r\n",
        "        #  each word in the trg sentence\r\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #attention = [batch size, trg len - 1, src len]\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBUcXsz0PdUB"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "EMB_DIM = 256\r\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\r\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\r\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\r\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\r\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\r\n",
        "ENC_DROPOUT = 0.25\r\n",
        "DEC_DROPOUT = 0.25\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "    \r\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\r\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vPNjyGcPevW",
        "outputId": "12114990-ce6d-48af-c584-8dc7afd09535"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 37,351,685 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA11W1cGPgBi"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVexyPAPPiPO"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.src\r\n",
        "        trg = batch.trg\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "        \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-w9g5kZPjhS"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.src\r\n",
        "            trg = batch.trg\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhPAu069Pk20"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "FPUxl1tMPmVg",
        "outputId": "34ba3024-b35b-496d-8d60-1b10bec914d1"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 0.1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss} | Train PPL: {math.exp(train_loss)}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss} |  Val. PPL: {math.exp(valid_loss)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 58s\n",
            "\tTrain Loss: 4.266381767877923 | Train PPL: 71.26332134211806\n",
            "\t Val. Loss: 3.1878161430358887 |  Val. PPL: 24.235442868100158\n",
            "Epoch: 02 | Time: 1m 3s\n",
            "\tTrain Loss: 3.1760175921318288 | Train PPL: 23.951180004252112\n",
            "\t Val. Loss: 2.5195688903331757 |  Val. PPL: 12.423239730206662\n",
            "Epoch: 03 | Time: 1m 3s\n",
            "\tTrain Loss: 2.7741293623583956 | Train PPL: 16.024669240228906\n",
            "\t Val. Loss: 2.270636484026909 |  Val. PPL: 9.685563559117389\n",
            "Epoch: 04 | Time: 1m 4s\n",
            "\tTrain Loss: 2.589181928382571 | Train PPL: 13.318871354580308\n",
            "\t Val. Loss: 2.1785342693328857 |  Val. PPL: 8.833349454259277\n",
            "Epoch: 05 | Time: 1m 3s\n",
            "\tTrain Loss: 2.509461149770258 | Train PPL: 12.298301332226618\n",
            "\t Val. Loss: 2.1588054448366165 |  Val. PPL: 8.660785690430549\n",
            "Epoch: 06 | Time: 1m 4s\n",
            "\tTrain Loss: 2.5373395592105545 | Train PPL: 12.645982290226888\n",
            "\t Val. Loss: 2.154495269060135 |  Val. PPL: 8.623536514656635\n",
            "Epoch: 07 | Time: 1m 3s\n",
            "\tTrain Loss: 2.6472454312614406 | Train PPL: 14.115104021666067\n",
            "\t Val. Loss: 2.267509639263153 |  Val. PPL: 9.655325604749946\n",
            "Epoch: 08 | Time: 1m 3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-eaf718f8088b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\tTrain Loss: {train_loss} | Train PPL: {math.exp(train_loss)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\t Val. Loss: {valid_loss} |  Val. PPL: {math.exp(valid_loss)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: math range error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN-GkjB7Pn7H"
      },
      "source": [
        "model.load_state_dict(torch.load('tut5-model.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MGwOb3qPrS4"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('de')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        encoder_conved, encoder_combined = model.encoder(src_tensor)\r\n",
        "\r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ApPvr9PtgK"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\r\n",
        "    \r\n",
        "    fig = plt.figure(figsize=(10,10))\r\n",
        "    ax = fig.add_subplot(111)\r\n",
        "        \r\n",
        "    attention = attention.squeeze(0).cpu().detach().numpy()\r\n",
        "    \r\n",
        "    cax = ax.matshow(attention, cmap='bone')\r\n",
        "   \r\n",
        "    ax.tick_params(labelsize=15)\r\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n",
        "                       rotation=45)\r\n",
        "    ax.set_yticklabels(['']+translation)\r\n",
        "\r\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ48fLMsPu-1"
      },
      "source": [
        "example_idx = 2\r\n",
        "\r\n",
        "src = vars(train_data.examples[example_idx])['src']\r\n",
        "trg = vars(train_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULOvdnc1PwG0"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bU8kjg4PxTq"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADickZvsPyhI"
      },
      "source": [
        "example_idx = 2\r\n",
        "\r\n",
        "src = vars(valid_data.examples[example_idx])['src']\r\n",
        "trg = vars(valid_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkXuD77iP0f6"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uphiAwagP1ks"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndDCH8m6P23M"
      },
      "source": [
        "example_idx = 9\r\n",
        "\r\n",
        "src = vars(test_data.examples[example_idx])['src']\r\n",
        "trg = vars(test_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj_clFYtP4E0"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVkG0MkOP5QQ"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq0kiimuWRdT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "END_NLP_Assignment_13_Transformer_chatbot_tutorial_Separate_2.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfzAHluLRiXl"
      },
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E72FLf4oRtKa"
      },
      "source": [
        "import os\r\n",
        "!mkdir -p data\r\n",
        "os.chdir(\"/content/data\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCu5pAHTRugA",
        "outputId": "7308a64c-c0c0-475b-c984-1c85e54a9bee"
      },
      "source": [
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\r\n",
        "!unzip -o cornell_movie_dialogs_corpus.zip\r\n",
        "\r\n",
        "# os.chdir(\"/content\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-16 10:02:18--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  12.4MB/s    in 0.8s    \n",
            "\n",
            "2021-02-16 10:02:19 (12.4 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n",
            "Archive:  cornell_movie_dialogs_corpus.zip\n",
            "   creating: cornell movie-dialogs corpus/\n",
            "  inflating: cornell movie-dialogs corpus/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/cornell movie-dialogs corpus/\n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
            "  inflating: cornell movie-dialogs corpus/chameleons.pdf  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
            "  inflating: cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_conversations.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_lines.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/raw_script_urls.txt  \n",
            "  inflating: cornell movie-dialogs corpus/README.txt  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._README.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG3ir6w_RiXo"
      },
      "source": [
        "corpus_movie_conv = 'cornell movie-dialogs corpus/movie_conversations.txt'\n",
        "corpus_movie_lines = 'cornell movie-dialogs corpus/movie_lines.txt'\n",
        "max_len = 25"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8HMbzixRiXr"
      },
      "source": [
        "with open(corpus_movie_conv, 'r',\n",
        "           encoding='utf-8',\n",
        "          errors='ignore') as c:\n",
        "    conv = c.readlines()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COM7vf54RiXs"
      },
      "source": [
        "with open(corpus_movie_lines, 'r',\n",
        "          encoding='utf-8',\n",
        "          errors='ignore') as l:\n",
        "    lines = l.readlines()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe0bgp4PRiXs"
      },
      "source": [
        "lines_dic = {}\n",
        "for line in lines:\n",
        "    objects = line.split(\" +++$+++ \")\n",
        "    lines_dic[objects[0]] = objects[-1]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNbcY967RiXs"
      },
      "source": [
        "def remove_punc(string):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in string:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char  # space is also a character\n",
        "    return no_punct.lower()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEmz6evaRiXt"
      },
      "source": [
        "pairs = []\n",
        "for con in conv:\n",
        "    ids = eval(con.split(\" +++$+++ \")[-1])\n",
        "    for i in range(len(ids)):\n",
        "        qa_pairs = []\n",
        "        \n",
        "        if i==len(ids)-1:\n",
        "            break\n",
        "        \n",
        "        first = remove_punc(lines_dic[ids[i]].strip())      \n",
        "        second = remove_punc(lines_dic[ids[i+1]].strip())\n",
        "        qa_pairs.append(first.split()[:max_len])\n",
        "        qa_pairs.append(second.split()[:max_len])\n",
        "        pairs.append(qa_pairs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a5WMydjRiXt"
      },
      "source": [
        "word_freq = Counter()\n",
        "for pair in pairs:\n",
        "    word_freq.update(pair[0])\n",
        "    word_freq.update(pair[1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbKGDJorRiXt"
      },
      "source": [
        "min_word_freq = 5\n",
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
        "word_map['<unk>'] = len(word_map) + 1\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6m4va-qRiXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d42683e-4a96-420b-d346-400b795ed028"
      },
      "source": [
        "print(\"Total words are: {}\".format(len(word_map)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words are: 18190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IaHapxNRiXu"
      },
      "source": [
        "with open('WORDMAP_corpus.json', 'w') as j:\n",
        "    json.dump(word_map, j)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXdOPKxORiXu"
      },
      "source": [
        "def encode_question(words, word_map):\n",
        "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
        "    return enc_c"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R63QfabqRiXu"
      },
      "source": [
        "def encode_reply(words, word_map):\n",
        "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
        "    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
        "    return enc_c"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gW3mAOnRiXu"
      },
      "source": [
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "    qus = encode_question(pair[0], word_map)\n",
        "    ans = encode_reply(pair[1], word_map)\n",
        "    pairs_encoded.append([qus, ans])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iESfAFp_RiXu"
      },
      "source": [
        "with open('pairs_encoded.json', 'w') as p:\n",
        "    json.dump(pairs_encoded, p)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A4b4yFrRiXv"
      },
      "source": [
        "# rev_word_map = {v: k for k, v in word_map.items()}\n",
        "# ' '.join([rev_word_map[v] for v in pairs_encoded[1][0]])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlyBmc-YRiXv"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.pairs = json.load(open('pairs_encoded.json'))\n",
        "        self.dataset_size = len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        question = torch.LongTensor(self.pairs[i][0])\n",
        "        reply = torch.LongTensor(self.pairs[i][1])\n",
        "            \n",
        "        return question, reply\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rEn1b4MRiXv"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
        "                                           batch_size = 100, \n",
        "                                           shuffle=True, \n",
        "                                           pin_memory=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eVGZbIrRiXv"
      },
      "source": [
        "# question, reply = next(iter(train_loader))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Lm98FPRiXv"
      },
      "source": [
        "def create_masks(question, reply_input, reply_target):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask, reply_target_mask"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6di4rFxRiXw"
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n",
        "        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, embedding, layer_idx):\n",
        "        if layer_idx == 0:\n",
        "            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n",
        "        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTux9ANFRiXw"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_1lJ-v8RiXx"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z1ONDBQRiXx"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma-y5ngRRiXx"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSeSTa-oRiXy"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads, num_layers, word_map):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = len(word_map)\n",
        "        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers)\n",
        "        self.encoder = EncoderLayer(d_model, heads) \n",
        "        self.decoder = DecoderLayer(d_model, heads)\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
        "        \n",
        "    def encode(self, src_embeddings, src_mask):\n",
        "        for i in range(self.num_layers):\n",
        "            src_embeddings = self.embed(src_embeddings, i)\n",
        "            src_embeddings = self.encoder(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n",
        "        for i in range(self.num_layers):\n",
        "            tgt_embeddings = self.embed(tgt_embeddings, i)\n",
        "            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKMpBvb7RiXy"
      },
      "source": [
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()       "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry4RPO4Ic4Ke"
      },
      "source": [
        "# def maskNLLLoss(inp, target, mask):\r\n",
        "#     nTotal = mask.sum()\r\n",
        "#     crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\r\n",
        "#     loss = crossEntropy.masked_select(mask).mean()\r\n",
        "#     loss = loss.to(device)\r\n",
        "#     return loss, nTotal.item()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jObtgeVZBKH7"
      },
      "source": [
        "The torch.gather function (or torch.Tensor.gather) is a multi-index selection method. Look at the following example from the official docs:\r\n",
        "\r\n",
        "Let's start with going through the semantics of the different arguments: The first argument, input, is the source tensor that we want to select elements from. The second, dim, is the dimension (or axis in tensorflow/numpy) that we want to collect along. And finally, index are the indices to index input. As for the semantics of the operation, this is how the official docs explain it:\r\n",
        "\r\n",
        "So let's go through the example.\r\n",
        "\r\n",
        "the input tensor is [[1, 2], [3, 4]], and the dim argument is 1, i.e. we want to collect from the second dimension. The indices for the second dimension are given as [0, 0] and [1, 0].\r\n",
        "\r\n",
        "As we \"skip\" the first dimension (the dimension we want to collect along is 1), the first dimension of the result is implicitly given as the first dimension of the index. That means that the indices hold the second dimension, or the column indices, but not the row indices. Those are given by the indices of the index tensor itself. For the example, this means that the output will have in its first row a selection of the elements of the input tensor's first row as well, as given by the first row of the index tensor's first row. As the column-indices are given by [0, 0], we therefore select the first element of the first row of the input twice, resulting in [1, 1]. Similarly, the elements of the second row of the result are a result of indexing the second row of the input tensor by the elements of the second row of the index tensor, resulting in [4, 3].\r\n",
        "https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2fwjmG7CTtJ"
      },
      "source": [
        "t = torch.tensor([[1,2],[3,4]])\r\n",
        "r = torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))\r\n",
        "# r now holds:\r\n",
        "# tensor([[ 1,  1],\r\n",
        "#        [ 4,  3]])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS3C3MN70Ck9"
      },
      "source": [
        "## ORGINAL CODE\r\n",
        "\r\n",
        "## ORGINAL\r\n",
        "# target shape: torch.Size([64])\r\n",
        "# prediction shape: torch.Size([64, 7826])\r\n",
        "# mask shape: torch.Size([64])\r\n",
        "\r\n",
        "## THIS CODE\r\n",
        "# target shape: torch.Size([2600])\r\n",
        "# prediction shape: torch.Size([2600, 18190])\r\n",
        "# mask shape: torch.Size([100, 26])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko49ErXlc76W"
      },
      "source": [
        "class maskNLLLoss(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, size, smooth):\r\n",
        "        super(maskNLLLoss, self).__init__()\r\n",
        "        # self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\r\n",
        "        # self.confidence = 1.0 - smooth\r\n",
        "        # self.smooth = smooth\r\n",
        "        # self.size = size\r\n",
        "        self.ls = nn.LogSoftmax(dim=1)\r\n",
        "        self.nllloss = nn.NLLLoss()\r\n",
        "        \r\n",
        "    def forward(self, prediction, target, mask):\r\n",
        "        \"\"\"\r\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\r\n",
        "        target and mask of shape: (batch_size, max_words)\r\n",
        "        \"\"\"\r\n",
        "        mask = mask.to(device)\r\n",
        "        nTotal = mask.sum()\r\n",
        "        \r\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\r\n",
        "        # labels = prediction.data.clone()\r\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\r\n",
        "        mask = mask.contiguous().view(-1)   # (batch_size * max_words)\r\n",
        "        output = self.ls(prediction)\r\n",
        "        loss = self.nllloss(output, target)\r\n",
        "        # print(f\"target shape: {target.shape}\") ## target shape: torch.Size([2600])\r\n",
        "        # print(f\"prediction shape: {prediction.shape}\") ## prediction shape: torch.Size([2600, 18190])\r\n",
        "        # print(f\"mask shape: {mask.shape}\") ## mask shape: torch.Size([100, 26]). here 100 is batch\r\n",
        "        # crossEntropy = -torch.log(torch.gather(prediction.data, 1, target.data.view(-1, 1)).squeeze(1))\r\n",
        "        # crossEntropy.requres_grad = True\r\n",
        "        # loss = crossEntropy.masked_select(mask).mean()\r\n",
        "        # loss.requres_grad = True\r\n",
        "        # print(f\"Loss:{loss}\")\r\n",
        "        # loss = loss.to(device)\r\n",
        "        \r\n",
        "        # loss = Variable(loss, requires_grad = True)\r\n",
        "        return loss"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uukdtDUtRiXy"
      },
      "source": [
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqQx468jRiX0"
      },
      "source": [
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 10\n",
        "\n",
        "with open('WORDMAP_corpus.json', 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "    \n",
        "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
        "transformer = transformer.to(device)\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "# criterion = LossWithLS(len(word_map), 0.2)  ## changed loss function\n",
        "criterion = maskNLLLoss(len(word_map), 0.2)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeCFcc83RiX1"
      },
      "source": [
        "def train(train_loader, transformer, criterion, epoch):\n",
        "    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, (question, reply) in enumerate(train_loader):\n",
        "        \n",
        "        samples = question.shape[0]\n",
        "\n",
        "        # Move to device\n",
        "        question = question.to(device)\n",
        "        reply = reply.to(device)\n",
        "\n",
        "        # Prepare Target Data\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
        "\n",
        "        # Get the transformer outputs\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(out, reply_target, reply_target_mask)\n",
        "        loss.requres_grad = True\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "        sum_loss += loss.item() * samples\n",
        "        count += samples\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zk-JXqpRiX1"
      },
      "source": [
        "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    rev_word_map = {v: k for k, v in word_map.items()}\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<start>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    \n",
        "    for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<end>']:\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRJPH1dxRiX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d273945-441b-411c-cbad-4cdbcd0edd58"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    \n",
        "    train(train_loader, transformer, criterion, epoch)\n",
        "    \n",
        "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
        "    torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0][0/2217]\tLoss: 10.009\n",
            "Epoch [0][100/2217]\tLoss: 7.575\n",
            "Epoch [0][200/2217]\tLoss: 5.512\n",
            "Epoch [0][300/2217]\tLoss: 4.579\n",
            "Epoch [0][400/2217]\tLoss: 4.046\n",
            "Epoch [0][500/2217]\tLoss: 3.707\n",
            "Epoch [0][600/2217]\tLoss: 3.472\n",
            "Epoch [0][700/2217]\tLoss: 3.302\n",
            "Epoch [0][800/2217]\tLoss: 3.169\n",
            "Epoch [0][900/2217]\tLoss: 3.059\n",
            "Epoch [0][1000/2217]\tLoss: 2.969\n",
            "Epoch [0][1100/2217]\tLoss: 2.893\n",
            "Epoch [0][1200/2217]\tLoss: 2.832\n",
            "Epoch [0][1300/2217]\tLoss: 2.775\n",
            "Epoch [0][1400/2217]\tLoss: 2.729\n",
            "Epoch [0][1500/2217]\tLoss: 2.686\n",
            "Epoch [0][1600/2217]\tLoss: 2.649\n",
            "Epoch [0][1700/2217]\tLoss: 2.616\n",
            "Epoch [0][1800/2217]\tLoss: 2.585\n",
            "Epoch [0][1900/2217]\tLoss: 2.557\n",
            "Epoch [0][2000/2217]\tLoss: 2.533\n",
            "Epoch [0][2100/2217]\tLoss: 2.510\n",
            "Epoch [0][2200/2217]\tLoss: 2.490\n",
            "Epoch [1][0/2217]\tLoss: 2.143\n",
            "Epoch [1][100/2217]\tLoss: 2.040\n",
            "Epoch [1][200/2217]\tLoss: 2.033\n",
            "Epoch [1][300/2217]\tLoss: 2.028\n",
            "Epoch [1][400/2217]\tLoss: 2.030\n",
            "Epoch [1][500/2217]\tLoss: 2.030\n",
            "Epoch [1][600/2217]\tLoss: 2.035\n",
            "Epoch [1][700/2217]\tLoss: 2.037\n",
            "Epoch [1][800/2217]\tLoss: 2.037\n",
            "Epoch [1][900/2217]\tLoss: 2.038\n",
            "Epoch [1][1000/2217]\tLoss: 2.037\n",
            "Epoch [1][1100/2217]\tLoss: 2.037\n",
            "Epoch [1][1200/2217]\tLoss: 2.037\n",
            "Epoch [1][1300/2217]\tLoss: 2.036\n",
            "Epoch [1][1400/2217]\tLoss: 2.036\n",
            "Epoch [1][1500/2217]\tLoss: 2.036\n",
            "Epoch [1][1600/2217]\tLoss: 2.036\n",
            "Epoch [1][1700/2217]\tLoss: 2.036\n",
            "Epoch [1][1800/2217]\tLoss: 2.036\n",
            "Epoch [1][1900/2217]\tLoss: 2.036\n",
            "Epoch [1][2000/2217]\tLoss: 2.036\n",
            "Epoch [1][2100/2217]\tLoss: 2.036\n",
            "Epoch [1][2200/2217]\tLoss: 2.035\n",
            "Epoch [2][0/2217]\tLoss: 2.099\n",
            "Epoch [2][100/2217]\tLoss: 1.984\n",
            "Epoch [2][200/2217]\tLoss: 1.982\n",
            "Epoch [2][300/2217]\tLoss: 1.993\n",
            "Epoch [2][400/2217]\tLoss: 1.991\n",
            "Epoch [2][500/2217]\tLoss: 1.991\n",
            "Epoch [2][600/2217]\tLoss: 1.996\n",
            "Epoch [2][700/2217]\tLoss: 1.995\n",
            "Epoch [2][800/2217]\tLoss: 1.994\n",
            "Epoch [2][900/2217]\tLoss: 1.995\n",
            "Epoch [2][1000/2217]\tLoss: 1.993\n",
            "Epoch [2][1100/2217]\tLoss: 1.993\n",
            "Epoch [2][1200/2217]\tLoss: 1.995\n",
            "Epoch [2][1300/2217]\tLoss: 1.996\n",
            "Epoch [2][1400/2217]\tLoss: 1.994\n",
            "Epoch [2][1500/2217]\tLoss: 1.994\n",
            "Epoch [2][1600/2217]\tLoss: 1.994\n",
            "Epoch [2][1700/2217]\tLoss: 1.996\n",
            "Epoch [2][1800/2217]\tLoss: 1.997\n",
            "Epoch [2][1900/2217]\tLoss: 1.997\n",
            "Epoch [2][2000/2217]\tLoss: 1.996\n",
            "Epoch [2][2100/2217]\tLoss: 1.996\n",
            "Epoch [2][2200/2217]\tLoss: 1.996\n",
            "Epoch [3][0/2217]\tLoss: 2.220\n",
            "Epoch [3][100/2217]\tLoss: 1.971\n",
            "Epoch [3][200/2217]\tLoss: 1.968\n",
            "Epoch [3][300/2217]\tLoss: 1.963\n",
            "Epoch [3][400/2217]\tLoss: 1.958\n",
            "Epoch [3][500/2217]\tLoss: 1.964\n",
            "Epoch [3][600/2217]\tLoss: 1.965\n",
            "Epoch [3][700/2217]\tLoss: 1.964\n",
            "Epoch [3][800/2217]\tLoss: 1.963\n",
            "Epoch [3][900/2217]\tLoss: 1.963\n",
            "Epoch [3][1000/2217]\tLoss: 1.960\n",
            "Epoch [3][1100/2217]\tLoss: 1.957\n",
            "Epoch [3][1200/2217]\tLoss: 1.959\n",
            "Epoch [3][1300/2217]\tLoss: 1.961\n",
            "Epoch [3][1400/2217]\tLoss: 1.962\n",
            "Epoch [3][1500/2217]\tLoss: 1.963\n",
            "Epoch [3][1600/2217]\tLoss: 1.963\n",
            "Epoch [3][1700/2217]\tLoss: 1.964\n",
            "Epoch [3][1800/2217]\tLoss: 1.963\n",
            "Epoch [3][1900/2217]\tLoss: 1.963\n",
            "Epoch [3][2000/2217]\tLoss: 1.965\n",
            "Epoch [3][2100/2217]\tLoss: 1.965\n",
            "Epoch [3][2200/2217]\tLoss: 1.966\n",
            "Epoch [4][0/2217]\tLoss: 1.830\n",
            "Epoch [4][100/2217]\tLoss: 1.919\n",
            "Epoch [4][200/2217]\tLoss: 1.921\n",
            "Epoch [4][300/2217]\tLoss: 1.924\n",
            "Epoch [4][400/2217]\tLoss: 1.930\n",
            "Epoch [4][500/2217]\tLoss: 1.932\n",
            "Epoch [4][600/2217]\tLoss: 1.931\n",
            "Epoch [4][700/2217]\tLoss: 1.932\n",
            "Epoch [4][800/2217]\tLoss: 1.932\n",
            "Epoch [4][900/2217]\tLoss: 1.937\n",
            "Epoch [4][1000/2217]\tLoss: 1.939\n",
            "Epoch [4][1100/2217]\tLoss: 1.942\n",
            "Epoch [4][1200/2217]\tLoss: 1.940\n",
            "Epoch [4][1300/2217]\tLoss: 1.942\n",
            "Epoch [4][1400/2217]\tLoss: 1.943\n",
            "Epoch [4][1500/2217]\tLoss: 1.942\n",
            "Epoch [4][1600/2217]\tLoss: 1.943\n",
            "Epoch [4][1700/2217]\tLoss: 1.943\n",
            "Epoch [4][1800/2217]\tLoss: 1.942\n",
            "Epoch [4][1900/2217]\tLoss: 1.943\n",
            "Epoch [4][2000/2217]\tLoss: 1.943\n",
            "Epoch [4][2100/2217]\tLoss: 1.944\n",
            "Epoch [4][2200/2217]\tLoss: 1.945\n",
            "Epoch [5][0/2217]\tLoss: 1.800\n",
            "Epoch [5][100/2217]\tLoss: 1.898\n",
            "Epoch [5][200/2217]\tLoss: 1.909\n",
            "Epoch [5][300/2217]\tLoss: 1.906\n",
            "Epoch [5][400/2217]\tLoss: 1.912\n",
            "Epoch [5][500/2217]\tLoss: 1.914\n",
            "Epoch [5][600/2217]\tLoss: 1.915\n",
            "Epoch [5][700/2217]\tLoss: 1.918\n",
            "Epoch [5][800/2217]\tLoss: 1.920\n",
            "Epoch [5][900/2217]\tLoss: 1.920\n",
            "Epoch [5][1000/2217]\tLoss: 1.920\n",
            "Epoch [5][1100/2217]\tLoss: 1.923\n",
            "Epoch [5][1200/2217]\tLoss: 1.922\n",
            "Epoch [5][1300/2217]\tLoss: 1.924\n",
            "Epoch [5][1400/2217]\tLoss: 1.924\n",
            "Epoch [5][1500/2217]\tLoss: 1.924\n",
            "Epoch [5][1600/2217]\tLoss: 1.924\n",
            "Epoch [5][1700/2217]\tLoss: 1.923\n",
            "Epoch [5][1800/2217]\tLoss: 1.924\n",
            "Epoch [5][1900/2217]\tLoss: 1.925\n",
            "Epoch [5][2000/2217]\tLoss: 1.927\n",
            "Epoch [5][2100/2217]\tLoss: 1.926\n",
            "Epoch [5][2200/2217]\tLoss: 1.927\n",
            "Epoch [6][0/2217]\tLoss: 1.841\n",
            "Epoch [6][100/2217]\tLoss: 1.903\n",
            "Epoch [6][200/2217]\tLoss: 1.897\n",
            "Epoch [6][300/2217]\tLoss: 1.897\n",
            "Epoch [6][400/2217]\tLoss: 1.892\n",
            "Epoch [6][500/2217]\tLoss: 1.897\n",
            "Epoch [6][600/2217]\tLoss: 1.893\n",
            "Epoch [6][700/2217]\tLoss: 1.890\n",
            "Epoch [6][800/2217]\tLoss: 1.891\n",
            "Epoch [6][900/2217]\tLoss: 1.894\n",
            "Epoch [6][1000/2217]\tLoss: 1.895\n",
            "Epoch [6][1100/2217]\tLoss: 1.897\n",
            "Epoch [6][1200/2217]\tLoss: 1.899\n",
            "Epoch [6][1300/2217]\tLoss: 1.903\n",
            "Epoch [6][1400/2217]\tLoss: 1.905\n",
            "Epoch [6][1500/2217]\tLoss: 1.907\n",
            "Epoch [6][1600/2217]\tLoss: 1.908\n",
            "Epoch [6][1700/2217]\tLoss: 1.910\n",
            "Epoch [6][1800/2217]\tLoss: 1.910\n",
            "Epoch [6][1900/2217]\tLoss: 1.911\n",
            "Epoch [6][2000/2217]\tLoss: 1.911\n",
            "Epoch [6][2100/2217]\tLoss: 1.911\n",
            "Epoch [6][2200/2217]\tLoss: 1.912\n",
            "Epoch [7][0/2217]\tLoss: 1.937\n",
            "Epoch [7][100/2217]\tLoss: 1.891\n",
            "Epoch [7][200/2217]\tLoss: 1.872\n",
            "Epoch [7][300/2217]\tLoss: 1.873\n",
            "Epoch [7][400/2217]\tLoss: 1.886\n",
            "Epoch [7][500/2217]\tLoss: 1.887\n",
            "Epoch [7][600/2217]\tLoss: 1.890\n",
            "Epoch [7][700/2217]\tLoss: 1.893\n",
            "Epoch [7][800/2217]\tLoss: 1.891\n",
            "Epoch [7][900/2217]\tLoss: 1.889\n",
            "Epoch [7][1000/2217]\tLoss: 1.892\n",
            "Epoch [7][1100/2217]\tLoss: 1.892\n",
            "Epoch [7][1200/2217]\tLoss: 1.892\n",
            "Epoch [7][1300/2217]\tLoss: 1.894\n",
            "Epoch [7][1400/2217]\tLoss: 1.895\n",
            "Epoch [7][1500/2217]\tLoss: 1.894\n",
            "Epoch [7][1600/2217]\tLoss: 1.894\n",
            "Epoch [7][1700/2217]\tLoss: 1.895\n",
            "Epoch [7][1800/2217]\tLoss: 1.896\n",
            "Epoch [7][1900/2217]\tLoss: 1.897\n",
            "Epoch [7][2000/2217]\tLoss: 1.899\n",
            "Epoch [7][2100/2217]\tLoss: 1.900\n",
            "Epoch [7][2200/2217]\tLoss: 1.900\n",
            "Epoch [8][0/2217]\tLoss: 2.038\n",
            "Epoch [8][100/2217]\tLoss: 1.867\n",
            "Epoch [8][200/2217]\tLoss: 1.879\n",
            "Epoch [8][300/2217]\tLoss: 1.877\n",
            "Epoch [8][400/2217]\tLoss: 1.878\n",
            "Epoch [8][500/2217]\tLoss: 1.880\n",
            "Epoch [8][600/2217]\tLoss: 1.880\n",
            "Epoch [8][700/2217]\tLoss: 1.883\n",
            "Epoch [8][800/2217]\tLoss: 1.881\n",
            "Epoch [8][900/2217]\tLoss: 1.882\n",
            "Epoch [8][1000/2217]\tLoss: 1.883\n",
            "Epoch [8][1100/2217]\tLoss: 1.885\n",
            "Epoch [8][1200/2217]\tLoss: 1.885\n",
            "Epoch [8][1300/2217]\tLoss: 1.887\n",
            "Epoch [8][1400/2217]\tLoss: 1.888\n",
            "Epoch [8][1500/2217]\tLoss: 1.887\n",
            "Epoch [8][1600/2217]\tLoss: 1.887\n",
            "Epoch [8][1700/2217]\tLoss: 1.887\n",
            "Epoch [8][1800/2217]\tLoss: 1.887\n",
            "Epoch [8][1900/2217]\tLoss: 1.888\n",
            "Epoch [8][2000/2217]\tLoss: 1.889\n",
            "Epoch [8][2100/2217]\tLoss: 1.888\n",
            "Epoch [8][2200/2217]\tLoss: 1.889\n",
            "Epoch [9][0/2217]\tLoss: 2.012\n",
            "Epoch [9][100/2217]\tLoss: 1.842\n",
            "Epoch [9][200/2217]\tLoss: 1.851\n",
            "Epoch [9][300/2217]\tLoss: 1.858\n",
            "Epoch [9][400/2217]\tLoss: 1.861\n",
            "Epoch [9][500/2217]\tLoss: 1.869\n",
            "Epoch [9][600/2217]\tLoss: 1.871\n",
            "Epoch [9][700/2217]\tLoss: 1.870\n",
            "Epoch [9][800/2217]\tLoss: 1.871\n",
            "Epoch [9][900/2217]\tLoss: 1.872\n",
            "Epoch [9][1000/2217]\tLoss: 1.874\n",
            "Epoch [9][1100/2217]\tLoss: 1.875\n",
            "Epoch [9][1200/2217]\tLoss: 1.876\n",
            "Epoch [9][1300/2217]\tLoss: 1.877\n",
            "Epoch [9][1400/2217]\tLoss: 1.878\n",
            "Epoch [9][1500/2217]\tLoss: 1.877\n",
            "Epoch [9][1600/2217]\tLoss: 1.878\n",
            "Epoch [9][1700/2217]\tLoss: 1.879\n",
            "Epoch [9][1800/2217]\tLoss: 1.881\n",
            "Epoch [9][1900/2217]\tLoss: 1.880\n",
            "Epoch [9][2000/2217]\tLoss: 1.881\n",
            "Epoch [9][2100/2217]\tLoss: 1.880\n",
            "Epoch [9][2200/2217]\tLoss: 1.881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Eh2PTsL4yu"
      },
      "source": [
        "## ORGINAL\r\n",
        "target shape: torch.Size([64])\r\n",
        "prediction shape: torch.Size([64, 7826])\r\n",
        "mask shape: torch.Size([64])\r\n",
        "\r\n",
        "## THIS CODE\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDhFb3vL97d"
      },
      "source": [
        "## ORGINAL CODE\r\n",
        "\r\n",
        "## ORGINAL\r\n",
        "# target shape: torch.Size([64])\r\n",
        "# prediction shape: torch.Size([64, 7826])\r\n",
        "# mask shape: torch.Size([64])\r\n",
        "\r\n",
        "## THIS CODE\r\n",
        "# target shape: torch.Size([2600])\r\n",
        "# prediction shape: torch.Size([2600, 18190])\r\n",
        "# mask shape: torch.Size([100, 26])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MugQ-PgyRiX1"
      },
      "source": [
        "checkpoint = torch.load('checkpoint_0.pth.tar')\n",
        "transformer = checkpoint['transformer']"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeBaCOmdRiX1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "outputId": "45ee068e-2a6d-413e-b74a-ab438b49870c"
      },
      "source": [
        "while(1):\n",
        "    question = input(\"Question: \") \n",
        "    if question == 'quit':\n",
        "        break\n",
        "    max_len = input(\"Maximum Reply Length: \")\n",
        "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
        "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
        "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
        "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
        "    print(sentence)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: How are you today?\n",
            "Maximum Reply Length: 20\n",
            "i dont know\n",
            "Question: WHAT ARE YOU DOING?\n",
            "Maximum Reply Length: 10\n",
            "i dont know what about it\n",
            "Question: When is the show?\n",
            "Maximum Reply Length: 10\n",
            "i dont know\n",
            "Question: what is your name?\n",
            "Maximum Reply Length: 20\n",
            "i dont know\n",
            "Question: how is weather today?\n",
            "Maximum Reply Length: 10\n",
            "i dont know\n",
            "Question: how is the movie?\n",
            "Maximum Reply Length: 10\n",
            "i dont know\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-11bed82a5804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maximum Reply Length: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPzsyy2cRiX1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
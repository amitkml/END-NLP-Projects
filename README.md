# END-NLP-Projects
This repo contains all my nlp work and learning. Made public so that others can learn and get benefits.The repo will contain all my project related to NLP learning and vision model deployment using mediapipe.

![image](https://media.giphy.com/media/YknAouVrcbkiDvWUOR/giphy.gif)

![image](https://media.giphy.com/media/26xBtSyoi5hUUkCEo/giphy.gif)

![image](https://media.giphy.com/media/3o6Ztg5jGKDQSjaZ1K/giphy.gif)

## The Super Duper NLP Repo
- [The Super Duper NLP Repo](https://notebooks.quantumstat.com/)
- [Quantum Stat](https://quantumstat.medium.com/)

## Reading Content

##General Machine Learning
- [Machine Learning 101 from Google's Senior Creative Engineer explains Machine Learning for engineer's and executives alike](https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/edit?usp=sharing)
- [AI Playbook - a16z AI playbook is a great link to forward to your managers or content for your presentations](https://aiplaybook.a16z.com/)
- [Ruder's Blog](http://ruder.io/#open) by [Sebastian Ruder](https://twitter.com/seb_ruder) for commentary on the best of NLP Research
- [How To Label Data](https://www.lighttag.io/how-to-label-data/) guide to managing larger linguistic annotation projects
- [awesome-nlp](https://github.com/keon/awesome-nlp#books)
- [Zero To One For NLP](https://pakodas.substack.com/p/nlp-metablog-a-blog-of-blogs-693e3a8f1e0c)

### Introductions and Guides to NLP
- [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
- [The Illustrated BERT, ELMo, and co](https://jalammar.github.io/illustrated-bert/). (How NLP Cracked Transfer Learning) and The [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Super Duper NLP Repo (SDNLPR): Collection of Colab notebooks covering a wide array of NLP task implementations.](https://notebooks.quantumstat.com/)
- [Natural Language Processing by Hal DaumÃ© III](https://nlpers.blogspot.com/)


### Books
- [Speech and Language Processing - free, by Prof. Dan Jurafsy](https://web.stanford.edu/~jurafsky/slp3/)
- [Natural Language Processing - free, NLP notes by Dr. Jacob Eisenstein at GeorgiaTech](https://github.com/jacobeisenstein/gt-nlp-class)
- [NLP with PyTorch - Brian & Delip Rao](https://github.com/joosthub/PyTorchNLPBook)
- [Natural Language Processing with Python](https://www.nltk.org/book/)

## NLP Zero to One: Full Course
- [NLP Zero to One: Full Course](https://medium.com/nerd-for-tech/nlp-zero-to-one-full-course-4f8e1902c379)

## Annotation Tools
- [GATE - General Architecture and Text Engineering is 15+ years old, free and open source](https://gate.ac.uk/overview.html)
- [Anafora is free and open source, web-based raw text annotation tool](https://github.com/weitechen/anafora)
- [brat - brat rapid annotation tool is an online environment for collaborative text annotation](https://brat.nlplab.org/)

## Text Embeddings
- word2vec - [implementation](https://code.google.com/archive/p/word2vec/) - [explainer blog](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
- [glove](https://nlp.stanford.edu/pubs/glove.pdf) - [explainer blog](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)
- fasttext - [implementation](https://github.com/facebookresearch/fastText) - paper - [explainer blog](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)

## Sentence and Language Model Based Word Embeddings
- [Sequence to Sequence Learning - word vectors for machine translation](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
- [Skip Thought Vectors - word representation method](https://arxiv.org/abs/1506.06726)



## Loss Function
- [How to mask the loss](https://www.programmersought.com/article/7809233934/)
- [How Pytorch Customizes Loss](https://www.programmersought.com/article/4356271119/)

## Curated resource in NLP
This section will contain my curated resources which helped me during NLP learning
For NLP, [@quantum_stat](https://twitter.com/Quantum_Stat),[amitness](https://twitter.com/amitness) ,[Stanford NLP Group](https://twitter.com/stanfordnlp) and [@nlpguy_'s](https://twitter.com/nlpguy_) curation of resources and experience is super-helpful.

- Data: http://datasets.quantumstat.com
- Model selection: http://models.pratik.ai
- Models: http://models.quantumstat.com
- Notebook: http://notebooks.quantumstat.com
- Deployment: http://deployment.pratik.ai
- [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
- [Natural Language Processing with Python](http://www.nltk.org/book/)
- [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)
- [A Visual Survey of Data Augmentation in NLP](https://amitness.com/2020/05/data-augmentation-for-nlp/)
- [NLP Augmentation Package](https://github.com/makcedward/nlpaug)
- [Allen NLP](https://allennlp.org/)
- [Simple Transformers](https://simpletransformers.ai/)
- [Automatically Generate True or False questions from any content with OpenAI GPT2, Sentence BERT and Berkley Constituency parse](https://medium.com/swlh/practical-ai-automatically-generate-true-or-false-questions-from-any-content-with-openai-gpt2-9081ffe4d4c9)
- [Generative Model Chatbots](https://medium.com/botsupply/generative-model-chatbots-e422ab08461e)
- [Deep Learning for Chatbots, Part 1 â€“ Introduction](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/?subscribe=success#blog_subscription-2)
- [Natural Language Understanding for Chatbots](https://medium.com/neuralspace/natural-language-understanding-for-chatbots-2eb7a81b9390)
- [Indic Transformers: An Analysis of Transformer Language Models for Indian Languages](https://medium.com/neuralspace/indic-transformers-an-analysis-of-transformer-language-models-for-indian-languages-c6b4db0643b)
- [How Does Attention Work in Encoder-Decoder Recurrent Neural Networks](https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/)

### Convolution for Text
- [Convolutional Sequence to Sequence Learning](https://charon.me/posts/pytorch/pytorch_seq2seq_5/)
- [Sequence models & Attention mechanism](https://charon.me/posts/dl/dl15/)


### BERT
- [BERT (Word Embeddings)](https://charon.me/posts/nlp/bert/)
- [BERT Text Classification Using Pytorch](https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b)

### GPT2
- [How GPT 2 works](https://charon.me/posts/model/gpt-2/)

### Convolution for NLP
- [Translation Pytorch Tutorial NLP Part: Use TorchText for Text Classification](https://www.programmersought.com/article/93015028948/)
- [CNNs for Text Classification](https://cezannec.github.io/CNN_Text_Classification/)
- [Extracting Keyphrases from Text: RAKE and Gensim in Python](https://medium.com/@nikitasaxena0209)
- [PyTorch: Scene Text Detection and Recognition by CRAFT and a Four-Stage Network](https://towardsdatascience.com/pytorch-scene-text-detection-and-recognition-by-craft-and-a-four-stage-network-ec814d39db05)
- [CNN Text Classification](https://github.com/amitkml/CNN_Text_Classification)

### Attention? Attention is what you need!
- [Attention why?](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
- [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 8 â€“ Translation, Seq2Seq, Attention](https://www.youtube.com/watch?v=XXtpJxZBa2c)
- [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 â€“ Transformers and Self-Attention](https://www.youtube.com/watch?v=5vcj8kSwBCY)
- [TRANSFORMERS FROM SCRATCH](http://peterbloem.nl/blog/transformers)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

### More details on Transformer
- [ðŸ¤— Transformers Notebooks](https://huggingface.co/transformers/notebooks.html)
- [Live Session- Encoder Decoder,Attention Models, Transformers, Bert Part 1](https://www.youtube.com/watch?v=bHfXYQgn0Cc)
- [Live Session- Encoder Decoder,Attention Models, Transformers, Bert Part 2](https://www.youtube.com/watch?v=ZU12u6-ewP0)
- [Natural Language Processing: the age of Transformers](https://blog.scaleway.com/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/)
- [10 Things You Need to Know About BERT and the Transformer Architecture That Are Reshaping the AI Landscape](https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape)
 
## Curated repo for NLP
- https://github.com/bentrevett/pytorch-seq2seq
- [SNU 4th Industrial Revolution Academy: Artificial Intelligence Agent](http://ling.snu.ac.kr/class/AI_Agent/)
- [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 11 â€“ Convolutional Networks for NLP](https://www.youtube.com/watch?v=EAJoRA0KX7I)
- [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 8 â€“ Translation, Seq2Seq, Attention](https://www.youtube.com/watch?v=XXtpJxZBa2c)

## Details on Medipipe

I am going to follow [End To End Project Example In Mediapipe](https://medium.com/@mahakal001/end-to-end-project-example-in-mediapipe-b74a4a8ebb61) and github issue [How to put my own Trained model in mediapipe?](https://github.com/google/mediapipe/issues/507)for adding my model into mediapipe.

More details on mediapipe can be found from [Background Features in Google Meet, Powered by Web ML](https://www.googblogs.com/tag/machine-perception/)

## Machine Learning Projects Solved and Explained
- [130 Machine Learning Projects Solved and Explained](https://medium.com/the-innovation/130-machine-learning-projects-solved-and-explained-605d188fb392)

<!DOCTYPE html>
<!-- saved from url=(0059)https://krypticmouse.hashnode.dev/attention-is-all-you-need -->
<html lang="en" id="current-style"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link rel="preconnect" href="https://fonts.googleapis.com/"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="true"><style data-emotion-css=""></style><style>.article-width{width:800px}@media (max-width:800px){.article-width{width:100%!important}}.tooltip-handle{position:relative}.tooltip-handle:after{content:attr(data-title);--tw-bg-opacity:1;background-color:rgba(31,41,55,var(--tw-bg-opacity));border-radius:.25rem;font-size:.75rem;line-height:1rem;margin-top:.5rem;opacity:0;padding:.5rem;position:absolute;right:0;top:100%;--tw-shadow:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);box-shadow:var(--tw-ring-offset-shadow,0 0 #0000),var(--tw-ring-shadow,0 0 #0000),var(--tw-shadow);text-align:right;--tw-text-opacity:1;color:rgba(255,255,255,var(--tw-text-opacity));visibility:hidden;white-space:nowrap;z-index:20;transition-property:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:150ms}.tooltip-handle.gold-tooltip:after{background:linear-gradient(135.06deg,#fcc201 .05%,#b78628 99.96%)}.tooltip-handle:hover::after{opacity:1;visibility:visible;transition-delay:1s}.tooltip-handle.tooltip-right::after{margin-top:.5rem;margin-left:.5rem;position:absolute;top:0;left:100%;right:auto}.tooltip-handle.tooltip-left-aligned::after{left:0;top:100%;right:auto}.tooltip-handle.tooltip-right-aligned::after{right:0;top:100%}.tooltip-handle.tooltip-left::after{top:0;right:100%;left:auto}@media (max-width:1023px){.tooltip-handle:after{display:none}}.hn-table{display:block;width:100%;overflow-x:auto;margin-top:2em;margin-bottom:2em}.prose .hn-table table{border:1px solid #e0e0e0;margin-top:0;margin-bottom:0}.prose .hn-table table tbody td:first-child{padding-left:.5rem}.prose .hn-table table td{min-width:200px;padding:.5rem}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}@media (min-width:1536px){.container{max-width:1536px}}@media (min-width:1440px){.feed-width{min-width:850px;max-width:1000px}.container-wrapper{max-width:1232px}}.prose code{border-radius:.25rem;font-weight:400!important;padding:.25rem;--tw-text-opacity:1;color:rgba(0,0,0,var(--tw-text-opacity));font-size:80%;background:#f0efed}.dark .prose code{--tw-bg-opacity:1;background-color:rgba(36,41,46,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(238,238,238,var(--tw-text-opacity))}.prose pre{padding-top:2rem!important;padding-bottom:2rem!important}.prose pre code{background-color:transparent;border-radius:0;font-weight:500!important;padding:0;font-size:100%}.prose blockquote p:first-of-type::before,.prose blockquote p:last-of-type::after{content:none}.dark .prose pre code{background-color:transparent;color:#ddd}.prose .embed-wrapper{max-width:100%;margin-top:1.25em;margin-bottom:1.25em}.prose .embed-wrapper iframe{max-width:100%!important}a.user-mention{color:#2962ff!important;--tw-border-opacity:1;border-color:rgba(158,158,158,var(--tw-border-opacity));border-style:dashed;border-bottom-width:2px;display:inline;font-weight:500;line-height:1.375;text-decoration:none!important}</style><style>#nprogress{pointer-events:none}#nprogress .bar{background:#29d;position:fixed;z-index:1031;top:0;left:0;width:100%;height:2px}#nprogress .peg{display:block;position:absolute;right:0;width:100px;height:100%;box-shadow:0 0 10px #29d,0 0 5px #29d;opacity:1;-webkit-transform:rotate(3deg) translate(0,-4px);-ms-transform:rotate(3deg) translate(0,-4px);transform:rotate(3deg) translate(0,-4px)}#nprogress .spinner{display:block;position:fixed;z-index:1031;top:15px;right:15px}#nprogress .spinner-icon{width:18px;height:18px;box-sizing:border-box;border:solid 2px transparent;border-top-color:#29d;border-left-color:#29d;border-radius:50%;-webkit-animation:nprogress-spinner .4s linear infinite;animation:nprogress-spinner .4s linear infinite}.nprogress-custom-parent{overflow:hidden;position:relative}.nprogress-custom-parent #nprogress .bar,.nprogress-custom-parent #nprogress .spinner{position:absolute}@-webkit-keyframes nprogress-spinner{0%{-webkit-transform:rotate(0)}100%{-webkit-transform:rotate(360deg)}}@keyframes nprogress-spinner{0%{transform:rotate(0)}100%{transform:rotate(360deg)}}</style><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="canonical" href="https://krypticmouse.hashnode.dev/attention-is-all-you-need"><title>Transformers: Attention is all you need</title><meta name="description" content="Let&#39;s go ahead and break down the sections of the paper, **Attention is all you need**. To give you a gist of what&#39;ll be there, it&#39;ll be an explanation of e"><meta property="og:title" content="Transformers: Attention is all you need"><meta property="og:description" content="Let&#39;s go ahead and break down the sections of the paper, **Attention is all you need**. To give you a gist of what&#39;ll be there, it&#39;ll be an explanation of e"><meta property="og:site_name" content="krypticmouse"><meta property="og:type" content="article"><meta property="og:url" content="https://krypticmouse.hashnode.dev/attention-is-all-you-need"><meta name="image" property="og:image" content="https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1638987431300%2F0yWP8XDYJ.png%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng"><link rel="icon" type="image/png" href="https://cdn.hashnode.com/res/hashnode/image/upload/v1619641126923/FnNt45KTC.png?auto=compress,format&amp;format=webp&amp;fm=png"><meta name="theme-color" content="#f6f7fb"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:title" content="Transformers: Attention is all you need"><meta property="twitter:description" content="Let&#39;s go ahead and break down the sections of the paper, **Attention is all you need**. To give you a gist of what&#39;ll be there, it&#39;ll be an explanation of e"><meta property="twitter:image" content="https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1638987431300%2F0yWP8XDYJ.png%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng"><style>/* Monkai theme */
          .hljs{display:block;overflow-x:auto;padding:.5em;background:#23241f}.hljs,.hljs-subst,.hljs-tag{color:#f8f8f2}.hljs-emphasis,.hljs-strong{color:#a8a8a2}.hljs-bullet,.hljs-link,.hljs-literal,.hljs-number,.hljs-quote,.hljs-regexp{color:#ae81ff}.hljs-code,.hljs-section,.hljs-selector-class,.hljs-title{color:#a6e22e}.hljs-strong{font-weight:700}.hljs-emphasis{font-style:italic}.hljs-attr,.hljs-keyword,.hljs-name,.hljs-selector-tag{color:#f92672}.hljs-attribute,.hljs-symbol{color:#66d9ef}.hljs-class .hljs-title,.hljs-params{color:#f8f8f2}.hljs-addition,.hljs-built_in,.hljs-builtin-name,.hljs-selector-attr,.hljs-selector-id,.hljs-selector-pseudo,.hljs-string,.hljs-template-variable,.hljs-type,.hljs-variable{color:#e6db74}.hljs-comment,.hljs-deletion,.hljs-meta{color:#75715e}
            /* Monkai theme ends */</style><link rel="preload" as="image" imagesrcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2x"><meta name="next-head-count" content="19"><noscript data-n-css=""></noscript><script src="./Transformers_ Attention is all you need_files/iframe-resizer.js.download" async="" defer=""></script><script defer="" nomodule="" src="./Transformers_ Attention is all you need_files/polyfills-5cd94c89d3acac5f.js.download"></script><script defer="" src="./Transformers_ Attention is all you need_files/300-ee7e4fbc0e3f28a1.js.download"></script><script defer="" src="./Transformers_ Attention is all you need_files/426.d44afca4ecaa97bf.js.download"></script><script defer="" src="./Transformers_ Attention is all you need_files/522-4b14301de0a4a7e3.js.download"></script><script defer="" src="./Transformers_ Attention is all you need_files/757.b74614d32272db56.js.download"></script><script src="./Transformers_ Attention is all you need_files/webpack-c5ffbb45ea65c188.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/framework-8957c350a55da097.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/main-11d5d8729679f779.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/_app-5cd973b6eac37d86.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/675-d77a360e8fb3ed28.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/612-d212f4063eeab99e.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/942-86ae5f6e5ecc8f8d.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/228-40b75f3205f6fdac.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/361-279bef72286c1576.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/372-68a3991963fafe9e.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/[...slug]-5725853e34a82427.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/_buildManifest.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/_ssgManifest.js.download" defer=""></script><script src="./Transformers_ Attention is all you need_files/_middlewareManifest.js.download" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&amp;display=block">@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuOKfMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuLyfMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuI6fMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuGKYMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuFuYMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuDyYMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcCO3FwrK3iLTeHuS_fvQtMwCp50KnMw2boKoduKmMEVuBWYMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:700;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:800;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:900;font-display:block;src:url(https://fonts.gstatic.com/s/inter/v7/UcC73FwrK3iLTeHuS_fvQtMwCp50KnMa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style><style data-emotion="css-global" data-s=""></style><style data-emotion="css 3tbo75" data-s="">.css-3tbo75{line-height:1.5;--tw-bg-opacity:1;background-color:rgba(255, 255, 255, var(--tw-bg-opacity));}.dark .css-3tbo75{--tw-bg-opacity:1;background-color:rgba(24, 25, 26, var(--tw-bg-opacity));}</style><style data-emotion="css 3159x8" data-s="">.css-3159x8{--tw-bg-opacity:1;background-color:rgba(255, 255, 255, var(--tw-bg-opacity));}.dark .css-3159x8{--tw-bg-opacity:1;background-color:rgba(24, 25, 26, var(--tw-bg-opacity));}</style><style data-emotion="css qcu6iv" data-s="">.css-qcu6iv{position:relative;z-index:50;border-bottom-width:1px;}.dark .css-qcu6iv{--tw-border-opacity:1;border-color:rgba(31, 41, 55, var(--tw-border-opacity));}</style><style data-emotion="css yd8da5" data-s="">.css-yd8da5{padding-left:0.5rem;padding-right:0.5rem;margin-left:auto;margin-right:auto;}@media (min-width: 768px){.css-yd8da5{padding-left:1rem;padding-right:1rem;}}@media (min-width: 1536px){.css-yd8da5{padding-left:2.5rem;padding-right:2.5rem;}}</style><style data-emotion="css 1ll3e1i" data-s="">.css-1ll3e1i{position:relative;z-index:40;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;padding-top:2rem;padding-bottom:0.5rem;margin-bottom:0.5rem;}@media (min-width: 768px){.css-1ll3e1i{margin-bottom:1rem;}}</style><style data-emotion="css 37nvso" data-s="">.css-37nvso{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-top:0.25rem;padding-bottom:0.25rem;padding-right:1rem;}</style><style data-emotion="css 1lfhv42" data-s="">.css-1lfhv42{margin-right:1rem;}@media (min-width: 768px){.css-1lfhv42{display:none;}}</style><style data-emotion="css 1tdo4yy" data-s="">.css-1tdo4yy{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0.5rem;font-weight:700;text-transform:uppercase;border-radius:0.5rem;}.css-1tdo4yy:focus{outline:2px solid transparent;outline-offset:2px;}.dark .css-1tdo4yy{--tw-text-opacity:1;color:rgba(255, 255, 255, var(--tw-text-opacity));}.css-1tdo4yy:hover{background-color:rgba(255,255,255,.2);}</style><style data-emotion="css 176cdx5" data-s="">.css-176cdx5{width:1.5rem;height:1.5rem;margin-right:0.5rem;fill:currentColor;}</style><style data-emotion="css 1x6nj8n" data-s="">.css-1x6nj8n{width:1.5rem;height:1.5rem;margin-right:0.5rem;fill:currentColor;}</style><style data-emotion="css v1ygcs" data-s="">.css-v1ygcs{display:none;}@media (min-width: 768px){.css-v1ygcs{display:block;}}</style><style data-emotion="css 1b9oi20" data-s="">.css-1b9oi20{display:block;width:8rem;}@media (min-width: 768px){.css-1b9oi20{width:16rem;}}</style><style data-emotion="css 1082qq3" data-s="">.css-1082qq3{display:block;width:100%;}</style><style data-emotion="css 4pib4r" data-s="">.css-4pib4r{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:3rem;}.dark .css-4pib4r{--tw-text-opacity:1;color:rgba(255, 255, 255, var(--tw-text-opacity));}</style><style data-emotion="css wme9tu" data-s="">.css-wme9tu{transition-property:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);transition-duration:100ms;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:0.5rem;padding-right:0.5rem;padding-top:0.5rem;padding-bottom:0.5rem;margin-left:0.25rem;margin-right:0.25rem;font-weight:600;border-radius:9999px;}.css-wme9tu:focus{outline:2px solid transparent;outline-offset:2px;}.css-wme9tu:hover{background-color:rgba(0,0,0,.1);}.dark .css-wme9tu:hover{background-color:rgba(255,255,255,.2);}</style><style data-emotion="css 1fjoo34" data-s="">.css-1fjoo34{width:1.5rem;height:1.5rem;fill:currentColor;}</style><style data-emotion="css 17mdxl4" data-s="">.css-17mdxl4{width:1.5rem;height:1.5rem;fill:currentColor;}</style><style data-emotion="css 1g8oej1" data-s="">.css-1g8oej1{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:100%;padding-top:1rem;padding-bottom:1rem;}@media (min-width: 768px){.css-1g8oej1{display:none;}}</style><style data-emotion="css df1pn7" data-s="">.css-df1pn7{display:block;width:16rem;}</style><style data-emotion="css 1nl2bbl" data-s="">.css-1nl2bbl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:100%;overflow:hidden;font-size:1rem;line-height:1.5rem;font-weight:500;text-transform:uppercase;--tw-text-opacity:1;color:rgba(0, 0, 0, var(--tw-text-opacity));}@media (min-width: 768px){.css-1nl2bbl{-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}}.dark .css-1nl2bbl{--tw-text-opacity:1;color:rgba(255, 255, 255, var(--tw-text-opacity));}</style><style data-emotion="css ddbtpe" data-s="">.css-ddbtpe{overflow:hidden;display:none;height:100%;}@media (min-width: 768px){.css-ddbtpe{display:block;-webkit-flex:1 1 0%;-ms-flex:1 1 0%;flex:1 1 0%;}}</style><style data-emotion="css mq47qm" data-s="">.css-mq47qm{display:none;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex:1 1 0%;-ms-flex:1 1 0%;flex:1 1 0%;height:100%;font-size:0.875rem;line-height:1.25rem;white-space:nowrap;}@media (min-width: 768px){.css-mq47qm{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}</style><style data-emotion="css baperf" data-s="">.css-baperf{display:block;padding-top:0.75rem;padding-bottom:0.75rem;font-weight:600;transition-property:background-color,border-color,color,fill,stroke;transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);transition-duration:100ms;padding-left:0.75rem;padding-right:0.75rem;border-top-left-radius:0.5rem;border-top-right-radius:0.5rem;opacity:0.75;}.css-baperf:hover{opacity:1;}.css-baperf:hover{background-color:rgba(0,0,0,.1);}.dark .css-baperf:hover{background-color:rgba(255,255,255,.2);}</style><style data-emotion="css tr47eg" data-s="">.css-tr47eg{display:block;padding-top:0.75rem;padding-bottom:0.75rem;font-weight:600;transition-property:background-color,border-color,color,fill,stroke;transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);transition-duration:100ms;padding-left:0.75rem;padding-right:0.75rem;border-top-left-radius:0.5rem;border-top-right-radius:0.5rem;border-color:transparent;opacity:0.75;}.css-tr47eg:hover{opacity:1;}.css-tr47eg:hover{background-color:rgba(0,0,0,.1);}.dark .css-tr47eg:hover{background-color:rgba(255,255,255,.2);}</style><style data-emotion="css mwyikp" data-s="">.css-mwyikp{display:block;padding-top:0.75rem;padding-bottom:0.75rem;font-weight:600;transition-property:background-color,border-color,color,fill,stroke;transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);transition-duration:100ms;padding-left:0.75rem;padding-right:0.75rem;border-top-left-radius:0.5rem;border-top-right-radius:0.5rem;border-color:transparent;opacity:0.75;}.css-mwyikp:hover{opacity:1;}.css-mwyikp:hover{background-color:rgba(0,0,0,.1);}.dark .css-mwyikp:hover{background-color:rgba(255,255,255,.2);}</style><style data-emotion="css ho1qnd" data-s="">.css-ho1qnd{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}</style><style data-emotion="css eqcmow" data-s="">.css-eqcmow{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding-left:0.75rem;padding-right:0.75rem;padding-top:0.75rem;padding-bottom:0.75rem;transition-property:background-color,border-color,color,fill,stroke;transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);transition-duration:100ms;border-top-left-radius:0.5rem;border-top-right-radius:0.5rem;opacity:0.5;}.css-eqcmow:hover{opacity:1;}@media (min-width: 768px){.css-eqcmow{padding-left:0.5rem;padding-right:0.5rem;}}.css-eqcmow:hover{background-color:rgba(0,0,0,.1);}.dark .css-eqcmow:hover{background-color:rgba(255,255,255,.2);}</style><style data-emotion="css ms8fcx" data-s="">.css-ms8fcx{width:1.25rem;height:1.25rem;fill:currentColor;}</style><style data-emotion="css 1330i9l" data-s="">.css-1330i9l{width:1.25rem;height:1.25rem;fill:currentColor;}</style><style data-emotion="css 78a8kl" data-s="">.css-78a8kl{position:relative;z-index:40;}</style><style data-emotion="css 9slcbn" data-s="">.css-9slcbn{padding-bottom:6rem;}</style><style data-emotion="css m96uju" data-s="">.css-m96uju{position:relative;z-index:40;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="css 1ffmpwz" data-s="">.css-1ffmpwz{position:relative;z-index:20;padding-left:1rem;padding-right:1rem;margin-bottom:2.5rem;}@media (min-width: 768px){.css-1ffmpwz{z-index:10;}}@media (min-width: 1280px){.css-1ffmpwz{padding-left:0px;padding-right:3rem;}}</style><style data-emotion="css bjn8wh" data-s="">.css-bjn8wh{position:relative;}</style><style data-emotion="css 1jqt27n" data-s="">.css-1jqt27n{display:block;width:100%;margin-bottom:0px;}</style><style data-emotion="css 1rtko81" data-s="">.css-1rtko81{margin-top:2.5rem;line-height:1.75;letter-spacing:-0.025em;color:#111;max-width:65ch;font-size:1rem;overflow-wrap:break-word;margin-bottom:0.5rem;}.css-1rtko81 [class~="lead"]{color:#4b5563;font-size:1.25em;line-height:1.6;margin-top:1.2em;margin-bottom:1.2em;}.css-1rtko81 a{color:#111;-webkit-text-decoration:underline;text-decoration:underline;font-weight:500;}.css-1rtko81 a .user-mention{-webkit-text-decoration:none;text-decoration:none;color:#2962ff;}.css-1rtko81 strong{color:#111827;font-weight:600;}.css-1rtko81 ol[type="A"]{--list-counter-style:upper-alpha;}.css-1rtko81 ol[type="a"]{--list-counter-style:lower-alpha;}.css-1rtko81 ol[type="A s"]{--list-counter-style:upper-alpha;}.css-1rtko81 ol[type="a s"]{--list-counter-style:lower-alpha;}.css-1rtko81 ol[type="I"]{--list-counter-style:upper-roman;}.css-1rtko81 ol[type="i"]{--list-counter-style:lower-roman;}.css-1rtko81 ol[type="I s"]{--list-counter-style:upper-roman;}.css-1rtko81 ol[type="i s"]{--list-counter-style:lower-roman;}.css-1rtko81 ol[type="1"]{--list-counter-style:decimal;}.css-1rtko81 ol>li{position:relative;padding-left:1.75em;}.css-1rtko81 ol>li::before{content:counter(list-item, var(--list-counter-style, decimal)) ".";position:absolute;font-weight:400;color:#6b7280;left:0;}.css-1rtko81 ul>li{position:relative;padding-left:1.75em;}.css-1rtko81 ul>li::before{content:"";position:absolute;background-color:#d1d5db;border-radius:50%;width:0.375em;height:0.375em;top:calc(0.875em - 0.1875em);left:0.25em;}.css-1rtko81 hr{border-color:#e5e7eb;border-top-width:1px;margin-top:3em;margin-bottom:3em;}.css-1rtko81 blockquote{font-weight:500;font-style:italic;color:#111827;border-left-width:0.25rem;border-left-color:#e5e7eb;quotes:"\201C""\201D""\2018""\2019";margin-top:1.6em;margin-bottom:1.6em;padding-left:1em;}.css-1rtko81 blockquote p:first-of-type::before{content:"";}.css-1rtko81 blockquote p:last-of-type::after{content:"";}.css-1rtko81 h1{color:#111827;font-weight:800;font-size:2.25em;margin-top:0;margin-bottom:0.8888889em;line-height:1.1111111;}.css-1rtko81 h2{color:#111827;font-weight:700;font-size:1.5em;margin-top:2em;margin-bottom:1em;line-height:1.3333333;}.css-1rtko81 h3{color:#111827;font-weight:600;font-size:1.25em;margin-top:1.6em;margin-bottom:0.6em;line-height:1.6;}.css-1rtko81 h4{color:#111827;font-weight:600;margin-top:1.5em;margin-bottom:0.5em;line-height:1.5;}.css-1rtko81 figure figcaption{color:#6b7280;font-size:0.875em;line-height:1.4285714;margin-top:0.8571429em;}.css-1rtko81 code{color:#111827;font-weight:600;font-size:0.875em;}.css-1rtko81 code::after{display:none;content:"`";}.css-1rtko81 code::before{display:none;content:"`";}.css-1rtko81 a code{color:#111827;}.css-1rtko81 pre{color:#e5e7eb;background-color:#1f2937;overflow-x:auto;font-size:0.875em;line-height:1.7142857;margin-top:1.7142857em;margin-bottom:1.7142857em;border-radius:0.375rem;padding-top:0.8571429em;padding-right:1.1428571em;padding-bottom:0.8571429em;padding-left:1.1428571em;}.css-1rtko81 pre code{background-color:transparent;border-width:0;border-radius:0;padding:0;font-weight:400;color:inherit;font-size:inherit;font-family:inherit;line-height:inherit;}.css-1rtko81 pre code::before{content:none;}.css-1rtko81 pre code::after{content:none;}.css-1rtko81 table{width:100%;table-layout:auto;text-align:left;margin-top:2em;margin-bottom:2em;font-size:0.875em;line-height:1.7142857;}.css-1rtko81 thead{color:#111827;font-weight:600;border-bottom-width:1px;border-bottom-color:#d1d5db;}.css-1rtko81 thead th{vertical-align:bottom;padding-right:0.5714286em;padding-bottom:0.5714286em;padding-left:0.5714286em;}.css-1rtko81 tbody tr{border-bottom-width:1px;border-bottom-color:#e5e7eb;}.css-1rtko81 tbody tr:last-child{border-bottom-width:0;}.css-1rtko81 tbody td{vertical-align:top;padding-top:0.5714286em;padding-right:0.5714286em;padding-bottom:0.5714286em;padding-left:0.5714286em;}.css-1rtko81 p{margin-top:1.25em;margin-bottom:1.25em;}.css-1rtko81 img{margin-top:2em;margin-bottom:2em;}.css-1rtko81 video{margin-top:2em;margin-bottom:2em;}.css-1rtko81 figure{margin-top:2em;margin-bottom:2em;}.css-1rtko81 figure>*{margin-top:0;margin-bottom:0;}.css-1rtko81 h2 code{font-size:0.875em;}.css-1rtko81 h3 code{font-size:0.9em;}.css-1rtko81 ol{margin-top:1.25em;margin-bottom:1.25em;}.css-1rtko81 ul{margin-top:1.25em;margin-bottom:1.25em;}.css-1rtko81 li{margin-top:0.5em;margin-bottom:0.5em;}.css-1rtko81 >ul>li p{margin-top:0.75em;margin-bottom:0.75em;}.css-1rtko81 >ul>li>*:first-child{margin-top:1.25em;}.css-1rtko81 >ul>li>*:last-child{margin-bottom:1.25em;}.css-1rtko81 >ol>li>*:first-child{margin-top:1.25em;}.css-1rtko81 >ol>li>*:last-child{margin-bottom:1.25em;}.css-1rtko81 ul ul{margin-top:0.75em;margin-bottom:0.75em;}.css-1rtko81 ul ol{margin-top:0.75em;margin-bottom:0.75em;}.css-1rtko81 ol ul{margin-top:0.75em;margin-bottom:0.75em;}.css-1rtko81 ol ol{margin-top:0.75em;margin-bottom:0.75em;}.css-1rtko81 hr+*{margin-top:0;}.css-1rtko81 h2+*{margin-top:0;}.css-1rtko81 h3+*{margin-top:0;}.css-1rtko81 h4+*{margin-top:0;}.css-1rtko81 thead th:first-child{padding-left:0;}.css-1rtko81 thead th:last-child{padding-right:0;}.css-1rtko81 tbody td:first-child{padding-left:0;}.css-1rtko81 tbody td:last-child{padding-right:0;}.css-1rtko81 >:first-child{margin-top:0;}.css-1rtko81 >:last-child{margin-bottom:0;}.css-1rtko81 details{border:1px solid rgba(204, 214, 221, .5);border-radius:4px;padding:.5em .5em 0;}.css-1rtko81 summary{font-weight:bold;margin:-.5em -.5em 0;padding:.5em;}.css-1rtko81 kbd{background-color:#eee;border-radius:3px;border:1px solid #b4b4b4;box-shadow:0 1px 1px rgba(0, 0, 0, .2),0 2px 0 0 rgba(255, 255, 255, .7) inset;color:#333;display:inline-block;font-size:.85em;font-weight:700;line-weight:1;padding:2px 4px;white-space:nowrap;}.css-1rtko81 abbr{font-style:italic;}.css-1rtko81 dt{font-weight:bold;}.css-1rtko81 dd{margin-bottom:1rem;-webkit-margin-start:1rem;margin-inline-start:1rem;}.dark .css-1rtko81{color:#fafafa;}.dark .css-1rtko81 a{color:#fafafa;}.dark .css-1rtko81 kbd{background-color:#444;border:1px solid #757575;color:#FFF;}.dark .css-1rtko81 strong{color:#fafafa;}.dark .css-1rtko81 ol>li::before{color:#bdbdbd;}.dark .css-1rtko81 blockquote{color:#bdbdbd;}.dark .css-1rtko81 h1{color:#fafafa;}.dark .css-1rtko81 h2{color:#fafafa;}.dark .css-1rtko81 h3{color:#fafafa;}.dark .css-1rtko81 h4{color:#fafafa;}.dark .css-1rtko81 figure figcaption{color:#bdbdbd;}.dark .css-1rtko81 code{color:#fafafa;background-color:#1f2937;}.dark .css-1rtko81 a code{color:#fafafa;}.dark .css-1rtko81 pre{color:#fafafa;}.dark .css-1rtko81 pre code{color:#fafafa;background-color:transparent;}.dark .css-1rtko81 thead{color:#fafafa;}@media (min-width: 1024px){.css-1rtko81{font-size:1.125rem;line-height:1.7777778;}.css-1rtko81 p{margin-top:1.3333333em;margin-bottom:1.3333333em;}.css-1rtko81 [class~="lead"]{font-size:1.2222222em;line-height:1.4545455;margin-top:1.0909091em;margin-bottom:1.0909091em;}.css-1rtko81 blockquote{margin-top:1.6666667em;margin-bottom:1.6666667em;padding-left:1em;}.css-1rtko81 h1{font-size:1.8em;margin-top:0;margin-bottom:0.8333333em;line-height:1.2;}.css-1rtko81 h2{font-size:1.6666667em;margin-top:1.8666667em;margin-bottom:1.0666667em;line-height:1.3333333;}.css-1rtko81 h3{font-size:1.3333333em;margin-top:1.6666667em;margin-bottom:0.6666667em;line-height:1.5;}.css-1rtko81 h4{margin-top:1.7777778em;margin-bottom:0.4444444em;line-height:1.5555556;}.css-1rtko81 img{margin-top:1.7777778em;margin-bottom:1.7777778em;}.css-1rtko81 video{margin-top:1.7777778em;margin-bottom:1.7777778em;}.css-1rtko81 figure{margin-top:1.7777778em;margin-bottom:1.7777778em;}.css-1rtko81 figure>*{margin-top:0;margin-bottom:0;}.css-1rtko81 figure figcaption{font-size:0.8888889em;line-height:1.5;margin-top:1em;}.css-1rtko81 code{font-size:0.8888889em;}.css-1rtko81 h2 code{font-size:0.8666667em;}.css-1rtko81 h3 code{font-size:0.875em;}.css-1rtko81 pre{font-size:0.8888889em;line-height:1.75;margin-top:2em;margin-bottom:2em;border-radius:0.375rem;padding-top:1em;padding-right:1.5em;padding-bottom:1em;padding-left:1.5em;}.css-1rtko81 ol{margin-top:1.3333333em;margin-bottom:1.3333333em;}.css-1rtko81 ul{margin-top:1.3333333em;margin-bottom:1.3333333em;}.css-1rtko81 li{margin-top:0.6666667em;margin-bottom:0.6666667em;}.css-1rtko81 ol>li{padding-left:1.6666667em;}.css-1rtko81 ol>li::before{left:0;}.css-1rtko81 ul>li{padding-left:1.6666667em;}.css-1rtko81 ul>li::before{width:0.3333333em;height:0.3333333em;top:calc(0.8888889em - 0.1666667em);left:0.2222222em;}.css-1rtko81 >ul>li p{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1rtko81 >ul>li>*:first-child{margin-top:1.3333333em;}.css-1rtko81 >ul>li>*:last-child{margin-bottom:1.3333333em;}.css-1rtko81 >ol>li>*:first-child{margin-top:1.3333333em;}.css-1rtko81 >ol>li>*:last-child{margin-bottom:1.3333333em;}.css-1rtko81 ul ul{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1rtko81 ul ol{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1rtko81 ol ul{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1rtko81 ol ol{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1rtko81 hr{margin-top:3.1111111em;margin-bottom:3.1111111em;}.css-1rtko81 hr+*{margin-top:0;}.css-1rtko81 h2+*{margin-top:0;}.css-1rtko81 h3+*{margin-top:0;}.css-1rtko81 h4+*{margin-top:0;}.css-1rtko81 table{font-size:0.8888889em;line-height:1.5;}.css-1rtko81 thead th{padding-right:0.75em;padding-bottom:0.75em;padding-left:0.75em;}.css-1rtko81 thead th:first-child{padding-left:0;}.css-1rtko81 thead th:last-child{padding-right:0;}.css-1rtko81 tbody td{padding-top:0.75em;padding-right:0.75em;padding-bottom:0.75em;padding-left:0.75em;}.css-1rtko81 tbody td:first-child{padding-left:0;}.css-1rtko81 tbody td:last-child{padding-right:0;}.css-1rtko81 >:first-child{margin-top:0;}.css-1rtko81 >:last-child{margin-bottom:0;}}@media (min-width: 1280px){.css-1rtko81{font-size:1.25rem;line-height:1.8;}.css-1rtko81 p{margin-top:1.2em;margin-bottom:1.2em;}.css-1rtko81 [class~="lead"]{font-size:1.2em;line-height:1.5;margin-top:1em;margin-bottom:1em;}.css-1rtko81 blockquote{margin-top:1.6em;margin-bottom:1.6em;padding-left:1.0666667em;}.css-1rtko81 h1{font-size:2em;margin-top:0;margin-bottom:0.8571429em;line-height:1.2;}.css-1rtko81 h2{font-size:1.8em;margin-top:1.5555556em;margin-bottom:0.8888889em;line-height:1.1111111;}.css-1rtko81 h3{font-size:1.5em;margin-top:1.6em;margin-bottom:0.6666667em;line-height:1.3333333;}.css-1rtko81 h4{margin-top:1.8em;margin-bottom:0.6em;line-height:1.6;}.css-1rtko81 img{margin-top:2em;margin-bottom:2em;}.css-1rtko81 video{margin-top:2em;margin-bottom:2em;}.css-1rtko81 figure{margin-top:2em;margin-bottom:2em;}.css-1rtko81 figure>*{margin-top:0;margin-bottom:0;}.css-1rtko81 figure figcaption{font-size:0.9em;line-height:1.5555556;margin-top:1em;}.css-1rtko81 code{font-size:0.9em;}.css-1rtko81 h2 code{font-size:0.8611111em;}.css-1rtko81 h3 code{font-size:0.9em;}.css-1rtko81 pre{font-size:0.9em;line-height:1.7777778;margin-top:2em;margin-bottom:2em;border-radius:0.5rem;padding-top:1.1111111em;padding-right:1.3333333em;padding-bottom:1.1111111em;padding-left:1.3333333em;}.css-1rtko81 ol{margin-top:1.2em;margin-bottom:1.2em;}.css-1rtko81 ul{margin-top:1.2em;margin-bottom:1.2em;}.css-1rtko81 li{margin-top:0.6em;margin-bottom:0.6em;}.css-1rtko81 ol>li{padding-left:1.8em;}.css-1rtko81 ol>li::before{left:0;}.css-1rtko81 ul>li{padding-left:1.8em;}.css-1rtko81 ul>li::before{width:0.35em;height:0.35em;top:calc(0.9em - 0.175em);left:0.25em;}.css-1rtko81 >ul>li p{margin-top:0.8em;margin-bottom:0.8em;}.css-1rtko81 >ul>li>*:first-child{margin-top:1.2em;}.css-1rtko81 >ul>li>*:last-child{margin-bottom:1.2em;}.css-1rtko81 >ol>li>*:first-child{margin-top:1.2em;}.css-1rtko81 >ol>li>*:last-child{margin-bottom:1.2em;}.css-1rtko81 ul ul{margin-top:0.8em;margin-bottom:0.8em;}.css-1rtko81 ul ol{margin-top:0.8em;margin-bottom:0.8em;}.css-1rtko81 ol ul{margin-top:0.8em;margin-bottom:0.8em;}.css-1rtko81 ol ol{margin-top:0.8em;margin-bottom:0.8em;}.css-1rtko81 hr{margin-top:2.8em;margin-bottom:2.8em;}.css-1rtko81 hr+*{margin-top:0;}.css-1rtko81 h2+*{margin-top:0;}.css-1rtko81 h3+*{margin-top:0;}.css-1rtko81 h4+*{margin-top:0;}.css-1rtko81 table{font-size:0.9em;line-height:1.5555556;}.css-1rtko81 thead th{padding-right:0.6666667em;padding-bottom:0.8888889em;padding-left:0.6666667em;}.css-1rtko81 thead th:first-child{padding-left:0;}.css-1rtko81 thead th:last-child{padding-right:0;}.css-1rtko81 tbody td{padding-top:0.8888889em;padding-right:0.6666667em;padding-bottom:0.8888889em;padding-left:0.6666667em;}.css-1rtko81 tbody td:first-child{padding-left:0;}.css-1rtko81 tbody td:last-child{padding-right:0;}.css-1rtko81 >:first-child{margin-top:0;}.css-1rtko81 >:last-child{margin-bottom:0;}}</style><style data-emotion="css hj8uh5" data-s="">.css-hj8uh5{margin-bottom:2.5rem;}</style><style data-emotion="css n9m7tc" data-s="">.css-n9m7tc{font-size:1.5rem;line-height:1.375;letter-spacing:-0.025em;--tw-text-opacity:1;color:rgba(97, 97, 97, var(--tw-text-opacity));}.dark .css-n9m7tc{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}</style><style data-emotion="css zd4cjh" data-s="">.css-zd4cjh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-bottom:1.25rem;margin-top:-1.25rem;}</style><style data-emotion="css 1pt9cni" data-s="">.css-1pt9cni{position:relative;z-index:10;padding-top:1.25rem;padding-bottom:1.25rem;margin-bottom:1.25rem;border-top-width:1px;border-bottom-width:1px;}.dark .css-1pt9cni{--tw-border-opacity:1;border-color:rgba(66, 66, 66, var(--tw-border-opacity));}@media (min-width: 768px){.css-1pt9cni{margin-bottom:2.5rem;}}</style><style data-emotion="css 4c6dm7" data-s="">.css-4c6dm7{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;}</style><style data-emotion="css 12sdugl" data-s="">.css-12sdugl{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;}@media (min-width: 768px){.css-12sdugl{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}}</style><style data-emotion="css nbo283" data-s="">.css-nbo283{width:3rem;height:3rem;margin-right:0.5rem;overflow:hidden;border-radius:9999px;}@media (min-width: 768px){.css-nbo283{width:4rem;height:4rem;margin-right:1.25rem;}}</style><style data-emotion="css 34p3w6" data-s="">.css-34p3w6{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-34p3w6{--tw-bg-opacity:1;background-color:rgba(24, 25, 26, var(--tw-bg-opacity));}</style><style data-emotion="css l9iun9" data-s="">.css-l9iun9{position:relative;display:block;width:100%;height:100%;}</style><style data-emotion="css 1yapcyz" data-s="">.css-1yapcyz{position:relative;z-index:20;display:block;width:100%;border-radius:9999px;}</style><style data-emotion="css 1k5pqev" data-s="">.css-1k5pqev{line-height:1.375;}</style><style data-emotion="css 51a556" data-s="">.css-51a556{margin-bottom:0.25rem;font-size:1.125rem;line-height:1.75rem;font-weight:700;--tw-text-opacity:1;color:rgba(51, 51, 51, var(--tw-text-opacity));}.dark .css-51a556{--tw-text-opacity:1;color:rgba(255, 255, 255, var(--tw-text-opacity));}</style><style data-emotion="css 1dyl29k" data-s="">.css-1dyl29k{margin-bottom:0.5rem;font-size:0.875rem;line-height:1.25rem;--tw-text-opacity:1;color:rgba(117, 117, 117, var(--tw-text-opacity));}.dark .css-1dyl29k{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}</style><style data-emotion="css 4zleql" data-s="">.css-4zleql{display:block;}</style><style data-emotion="css 16ceglb" data-s="">.css-16ceglb{font-weight:600;}</style><style data-emotion="css fhxb3m" data-s="">.css-fhxb3m{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css tb6s22" data-s="">.css-tb6s22{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:0.875rem;line-height:1.25rem;font-weight:500;--tw-text-opacity:1;color:rgba(117, 117, 117, var(--tw-text-opacity));}.dark .css-tb6s22{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}</style><style data-emotion="css 1vlfi6p" data-s="">.css-1vlfi6p{width:1rem;height:1rem;margin-right:0.25rem;fill:currentColor;}</style><style data-emotion="css hb3dfw" data-s="">.css-hb3dfw{width:1rem;height:1rem;margin-right:0.25rem;fill:currentColor;}</style><style data-emotion="css 1w7f2ot" data-s="">.css-1w7f2ot{padding:1rem;margin-bottom:1.25rem;border-width:1px;border-radius:0.5rem;--tw-bg-opacity:1;background-color:rgba(245, 247, 250, var(--tw-bg-opacity));}.dark .css-1w7f2ot{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));--tw-border-opacity:1;border-color:rgba(66, 66, 66, var(--tw-border-opacity));}@media (min-width: 768px){.css-1w7f2ot{padding:1.5rem;}}</style><style data-emotion="css vabva8" data-s="">.css-vabva8{position:relative;overflow:hidden;}</style><style data-emotion="css 1kuuouk" data-s="">.css-1kuuouk{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;}</style><style data-emotion="css b00jk2" data-s="">.css-b00jk2{width:100%;margin-bottom:0.75rem;margin-right:0.5rem;}@media (min-width: 1280px){.css-b00jk2{margin-bottom:0px;width:16rem;}}</style><style data-emotion="css b2cmdi" data-s="">.css-b2cmdi{font-weight:500;line-height:1.375;letter-spacing:-0.025em;--tw-text-opacity:1;color:rgba(66, 66, 66, var(--tw-text-opacity));}.dark .css-b2cmdi{--tw-text-opacity:1;color:rgba(156, 163, 175, var(--tw-text-opacity));}</style><style data-emotion="css sgivcr" data-s="">.css-sgivcr{position:relative;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;}@media (min-width: 1280px){.css-sgivcr{width:auto;-webkit-flex:1 1 0%;-ms-flex:1 1 0%;flex:1 1 0%;}}</style><style data-emotion="css 248rth" data-s="">.css-248rth{position:absolute;top:0px;left:0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;margin-left:1rem;--tw-text-opacity:1;color:rgba(117, 117, 117, var(--tw-text-opacity));}.dark .css-248rth{--tw-text-opacity:1;color:rgba(158, 158, 158, var(--tw-text-opacity));}</style><style data-emotion="css 148gys9" data-s="">.css-148gys9{width:100%;padding:1rem;background-color:transparent;border-width:1px;border-radius:0.5rem;outline:2px solid transparent;outline-offset:2px;width:100%;padding:1rem;padding-left:3rem;padding-right:8rem;font-size:0.875rem;line-height:1.25rem;font-weight:600;--tw-bg-opacity:1;background-color:rgba(255, 255, 255, var(--tw-bg-opacity));border-width:1px;border-radius:0.5rem;outline:2px solid transparent;outline-offset:2px;}.css-148gys9::-webkit-input-placeholder{--tw-placeholder-opacity:1;color:rgba(107, 114, 128, var(--tw-placeholder-opacity));}.css-148gys9::-moz-placeholder{--tw-placeholder-opacity:1;color:rgba(107, 114, 128, var(--tw-placeholder-opacity));}.css-148gys9:-ms-input-placeholder{--tw-placeholder-opacity:1;color:rgba(107, 114, 128, var(--tw-placeholder-opacity));}.css-148gys9::placeholder{--tw-placeholder-opacity:1;color:rgba(107, 114, 128, var(--tw-placeholder-opacity));}.css-148gys9:focus{--tw-border-opacity:1;border-color:rgba(41, 98, 255, var(--tw-border-opacity));}.css-148gys9:disabled{--tw-bg-opacity:1;background-color:rgba(250, 250, 250, var(--tw-bg-opacity));}.dark .css-148gys9{--tw-text-opacity:1;color:rgba(255, 255, 255, var(--tw-text-opacity));--tw-border-opacity:1;border-color:rgba(66, 66, 66, var(--tw-border-opacity));}.dark .css-148gys9:focus{--tw-bg-opacity:1;background-color:rgba(24, 25, 26, var(--tw-bg-opacity));--tw-border-opacity:1;border-color:rgba(41, 98, 255, var(--tw-border-opacity));}.dark .css-148gys9{--tw-bg-opacity:1;background-color:rgba(24, 25, 26, var(--tw-bg-opacity));}@media (min-width: 768px){.css-148gys9{font-size:1rem;line-height:1.5rem;}}</style><style data-emotion="css wswes9" data-s="">.css-wswes9{position:absolute;top:0px;right:0px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:100%;margin-right:0.5rem;}</style><style data-emotion="css 6dz9u5" data-s="">.css-6dz9u5{padding-left:1rem;padding-right:1rem;padding-top:0.75rem;padding-bottom:0.75rem;font-weight:700;line-height:1.375;text-transform:uppercase;--tw-text-opacity:1;color:rgba(41, 98, 255, var(--tw-text-opacity));}.dark .css-6dz9u5{--tw-text-opacity:1;color:rgba(41, 98, 255, var(--tw-text-opacity));}</style><style data-emotion="css azbqzp" data-s="">.css-azbqzp{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:0.5rem;padding-right:0.5rem;font-size:0.875rem;line-height:1.25rem;padding-left:1rem;padding-right:1rem;padding-top:0.75rem;padding-bottom:0.75rem;font-weight:700;line-height:1.375;text-transform:uppercase;--tw-text-opacity:1;color:rgba(41, 98, 255, var(--tw-text-opacity));}.dark .css-azbqzp{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-azbqzp:disabled{opacity:0.5;}.css-azbqzp:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-azbqzp:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-azbqzp:focus{outline:2px solid transparent;outline-offset:2px;}.dark .css-azbqzp{--tw-text-opacity:1;color:rgba(41, 98, 255, var(--tw-text-opacity));}</style><style data-emotion="css 1azsmog" data-s="">.css-1azsmog{margin-left:auto;margin-right:auto;margin-bottom:2.5rem;letter-spacing:-0.025em;color:#111;max-width:65ch;font-size:1rem;line-height:1.75;overflow-wrap:break-word;min-height:30vh;}.css-1azsmog [class~="lead"]{color:#4b5563;font-size:1.25em;line-height:1.6;margin-top:1.2em;margin-bottom:1.2em;}.css-1azsmog a{color:#111;-webkit-text-decoration:underline;text-decoration:underline;font-weight:500;}.css-1azsmog a .user-mention{-webkit-text-decoration:none;text-decoration:none;color:#2962ff;}.css-1azsmog strong{color:#111827;font-weight:600;}.css-1azsmog ol[type="A"]{--list-counter-style:upper-alpha;}.css-1azsmog ol[type="a"]{--list-counter-style:lower-alpha;}.css-1azsmog ol[type="A s"]{--list-counter-style:upper-alpha;}.css-1azsmog ol[type="a s"]{--list-counter-style:lower-alpha;}.css-1azsmog ol[type="I"]{--list-counter-style:upper-roman;}.css-1azsmog ol[type="i"]{--list-counter-style:lower-roman;}.css-1azsmog ol[type="I s"]{--list-counter-style:upper-roman;}.css-1azsmog ol[type="i s"]{--list-counter-style:lower-roman;}.css-1azsmog ol[type="1"]{--list-counter-style:decimal;}.css-1azsmog ol>li{position:relative;padding-left:1.75em;}.css-1azsmog ol>li::before{content:counter(list-item, var(--list-counter-style, decimal)) ".";position:absolute;font-weight:400;color:#6b7280;left:0;}.css-1azsmog ul>li{position:relative;padding-left:1.75em;}.css-1azsmog ul>li::before{content:"";position:absolute;background-color:#d1d5db;border-radius:50%;width:0.375em;height:0.375em;top:calc(0.875em - 0.1875em);left:0.25em;}.css-1azsmog hr{border-color:#e5e7eb;border-top-width:1px;margin-top:3em;margin-bottom:3em;}.css-1azsmog blockquote{font-weight:500;font-style:italic;color:#111827;border-left-width:0.25rem;border-left-color:#e5e7eb;quotes:"\201C""\201D""\2018""\2019";margin-top:1.6em;margin-bottom:1.6em;padding-left:1em;}.css-1azsmog blockquote p:first-of-type::before{content:"";}.css-1azsmog blockquote p:last-of-type::after{content:"";}.css-1azsmog h1{color:#111827;font-weight:800;font-size:2.25em;margin-top:0;margin-bottom:0.8888889em;line-height:1.1111111;}.css-1azsmog h2{color:#111827;font-weight:700;font-size:1.5em;margin-top:2em;margin-bottom:1em;line-height:1.3333333;}.css-1azsmog h3{color:#111827;font-weight:600;font-size:1.25em;margin-top:1.6em;margin-bottom:0.6em;line-height:1.6;}.css-1azsmog h4{color:#111827;font-weight:600;margin-top:1.5em;margin-bottom:0.5em;line-height:1.5;}.css-1azsmog figure figcaption{color:#6b7280;font-size:0.875em;line-height:1.4285714;margin-top:0.8571429em;}.css-1azsmog code{color:#111827;font-weight:600;font-size:0.875em;}.css-1azsmog code::after{display:none;content:"`";}.css-1azsmog code::before{display:none;content:"`";}.css-1azsmog a code{color:#111827;}.css-1azsmog pre{color:#e5e7eb;background-color:#1f2937;overflow-x:auto;font-size:0.875em;line-height:1.7142857;margin-top:1.7142857em;margin-bottom:1.7142857em;border-radius:0.375rem;padding-top:0.8571429em;padding-right:1.1428571em;padding-bottom:0.8571429em;padding-left:1.1428571em;}.css-1azsmog pre code{background-color:transparent;border-width:0;border-radius:0;padding:0;font-weight:400;color:inherit;font-size:inherit;font-family:inherit;line-height:inherit;}.css-1azsmog pre code::before{content:none;}.css-1azsmog pre code::after{content:none;}.css-1azsmog table{width:100%;table-layout:auto;text-align:left;margin-top:2em;margin-bottom:2em;font-size:0.875em;line-height:1.7142857;}.css-1azsmog thead{color:#111827;font-weight:600;border-bottom-width:1px;border-bottom-color:#d1d5db;}.css-1azsmog thead th{vertical-align:bottom;padding-right:0.5714286em;padding-bottom:0.5714286em;padding-left:0.5714286em;}.css-1azsmog tbody tr{border-bottom-width:1px;border-bottom-color:#e5e7eb;}.css-1azsmog tbody tr:last-child{border-bottom-width:0;}.css-1azsmog tbody td{vertical-align:top;padding-top:0.5714286em;padding-right:0.5714286em;padding-bottom:0.5714286em;padding-left:0.5714286em;}.css-1azsmog p{margin-top:1.25em;margin-bottom:1.25em;}.css-1azsmog img{margin-top:2em;margin-bottom:2em;}.css-1azsmog video{margin-top:2em;margin-bottom:2em;}.css-1azsmog figure{margin-top:2em;margin-bottom:2em;}.css-1azsmog figure>*{margin-top:0;margin-bottom:0;}.css-1azsmog h2 code{font-size:0.875em;}.css-1azsmog h3 code{font-size:0.9em;}.css-1azsmog ol{margin-top:1.25em;margin-bottom:1.25em;}.css-1azsmog ul{margin-top:1.25em;margin-bottom:1.25em;}.css-1azsmog li{margin-top:0.5em;margin-bottom:0.5em;}.css-1azsmog >ul>li p{margin-top:0.75em;margin-bottom:0.75em;}.css-1azsmog >ul>li>*:first-child{margin-top:1.25em;}.css-1azsmog >ul>li>*:last-child{margin-bottom:1.25em;}.css-1azsmog >ol>li>*:first-child{margin-top:1.25em;}.css-1azsmog >ol>li>*:last-child{margin-bottom:1.25em;}.css-1azsmog ul ul{margin-top:0.75em;margin-bottom:0.75em;}.css-1azsmog ul ol{margin-top:0.75em;margin-bottom:0.75em;}.css-1azsmog ol ul{margin-top:0.75em;margin-bottom:0.75em;}.css-1azsmog ol ol{margin-top:0.75em;margin-bottom:0.75em;}.css-1azsmog hr+*{margin-top:0;}.css-1azsmog h2+*{margin-top:0;}.css-1azsmog h3+*{margin-top:0;}.css-1azsmog h4+*{margin-top:0;}.css-1azsmog thead th:first-child{padding-left:0;}.css-1azsmog thead th:last-child{padding-right:0;}.css-1azsmog tbody td:first-child{padding-left:0;}.css-1azsmog tbody td:last-child{padding-right:0;}.css-1azsmog >:first-child{margin-top:0;}.css-1azsmog >:last-child{margin-bottom:0;}.css-1azsmog details{border:1px solid rgba(204, 214, 221, .5);border-radius:4px;padding:.5em .5em 0;}.css-1azsmog summary{font-weight:bold;margin:-.5em -.5em 0;padding:.5em;}.css-1azsmog kbd{background-color:#eee;border-radius:3px;border:1px solid #b4b4b4;box-shadow:0 1px 1px rgba(0, 0, 0, .2),0 2px 0 0 rgba(255, 255, 255, .7) inset;color:#333;display:inline-block;font-size:.85em;font-weight:700;line-weight:1;padding:2px 4px;white-space:nowrap;}.css-1azsmog abbr{font-style:italic;}.css-1azsmog dt{font-weight:bold;}.css-1azsmog dd{margin-bottom:1rem;-webkit-margin-start:1rem;margin-inline-start:1rem;}.dark .css-1azsmog{color:#fafafa;}.dark .css-1azsmog a{color:#fafafa;}.dark .css-1azsmog kbd{background-color:#444;border:1px solid #757575;color:#FFF;}.dark .css-1azsmog strong{color:#fafafa;}.dark .css-1azsmog ol>li::before{color:#bdbdbd;}.dark .css-1azsmog blockquote{color:#bdbdbd;}.dark .css-1azsmog h1{color:#fafafa;}.dark .css-1azsmog h2{color:#fafafa;}.dark .css-1azsmog h3{color:#fafafa;}.dark .css-1azsmog h4{color:#fafafa;}.dark .css-1azsmog figure figcaption{color:#bdbdbd;}.dark .css-1azsmog code{color:#fafafa;background-color:#1f2937;}.dark .css-1azsmog a code{color:#fafafa;}.dark .css-1azsmog pre{color:#fafafa;}.dark .css-1azsmog pre code{color:#fafafa;background-color:transparent;}.dark .css-1azsmog thead{color:#fafafa;}@media (min-width: 1024px){.css-1azsmog{font-size:1.125rem;line-height:1.7777778;}.css-1azsmog p{margin-top:1.3333333em;margin-bottom:1.3333333em;}.css-1azsmog [class~="lead"]{font-size:1.2222222em;line-height:1.4545455;margin-top:1.0909091em;margin-bottom:1.0909091em;}.css-1azsmog blockquote{margin-top:1.6666667em;margin-bottom:1.6666667em;padding-left:1em;}.css-1azsmog h1{font-size:1.8em;margin-top:0;margin-bottom:0.8333333em;line-height:1.2;}.css-1azsmog h2{font-size:1.6666667em;margin-top:1.8666667em;margin-bottom:1.0666667em;line-height:1.3333333;}.css-1azsmog h3{font-size:1.3333333em;margin-top:1.6666667em;margin-bottom:0.6666667em;line-height:1.5;}.css-1azsmog h4{margin-top:1.7777778em;margin-bottom:0.4444444em;line-height:1.5555556;}.css-1azsmog img{margin-top:1.7777778em;margin-bottom:1.7777778em;}.css-1azsmog video{margin-top:1.7777778em;margin-bottom:1.7777778em;}.css-1azsmog figure{margin-top:1.7777778em;margin-bottom:1.7777778em;}.css-1azsmog figure>*{margin-top:0;margin-bottom:0;}.css-1azsmog figure figcaption{font-size:0.8888889em;line-height:1.5;margin-top:1em;}.css-1azsmog code{font-size:0.8888889em;}.css-1azsmog h2 code{font-size:0.8666667em;}.css-1azsmog h3 code{font-size:0.875em;}.css-1azsmog pre{font-size:0.8888889em;line-height:1.75;margin-top:2em;margin-bottom:2em;border-radius:0.375rem;padding-top:1em;padding-right:1.5em;padding-bottom:1em;padding-left:1.5em;}.css-1azsmog ol{margin-top:1.3333333em;margin-bottom:1.3333333em;}.css-1azsmog ul{margin-top:1.3333333em;margin-bottom:1.3333333em;}.css-1azsmog li{margin-top:0.6666667em;margin-bottom:0.6666667em;}.css-1azsmog ol>li{padding-left:1.6666667em;}.css-1azsmog ol>li::before{left:0;}.css-1azsmog ul>li{padding-left:1.6666667em;}.css-1azsmog ul>li::before{width:0.3333333em;height:0.3333333em;top:calc(0.8888889em - 0.1666667em);left:0.2222222em;}.css-1azsmog >ul>li p{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1azsmog >ul>li>*:first-child{margin-top:1.3333333em;}.css-1azsmog >ul>li>*:last-child{margin-bottom:1.3333333em;}.css-1azsmog >ol>li>*:first-child{margin-top:1.3333333em;}.css-1azsmog >ol>li>*:last-child{margin-bottom:1.3333333em;}.css-1azsmog ul ul{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1azsmog ul ol{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1azsmog ol ul{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1azsmog ol ol{margin-top:0.8888889em;margin-bottom:0.8888889em;}.css-1azsmog hr{margin-top:3.1111111em;margin-bottom:3.1111111em;}.css-1azsmog hr+*{margin-top:0;}.css-1azsmog h2+*{margin-top:0;}.css-1azsmog h3+*{margin-top:0;}.css-1azsmog h4+*{margin-top:0;}.css-1azsmog table{font-size:0.8888889em;line-height:1.5;}.css-1azsmog thead th{padding-right:0.75em;padding-bottom:0.75em;padding-left:0.75em;}.css-1azsmog thead th:first-child{padding-left:0;}.css-1azsmog thead th:last-child{padding-right:0;}.css-1azsmog tbody td{padding-top:0.75em;padding-right:0.75em;padding-bottom:0.75em;padding-left:0.75em;}.css-1azsmog tbody td:first-child{padding-left:0;}.css-1azsmog tbody td:last-child{padding-right:0;}.css-1azsmog >:first-child{margin-top:0;}.css-1azsmog >:last-child{margin-bottom:0;}}@media (min-width: 1280px){.css-1azsmog{font-size:1.25rem;line-height:1.8;}.css-1azsmog p{margin-top:1.2em;margin-bottom:1.2em;}.css-1azsmog [class~="lead"]{font-size:1.2em;line-height:1.5;margin-top:1em;margin-bottom:1em;}.css-1azsmog blockquote{margin-top:1.6em;margin-bottom:1.6em;padding-left:1.0666667em;}.css-1azsmog h1{font-size:2em;margin-top:0;margin-bottom:0.8571429em;line-height:1.2;}.css-1azsmog h2{font-size:1.8em;margin-top:1.5555556em;margin-bottom:0.8888889em;line-height:1.1111111;}.css-1azsmog h3{font-size:1.5em;margin-top:1.6em;margin-bottom:0.6666667em;line-height:1.3333333;}.css-1azsmog h4{margin-top:1.8em;margin-bottom:0.6em;line-height:1.6;}.css-1azsmog img{margin-top:2em;margin-bottom:2em;}.css-1azsmog video{margin-top:2em;margin-bottom:2em;}.css-1azsmog figure{margin-top:2em;margin-bottom:2em;}.css-1azsmog figure>*{margin-top:0;margin-bottom:0;}.css-1azsmog figure figcaption{font-size:0.9em;line-height:1.5555556;margin-top:1em;}.css-1azsmog code{font-size:0.9em;}.css-1azsmog h2 code{font-size:0.8611111em;}.css-1azsmog h3 code{font-size:0.9em;}.css-1azsmog pre{font-size:0.9em;line-height:1.7777778;margin-top:2em;margin-bottom:2em;border-radius:0.5rem;padding-top:1.1111111em;padding-right:1.3333333em;padding-bottom:1.1111111em;padding-left:1.3333333em;}.css-1azsmog ol{margin-top:1.2em;margin-bottom:1.2em;}.css-1azsmog ul{margin-top:1.2em;margin-bottom:1.2em;}.css-1azsmog li{margin-top:0.6em;margin-bottom:0.6em;}.css-1azsmog ol>li{padding-left:1.8em;}.css-1azsmog ol>li::before{left:0;}.css-1azsmog ul>li{padding-left:1.8em;}.css-1azsmog ul>li::before{width:0.35em;height:0.35em;top:calc(0.9em - 0.175em);left:0.25em;}.css-1azsmog >ul>li p{margin-top:0.8em;margin-bottom:0.8em;}.css-1azsmog >ul>li>*:first-child{margin-top:1.2em;}.css-1azsmog >ul>li>*:last-child{margin-bottom:1.2em;}.css-1azsmog >ol>li>*:first-child{margin-top:1.2em;}.css-1azsmog >ol>li>*:last-child{margin-bottom:1.2em;}.css-1azsmog ul ul{margin-top:0.8em;margin-bottom:0.8em;}.css-1azsmog ul ol{margin-top:0.8em;margin-bottom:0.8em;}.css-1azsmog ol ul{margin-top:0.8em;margin-bottom:0.8em;}.css-1azsmog ol ol{margin-top:0.8em;margin-bottom:0.8em;}.css-1azsmog hr{margin-top:2.8em;margin-bottom:2.8em;}.css-1azsmog hr+*{margin-top:0;}.css-1azsmog h2+*{margin-top:0;}.css-1azsmog h3+*{margin-top:0;}.css-1azsmog h4+*{margin-top:0;}.css-1azsmog table{font-size:0.9em;line-height:1.5555556;}.css-1azsmog thead th{padding-right:0.6666667em;padding-bottom:0.8888889em;padding-left:0.6666667em;}.css-1azsmog thead th:first-child{padding-left:0;}.css-1azsmog thead th:last-child{padding-right:0;}.css-1azsmog tbody td{padding-top:0.8888889em;padding-right:0.6666667em;padding-bottom:0.8888889em;padding-left:0.6666667em;}.css-1azsmog tbody td:first-child{padding-left:0;}.css-1azsmog tbody td:last-child{padding-right:0;}.css-1azsmog >:first-child{margin-top:0;}.css-1azsmog >:last-child{margin-bottom:0;}}</style><style data-emotion="css 12n6jp" data-s="">.css-12n6jp{-webkit-flex:1 1 0%;-ms-flex:1 1 0%;flex:1 1 0%;}</style><style data-emotion="css g5d1mf" data-s="">.css-g5d1mf{position:relative;z-index:10;width:100%;}@media (min-width: 768px){.css-g5d1mf{z-index:20;}}@media (min-width: 1280px){.css-g5d1mf{width:10rem;height:100%;}}</style><style data-emotion="css 1qz4saa" data-s="">.css-1qz4saa{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;width:100%;padding-bottom:2.5rem;}@media (min-width: 1280px){.css-1qz4saa{padding-bottom:0px;padding-top:4rem;-webkit-box-pack:start;-ms-flex-pack:start;-webkit-justify-content:flex-start;justify-content:flex-start;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}}</style><style data-emotion="css sm43i1" data-s="">.css-sm43i1{z-index:20;width:100%;}@media (min-width: 1280px){.css-sm43i1{z-index:30;}}</style><style data-emotion="css yn21pq" data-s="">.css-yn21pq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex-wrap:wrap;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;position:relative;width:100%;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="css 18nz7be" data-s="">.css-18nz7be{padding-top:0.25rem;padding-bottom:0.25rem;padding-right:0.5rem;}@media (min-width: 768px){.css-18nz7be{padding-right:0.25rem;}}@media (min-width: 1280px){.css-18nz7be{width:50%;}}</style><style data-emotion="css 1jve0bf" data-s="">.css-1jve0bf{height:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="css tozgq3" data-s="">.css-tozgq3{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;opacity:0.5;height:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}.dark .css-tozgq3{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-tozgq3:disabled{opacity:0.5;}.css-tozgq3:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-tozgq3:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-tozgq3:focus{outline:2px solid transparent;outline-offset:2px;}</style><style data-emotion="css o6765v" data-s="">.css-o6765v{margin-right:0.25rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:2rem;}</style><style data-emotion="css 13n4lde" data-s="">.css-13n4lde{position:relative;z-index:30;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;margin-bottom:1.25rem;}@media (min-width: 1280px){.css-13n4lde{margin-top:2.5rem;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;z-index:20;}}</style><style data-emotion="css 1nszzz1" data-s="">.css-1nszzz1{display:none;}@media (min-width: 1280px){.css-1nszzz1{display:block;margin-bottom:2rem;}}</style><style data-emotion="css 1xh5maz" data-s="">.css-1xh5maz{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;}.dark .css-1xh5maz{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-1xh5maz:disabled{opacity:0.5;}.css-1xh5maz:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-1xh5maz:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-1xh5maz:focus{outline:2px solid transparent;outline-offset:2px;}</style><style data-emotion="css 4msi4k" data-s="">.css-4msi4k{fill:currentColor;width:1.75rem;height:1.75rem;}</style><style data-emotion="css 16d2z3r" data-s="">.css-16d2z3r{fill:currentColor;width:1.75rem;height:1.75rem;}</style><style data-emotion="css erhvri" data-s="">.css-erhvri{position:absolute;top:0px;right:0px;padding-left:0.25rem;padding-right:0.25rem;padding-top:1px;padding-bottom:1px;margin-top:-0.25rem;font-size:0.875rem;line-height:1.25rem;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));--tw-bg-opacity:1;background-color:rgba(255, 255, 255, var(--tw-bg-opacity));border-width:1px;--tw-border-opacity:1;border-color:rgba(229, 231, 235, var(--tw-border-opacity));border-radius:9999px;--tw-shadow:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);box-shadow:var(--tw-ring-offset-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shadow);}</style><style data-emotion="css v3j9lg" data-s="">.css-v3j9lg{--tw-text-opacity:1;color:rgba(41, 98, 255, var(--tw-text-opacity));}</style><style data-emotion="css 1se2ttb" data-s="">.css-1se2ttb{width:1rem;height:1rem;fill:currentColor;}</style><style data-emotion="css egedf7" data-s="">.css-egedf7{width:1rem;height:1rem;fill:currentColor;}</style><style data-emotion="css 1oe7b48" data-s="">.css-1oe7b48{margin-right:1rem;}@media (min-width: 1280px){.css-1oe7b48{margin-bottom:2rem;margin-right:0px;}}</style><style data-emotion="css x80nqe" data-s="">.css-x80nqe:focus{outline:2px solid transparent;outline-offset:2px;}</style><style data-emotion="css d0b8tw" data-s="">.css-d0b8tw{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.dark .css-d0b8tw{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-d0b8tw:disabled{opacity:0.5;}.css-d0b8tw:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-d0b8tw:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-d0b8tw:focus{outline:2px solid transparent;outline-offset:2px;}.css-d0b8tw:focus{outline:2px solid transparent;outline-offset:2px;}</style><style data-emotion="css 1o50nro" data-s="">.css-1o50nro{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.dark .css-1o50nro{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-1o50nro:disabled{opacity:0.5;}.css-1o50nro:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-1o50nro:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-1o50nro:focus{outline:2px solid transparent;outline-offset:2px;}</style><style data-emotion="css 1wt4vg7" data-s="">.css-1wt4vg7{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:block;}.dark .css-1wt4vg7{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-1wt4vg7:disabled{opacity:0.5;}.css-1wt4vg7:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-1wt4vg7:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-1wt4vg7:focus{outline:2px solid transparent;outline-offset:2px;}</style><style data-emotion="css 16i1tr9" data-s="">.css-16i1tr9{position:absolute;width:0px;height:0px;overflow:hidden;}</style><style data-emotion="css 1yzq4q2" data-s="">.css-1yzq4q2{position:absolute;top:0px;left:0px;width:0px;height:0px;overflow:hidden;}</style><style data-emotion="css pzj4ej" data-s="">.css-pzj4ej{position:absolute;z-index:50;margin-top:1rem;display:none;}</style><style data-emotion="css 6oz16o" data-s="">.css-6oz16o{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:1rem;--tw-bg-opacity:1;background-color:rgba(255, 255, 255, var(--tw-bg-opacity));border-width:1px;border-radius:0.5rem;--tw-shadow:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);box-shadow:var(--tw-ring-offset-shadow, 0 0 #0000),var(--tw-ring-shadow, 0 0 #0000),var(--tw-shadow);--tw-text-opacity:1;color:rgba(66, 66, 66, var(--tw-text-opacity));--tw-border-opacity:1;border-color:rgba(224, 224, 224, var(--tw-border-opacity));}.dark .css-6oz16o{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(224, 224, 224, var(--tw-text-opacity));--tw-border-opacity:1;border-color:rgba(255, 255, 255, var(--tw-border-opacity));}</style><style data-emotion="css 1gc63p3" data-s="">.css-1gc63p3{display:block;margin-right:0.75rem;}</style><style data-emotion="css pzw4ir" data-s="">.css-pzw4ir{--tw-text-opacity:1;color:rgba(29, 161, 242, var(--tw-text-opacity));}.dark .css-pzw4ir{--tw-text-opacity:1;color:rgba(29, 161, 242, var(--tw-text-opacity));}</style><style data-emotion="css mok5km" data-s="">.css-mok5km{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:0.5rem;padding-right:0.5rem;font-size:0.875rem;line-height:1.25rem;--tw-text-opacity:1;color:rgba(29, 161, 242, var(--tw-text-opacity));}.dark .css-mok5km{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-mok5km:disabled{opacity:0.5;}.css-mok5km:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-mok5km:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-mok5km:focus{outline:2px solid transparent;outline-offset:2px;}.dark .css-mok5km{--tw-text-opacity:1;color:rgba(29, 161, 242, var(--tw-text-opacity));}</style><style data-emotion="css 1jogb9w" data-s="">.css-1jogb9w{--tw-text-opacity:1;color:rgba(255, 101, 78, var(--tw-text-opacity));}.dark .css-1jogb9w{--tw-text-opacity:1;color:rgba(255, 101, 78, var(--tw-text-opacity));}</style><style data-emotion="css 15weny5" data-s="">.css-15weny5{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:0.5rem;padding-right:0.5rem;font-size:0.875rem;line-height:1.25rem;--tw-text-opacity:1;color:rgba(255, 101, 78, var(--tw-text-opacity));}.dark .css-15weny5{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-15weny5:disabled{opacity:0.5;}.css-15weny5:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-15weny5:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-15weny5:focus{outline:2px solid transparent;outline-offset:2px;}.dark .css-15weny5{--tw-text-opacity:1;color:rgba(255, 101, 78, var(--tw-text-opacity));}</style><style data-emotion="css 197yt2c" data-s="">.css-197yt2c{padding-left:0.75rem;padding-right:0.75rem;padding-top:0.25rem;padding-bottom:0.25rem;font-size:1rem;line-height:1.625;font-weight:500;--tw-text-opacity:1;color:rgba(55, 65, 81, var(--tw-text-opacity));border-width:1px;border-color:transparent;border-radius:0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:0.5rem;padding-right:0.5rem;font-size:0.875rem;line-height:1.25rem;}.dark .css-197yt2c{--tw-text-opacity:1;color:rgba(189, 189, 189, var(--tw-text-opacity));}.css-197yt2c:disabled{opacity:0.5;}.css-197yt2c:hover{--tw-bg-opacity:1;background-color:rgba(238, 238, 238, var(--tw-bg-opacity));}.dark .css-197yt2c:hover{--tw-bg-opacity:1;background-color:rgba(35, 38, 38, var(--tw-bg-opacity));}.css-197yt2c:focus{outline:2px solid transparent;outline-offset:2px;}</style><style data-emotion="css ec6vv8" data-s="">.css-ec6vv8{position:relative;z-index:30;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="css xl2uon" data-s="">@media (min-width: 1280px){.css-xl2uon{padding-right:3rem;}}</style><style data-emotion="css 12orxyd" data-s="">@media (min-width: 1280px){.css-12orxyd{width:10rem;}}</style><style data-emotion="css 17deu0r" data-s="">.css-17deu0r{padding-left:1.25rem;padding-right:1.25rem;padding-top:2.5rem;padding-bottom:2.5rem;text-align:center;--tw-bg-opacity:1;background-color:rgba(243, 244, 246, var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(66, 66, 66, var(--tw-text-opacity));}.dark .css-17deu0r{--tw-bg-opacity:1;background-color:rgba(51, 51, 51, var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(158, 158, 158, var(--tw-text-opacity));--tw-border-opacity:1;border-color:rgba(66, 66, 66, var(--tw-border-opacity));}@media (min-width: 768px){.css-17deu0r{padding-left:2.5rem;padding-right:2.5rem;padding-top:3rem;padding-bottom:3rem;}}@media (min-width: 1024px){.css-17deu0r{padding-top:5rem;padding-bottom:5rem;}}</style><style data-emotion="css 11en623" data-s="">.css-11en623{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;}</style><style data-emotion="css 1f7w4n" data-s="">.css-1f7w4n{margin-bottom:3rem;font-size:0.875rem;line-height:1.25rem;--tw-text-opacity:1;color:rgba(75, 85, 99, var(--tw-text-opacity));}.dark .css-1f7w4n{--tw-text-opacity:1;color:rgba(209, 213, 219, var(--tw-text-opacity));}</style><style data-emotion="css 1wbll7q" data-s="">.css-1wbll7q{-webkit-text-decoration:underline;text-decoration:underline;}</style><style data-emotion="css zigog8" data-s="">.css-zigog8{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style data-emotion="css 11pvioa" data-s="">.css-11pvioa{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0.75rem;margin-bottom:1rem;font-weight:500;--tw-text-opacity:1;color:rgba(75, 85, 99, var(--tw-text-opacity));transition-property:background-color,border-color,color,fill,stroke;transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);transition-duration:75ms;--tw-bg-opacity:1;background-color:rgba(255, 255, 255, var(--tw-bg-opacity));border-width:1px;--tw-border-opacity:1;border-color:rgba(209, 213, 219, var(--tw-border-opacity));border-radius:0.5rem;}.dark .css-11pvioa{--tw-border-opacity:1;border-color:rgba(97, 97, 97, var(--tw-border-opacity));--tw-text-opacity:1;color:rgba(209, 213, 219, var(--tw-text-opacity));--tw-bg-opacity:1;background-color:rgba(0, 0, 0, var(--tw-bg-opacity));}.dark .css-11pvioa:hover{--tw-text-opacity:1;color:rgba(255, 255, 255, var(--tw-text-opacity));--tw-border-opacity:1;border-color:rgba(158, 158, 158, var(--tw-border-opacity));}.css-11pvioa:hover{--tw-border-opacity:1;border-color:rgba(156, 163, 175, var(--tw-border-opacity));--tw-text-opacity:1;color:rgba(17, 24, 39, var(--tw-text-opacity));}</style><style data-emotion="css pstqnw" data-s="">.css-pstqnw{display:block;margin-right:0.5rem;--tw-text-opacity:1;color:rgba(41, 98, 255, var(--tw-text-opacity));}</style><style data-emotion="css kd754l" data-s="">.css-kd754l{font-size:0.875rem;line-height:1.25rem;--tw-text-opacity:1;color:rgba(75, 85, 99, var(--tw-text-opacity));}.dark .css-kd754l{--tw-text-opacity:1;color:rgba(209, 213, 219, var(--tw-text-opacity));}</style><style data-emotion="css" data-s=""></style><script type="text/javascript" src="./Transformers_ Attention is all you need_files/tex-chtml.js.download"></script><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-msqrt {
  display: inline-block;
  text-align: left;
}

mjx-root {
  display: inline-block;
  white-space: nowrap;
}

mjx-surd {
  display: inline-block;
  vertical-align: top;
}

mjx-sqrt {
  display: inline-block;
  padding-top: .07em;
}

mjx-sqrt > mjx-box {
  border-top: .07em solid;
}

mjx-sqrt.mjx-tall > mjx-box {
  padding-left: .3em;
  margin-left: -.3em;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c1D438.TEX-I::before {
  padding: 0.68em 0.764em 0 0;
  content: "E";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

mjx-c.mjx-c2217::before {
  padding: 0.465em 0.5em 0 0;
  content: "\2217";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D452.TEX-I::before {
  padding: 0.442em 0.466em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D463.TEX-I::before {
  padding: 0.443em 0.485em 0.011em 0;
  content: "v";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c1D45E.TEX-I::before {
  padding: 0.442em 0.46em 0.194em 0;
  content: "q";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c1D444.TEX-I::before {
  padding: 0.704em 0.791em 0.194em 0;
  content: "Q";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

mjx-c.mjx-c1D449.TEX-I::before {
  padding: 0.683em 0.769em 0.022em 0;
  content: "V";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c221A::before {
  padding: 0.8em 0.853em 0.2em 0;
  content: "\221A";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c1D465.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "x";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c7C::before {
  padding: 0.75em 0.278em 0.249em 0;
  content: "|";
}

mjx-c.mjx-c5B::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "[";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c5D::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "]";
}

mjx-c.mjx-c210E.TEX-I::before {
  padding: 0.694em 0.576em 0.011em 0;
  content: "h";
}

mjx-c.mjx-cA0::before {
  padding: 0 0.25em 0 0;
  content: "\A0";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D464.TEX-I::before {
  padding: 0.443em 0.716em 0.011em 0;
  content: "w";
}

mjx-c.mjx-c1D462.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}
</style></head><body class="css-3tbo75"><input type="hidden" id="hn-user"><div id="__next"><div class="css-3159x8"><header style="background-color:" class="blog-header css-qcu6iv"><div class="container css-yd8da5"><div class="css-1ll3e1i"><div class="css-37nvso"><div class="css-1lfhv42"><button type="button" class="css-1tdo4yy"><svg class="css-1x6nj8n" viewBox="0 0 448 512"><path d="M442 114H6a6 6 0 01-6-6V84a6 6 0 016-6h436a6 6 0 016 6v24a6 6 0 01-6 6zm0 160H6a6 6 0 01-6-6v-24a6 6 0 016-6h436a6 6 0 016 6v24a6 6 0 01-6 6zm0 160H6a6 6 0 01-6-6v-24a6 6 0 016-6h436a6 6 0 016 6v24a6 6 0 01-6 6z"></path></svg><span>Menu</span></button></div><div class="css-v1ygcs"><h1 class="blog-main-logo"><a class="blog-logo css-1b9oi20" aria-label="krypticmouse" href="https://krypticmouse.hashnode.dev/" title="krypticmouse"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAwMCIgaGVpZ2h0PSIyNTAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="krypticmouse" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2x" src="./Transformers_ Attention is all you need_files/RpN_hzmke.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"><noscript><img alt="krypticmouse" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></a></h1></div></div><div class="css-4pib4r"><button type="button" aria-label="Search the blog" data-title="Search" class="blog-search-button tooltip-handle css-wme9tu"><svg class="css-17mdxl4" viewBox="0 0 200 200" fill="none"><g clip-path="url(#clip0)"><path d="M186.804 176.609l-44.092-44.091a4.054 4.054 0 00-2.905-1.197h-3.521c11.724-12.68 18.902-29.599 18.902-48.227C155.188 43.82 123.366 12 84.094 12 44.82 12 13 43.821 13 83.094c0 39.272 31.821 71.094 71.094 71.094 18.628 0 35.547-7.178 48.227-18.868v3.487c0 1.093.445 2.119 1.197 2.905l44.091 44.092a4.107 4.107 0 005.811 0l3.384-3.384a4.107 4.107 0 000-5.811zM84.094 143.25c-33.257 0-60.156-26.899-60.156-60.156s26.899-60.156 60.156-60.156 60.156 26.899 60.156 60.156-26.899 60.156-60.156 60.156z"></path></g><defs><clippath><path transform="translate(13 12)" d="M0 0h175v175H0z"></path></clippath></defs></svg></button><button type="button" aria-label="Theme switcher" data-title="Toggle dark mode" class="blog-theme-switcher tooltip-handle css-wme9tu"><svg class="css-17mdxl4" viewBox="0 0 512 512"><path d="M448.964 365.617C348.188 384.809 255.14 307.765 255.14 205.419c0-58.893 31.561-112.832 82.574-141.862 25.83-14.7 19.333-53.859-10.015-59.28A258.114 258.114 0 00280.947 0c-141.334 0-256 114.546-256 256 0 141.334 114.547 256 256 256 78.931 0 151.079-35.924 198.85-94.783 18.846-23.22-1.706-57.149-30.833-51.6zM280.947 480c-123.712 0-224-100.288-224-224s100.288-224 224-224c13.984 0 27.665 1.294 40.94 3.745-58.972 33.56-98.747 96.969-98.747 169.674 0 122.606 111.613 214.523 231.81 191.632C413.881 447.653 351.196 480 280.947 480z"></path></svg></button><button type="button" class="blog-follow-button tooltip-handle css-15hk9u8" data-title="Follow this blog"><svg class="css-gflyy0" viewBox="0 0 384 512"><path d="M368 224H224V80c0-8.84-7.16-16-16-16h-32c-8.84 0-16 7.16-16 16v144H16c-8.84 0-16 7.16-16 16v32c0 8.84 7.16 16 16 16h144v144c0 8.84 7.16 16 16 16h32c8.84 0 16-7.16 16-16V288h144c8.84 0 16-7.16 16-16v-32c0-8.84-7.16-16-16-16z"></path></svg><span>Follow</span></button><div class="css-bjn8wh"><button type="button" class="blog-more-icon css-9ueuad" aria-label="More"><svg class="css-17mdxl4" viewBox="0 0 448 512"><path d="M443.5 162.6l-7.1-7.1c-4.7-4.7-12.3-4.7-17 0L224 351 28.5 155.5c-4.7-4.7-12.3-4.7-17 0l-7.1 7.1c-4.7 4.7-4.7 12.3 0 17l211 211.1c4.7 4.7 12.3 4.7 17 0l211-211.1c4.8-4.7 4.8-12.3.1-17z"></path></svg></button></div></div></div><div class="css-1g8oej1"><h1 class="blog-main-logo"><a class="blog-logo css-df1pn7" aria-label="Blog&#39;s logo" href="https://krypticmouse.hashnode.dev/" title="krypticmouse"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAwMCIgaGVpZ2h0PSIyNTAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Blog&#39;s logo" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2x" src="./Transformers_ Attention is all you need_files/RpN_hzmke.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"><noscript><img alt="Blog&#x27;s logo" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619641458812%2FRpN_hzmke.png%3Fw%3D1000%26h%3D250%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></a></h1></div><div class="blog-sub-header css-1nl2bbl" data-tom="blog-sub-header flex flex-row w-full overflow-hidden items-end justify-between text-base uppercase font-medium text-black dark:text-white"><div class="scrollbar-container css-ddbtpe ps"><nav class="css-mq47qm"><a href="https://krypticmouse.hashnode.dev/" class="blog-nav css-baperf">Home</a><a class="blog-nav css-tr47eg" href="https://github.com/krypticmouse">Github</a><a class="blog-nav css-tr47eg" href="https://www.linkedin.com/in/herumb-s-740163131/">LinkedIn</a><a class="blog-nav css-tr47eg" href="https://twitter.com/krypticmouse">Twitter</a><a class="blog-nav css-tr47eg" href="https://krypticmouse.hashnode.dev/about-me">About Me</a><a class="blog-nav css-mwyikp" href="https://krypticmouse.hashnode.dev/badges">Badges</a></nav><div class="ps__rail-x" style="left: 0px; top: 0px;"><div class="ps__thumb-x" tabindex="0" style="left: 0px; width: 0px;"></div></div><div class="ps__rail-y" style="top: 0px; left: 0px;"><div class="ps__thumb-y" tabindex="0" style="top: 0px; height: 0px;"></div></div></div><div class="blog-social-media-section css-ho1qnd"><a href="https://hashnode.com/@krypticmouse" aria-label="Find me on Hashnode" target="_blank" rel="noopener noreferrer" class="css-eqcmow"><svg class="css-1330i9l" viewBox="0 0 200 200" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M13.742 66.824c-18.323 18.323-18.323 48.029 0 66.352l53.082 53.082c18.323 18.323 48.029 18.323 66.352 0l53.082-53.082c18.323-18.323 18.323-48.03 0-66.352l-53.082-53.082c-18.323-18.323-48.03-18.323-66.352 0L13.742 66.824zm109.481 56.399c12.826-12.826 12.826-33.62 0-46.446s-33.62-12.826-46.446 0-12.826 33.62 0 46.446 33.62 12.826 46.446 0z"></path></svg></a><a href="https://krypticmouse.hashnode.dev/rss.xml" aria-label="Blog&#39;s XML Feed" class="css-eqcmow"><svg class="css-1330i9l" viewBox="0 0 448 512"><path d="M80 368c17.645 0 32 14.355 32 32s-14.355 32-32 32-32-14.355-32-32 14.355-32 32-32m0-48c-44.183 0-80 35.817-80 80s35.817 80 80 80 80-35.817 80-80-35.817-80-80-80zm367.996 147.615c-6.449-237.834-198.057-429.163-435.61-435.61C5.609 31.821 0 37.229 0 44.007v24.02c0 6.482 5.147 11.808 11.626 11.992 211.976 6.04 382.316 176.735 388.354 388.354.185 6.479 5.51 11.626 11.992 11.626h24.02c6.78.001 12.187-5.608 12.004-12.384zm-136.239-.05C305.401 305.01 174.966 174.599 12.435 168.243 5.643 167.977 0 173.444 0 180.242v24.024c0 6.431 5.072 11.705 11.497 11.98 136.768 5.847 246.411 115.511 252.258 252.258.275 6.425 5.549 11.497 11.98 11.497h24.024c6.797-.001 12.264-5.644 11.998-12.436z"></path></svg></a></div></div></div></header><div class="blog-post-area css-78a8kl"><main class="blog-post-detail-card css-9slcbn"><script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "NewsArticle",
      "url": "https://krypticmouse.hashnode.dev/attention-is-all-you-need",
      "mainEntityOfPage": "https://krypticmouse.hashnode.dev/attention-is-all-you-need",
      "headline": "Transformers: Attention is all you need",
      "description": "Attention is Transformer’s breath,
Multi-Head is Transformer’s release,
RNNs thou wert and art,
May thy model reach to greater accuracy.
Látom.
 - Enen No Shouboutai

I'm getting tired of butchering anime and video game quotes so I'm thinking I shoul...",
      "datePublished": "2021-12-09T22:53:33.788Z",
      "dateModified": "2021-12-10T04:49:08.188Z",
      "isAccessibleForFree": true,
      "image": {
        "@type": "ImageObject",
        "url": "https://cdn.hashnode.com/res/hashnode/image/upload/v1638987431300/0yWP8XDYJ.png"
      },
      "author": {
        "@type": "Person",
        "name": "Herumb Shandilya",
        "url": "https://hashnode.com/@krypticmouse"
      },
      "publisher": {
        "@type": "Organization",
        "name": "krypticmouse",
        "url": "https://krypticmouse.hashnode.dev",
        "logo": "https://cdn.hashnode.com/res/hashnode/image/upload/v1619641458812/RpN_hzmke.png"
      }
    }
  </script><article><div class="blog-content-wrapper article-main-wrapper css-m96uju"><section class="blog-content-main article-width css-1ffmpwz"><div class="css-bjn8wh"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYwMCIgaGVpZ2h0PSI4NDAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Transformers: Attention is all you need" src="./Transformers_ Attention is all you need_files/0yWP8XDYJ.jpg" decoding="async" data-nimg="intrinsic" class="css-1jqt27n" style="position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%; filter: none; background-size: cover; background-image: none; background-position: 0% 0%;" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1638987431300%2F0yWP8XDYJ.png%3Fw%3D1600%26h%3D840%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1638987431300%2F0yWP8XDYJ.png%3Fw%3D1600%26h%3D840%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=3840&amp;q=75 2x"><noscript><img alt="Transformers: Attention is all you need" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1638987431300%2F0yWP8XDYJ.png%3Fw%3D1600%26h%3D840%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1638987431300%2F0yWP8XDYJ.png%3Fw%3D1600%26h%3D840%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=3840&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1638987431300%2F0yWP8XDYJ.png%3Fw%3D1600%26h%3D840%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=3840&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1jqt27n" loading="lazy"/></noscript></span></div><div class="css-1rtko81"><h1>Transformers: Attention is all you need</h1></div><div class="css-hj8uh5"><h2 class="css-n9m7tc">Getting back to where it started</h2></div><div class="css-zd4cjh"></div><div class="css-1pt9cni"><div class="css-4c6dm7"><div class="css-12sdugl"><div class="css-nbo283"><a href="https://hashnode.com/@krypticmouse" class=" css-l9iun9" data-title="false"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjAwIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB2ZXJzaW9uPSIxLjEiLz4="></span><img alt="Herumb Shandilya&#39;s photo" src="./Transformers_ Attention is all you need_files/yUO-x7u_t.jpg" decoding="async" data-nimg="intrinsic" class="css-1yapcyz" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D200%26h%3D200%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=256&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D200%26h%3D200%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=640&amp;q=75 2x"><noscript><img alt="Herumb Shandilya&#x27;s photo" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D200%26h%3D200%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=256&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D200%26h%3D200%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=640&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D200%26h%3D200%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=640&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1yapcyz" loading="lazy"/></noscript></span></a></div><div class="css-1k5pqev"><a href="https://hashnode.com/@krypticmouse" class="css-51a556"><span>Herumb Shandilya</span></a><p class="css-1dyl29k"><a href="https://krypticmouse.hashnode.dev/attention-is-all-you-need" class="tooltip-handle css-4zleql" data-title="Dec 9, 2021 22:53">Published on <span class="css-16ceglb">Dec 10, 2021</span></a></p><div class="css-fhxb3m"><p class="css-tb6s22"><svg class="css-hb3dfw" viewBox="0 0 512 512"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm216 248c0 118.7-96.1 216-216 216-118.7 0-216-96.1-216-216 0-118.7 96.1-216 216-216 118.7 0 216 96.1 216 216zm-148.9 88.3l-81.2-59c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h14c6.6 0 12 5.4 12 12v146.3l70.5 51.3c5.4 3.9 6.5 11.4 2.6 16.8l-8.2 11.3c-3.9 5.3-11.4 6.5-16.8 2.6z"></path></svg><span>26<!-- --> min read</span></p></div></div></div><div></div></div></div><div class="css-bjn8wh"><div class="blog-subscription-article-wrapper css-1w7f2ot"><div class="blog-subscription-form-wrapper css-vabva8"><div class="css-1kuuouk"><div class="css-b00jk2"><p class="blog-subscription-form-message css-b2cmdi">Subscribe to <!-- -->my<!-- --> newsletter and never miss<!-- --> <!-- -->my<!-- --> upcoming articles</p></div><div class="css-sgivcr"><div class="css-248rth"><svg class="css-1330i9l" viewBox="0 0 512 512"><path d="M464 4.3L16 262.7C-7 276-4.7 309.9 19.8 320L160 378v102c0 30.2 37.8 43.3 56.7 20.3l60.7-73.8 126.4 52.2c19.1 7.9 40.7-4.2 43.8-24.7l64-417.1C515.7 10.2 487-9 464 4.3zM192 480v-88.8l54.5 22.5L192 480zm224-30.9l-206.2-85.2 199.5-235.8c4.8-5.6-2.9-13.2-8.5-8.4L145.5 337.3 32 290.5 480 32l-64 417.1z"></path></svg></div><input type="text" aria-label="Newsletter subscription form" class="blog-subscription-form-input css-148gys9" placeholder="Email address"><div class="css-wswes9"><button type="button" variant="transparent" class="blog-subscription-form-button css-azbqzp">Subscribe</button></div></div></div></div></div><div id="post-content-wrapper" class="prose css-1azsmog"><blockquote>
<p>Attention is Transformer’s breath,</p>
<p>Multi-Head is Transformer’s release,</p>
<p>RNNs thou wert and art,</p>
<p>May thy model reach to greater accuracy.</p>
<p>Látom.</p>
<p> - Enen No Shouboutai</p>
</blockquote>
<p>I'm getting tired of butchering anime and video game quotes so I'm thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper. </p>
<p>I mean I've been seeing blogs that go by the name of <strong>Paper Dissected</strong> so I wanted to try writing one myself. Well this is going to be my first try in this and frankly, I have no idea how it'll be but let's give it a try, shall we?</p>
<p>I guess it's safe to say that Attention Mechanism and Transformers is something that recently took over NLP. Not only did it show improvements over the SOTA models at the time but also overcome the shortcoming of the RNN models like LSTM and GRU.</p>
<p><img src="./Transformers_ Attention is all you need_files/oF2h1CrF8.jpeg" alt="1610374526-cheems1 (2).jpg"></p>
<p>So let's go ahead and break down the sections of the paper, <strong>Attention is all you need</strong>. To give you a gist of what'll be there, it'll be an explanation of each section in the paper like abstract, introduction, model architecture, etc.</p>
<h1 id="heading-abstract">Abstract</h1>
<p>This section pretty much summarizes the whole paper not in terms of working but rather in terms of what it has to offer. It starts with explaining how Seq2Seq models usually use RNN and CNN for encoder and decoder, and how the best ones connect encoder and decoder via attention mechanism.</p>
<p>Now just to be clear, attention itself is not a new concept but to rely completely on attention is something that this paper introduced. It went ahead explaining the results achieved on the machine translation task which were like the following on WMT-2014 dataset:-</p>
<div class="hn-table">
<table>
<thead>
<tr>
<td>MT - Task</td><td>BLEU Score</td></tr>
</thead>
<tbody>
<tr>
<td><strong>English to German</strong></td><td>28.4</td></tr>
<tr>
<td><strong>English to French</strong></td><td>41.8</td></tr>
</tbody>
</table>
</div><p>It also took 3.5 days and 8 GPUs to train on, which is certainly out of my budget. But the main thing that shined a bit differently than others in the abstract section was:-</p>
<blockquote>
<p>Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train.</p>
</blockquote>
<p>We saw how results were better than traditional models but the thing that separated transformers from LSTM is parallel computation. This can feel a bit tricky to understand at the start so let's break it down. By nature, RNN models aren't parallel but rather Sequential and this is because they compute results one step at a time.</p>
<p><img src="./Transformers_ Attention is all you need_files/0U5QBxJEa.png" alt="YjlBt.png"></p>
<p>However, in the Transformer model, there is no such thing as "time steps" but rather the inputs are passed all together and all this is made possible due to multi-headed attention and other stuff which we'll talk about later.</p>
<h1 id="heading-introduction">Introduction</h1>
<p>This section mainly talks about the working of RNNs and it's limitations, along with the recent developments to tackle them. Now that we are at it let's understand how does Seq2Seq modeling working in the case of RNNs and to make it easier, take a look a look at the picture below:-</p>
<p><img src="./Transformers_ Attention is all you need_files/yoH-0RbHk.png" alt="Seq2Seq Blog Img #1.png"></p>
<p>Seq2Seq Models consists of 2 models i.e. an Encoder and a Decoder. The Encoder model takes the text as input and generates a Context Vector as an output which is nothing but the last hidden state of RNN. Encoders and Decoders are nothing but RNNs and in few instances CNN 🌚. </p>
<p>Another thing you'll notice is the crosses above RNN cells except the last one that doesn't mean they aren't produced, they are but we just don't use them and all we'll be doing is to take the context vector from the last time step and feeding it to the decoder as input who'll output each word at a time giving us the resultant text. </p>
<p>In attention, however, you'll be taking the outputs of all the time step of encoder so you don't discard in that case. I mean you can also do mean pooling on all the states and pass that as context vector, so attention isn't the only way that uses all the states. But how does the decoder predict the words? Let's see, I'll try to be brief.</p>
<p>During inference the first token is the special token signifying the start of the sentence along with context vector from the encoder, after that you pass previous output as input to the decoder along with the hidden state to generate new output, this is known as <em>*Teacher Forcing</em>. </p>
<p>This way you'll use generated output as input to generate the next token, and it's cool except the output is being constructed on the context of inferred output rather than actual ones giving rise to the problem of <strong>exposure bias</strong>. Which basically means our model's output will be messed up if one of the outputs is bad.</p>
<p>Back to the topic, the nature of RNNs is sequential which makes parallelization difficult for training, making RNN models take more time to train. However, there are few developments that tackle it:-</p>
<blockquote>
<p> Recent work has achieved significant improvements in computational efficiency through <strong>factorization tricks</strong> and <strong>conditional computation</strong>, while also improving model performance in the case of the latter. The fundamental constraint of sequential computation, however, remains.</p>
</blockquote>
<p>To understand the factorization trick you can go ahead and read another paper, i.e. <strong>Factorization Tricks for LSTM Networks</strong> by Oleksii and Boris, which I recommend but I'll give you the gist of it.</p>
<blockquote>
<p> The major part of LSTMP cell computation is in computing affine transform T because it involves multiplication with 4n × 2p matrix W.</p>
</blockquote>
<p><img src="./Transformers_ Attention is all you need_files/s9rWIKqZA.png" alt="image.png"></p>
<p>The aim of the factorization trick is to approximate that W by replacing it with the product of 2 matrices W1(2p*r) and W2(r*4n), this way takes fewer parameters (2p<em>r + r</em>4n) as compared to W (2p*4n) given r &lt; p &lt;=n. Another factorization trick proposes to think parts of inputs and the hidden states as independent of each other and calculate affine transforms of these groups individually and parallelly. </p>
<p>I know it's a lot to grasp so if you don't understand it all at the moment it's fine, important thing to understand is that <strong>many attempts have been made to improve RNNs but the sequential nature isn't completely removed</strong>. The attention model did help in capturing long-term dependencies and improve results, however, it was used along with RNNs. </p>
<p>There is another concept of conditional computation that aims at activating parts of the network to improve computational efficiency, and performance too in this case, but let's not dive much into it. But if you want to you can read this <a target="_blank" href="https://trace.tennessee.edu/cgi/viewcontent.cgi?article=5323&amp;context=utk_graddiss"><strong>short book</strong></a> on the same.</p>
<p>The transformer model on the other hand relies entirely on attention mechanism allowing it to be more parallelizable and give better results with the cost of training being 12 hours on 8 P100 GPUs, something that even Colab Pro users can't get their hands on all the time anymore 🥲.</p>
<h1 id="heading-model-architecture">Model Architecture</h1>
<p>Finally, we've come to the real deal. Now while I wanna try and break down the explanation sub-section wise, I know that may or may not end up leaving you kinda confused, and hence instead of explaining each sub-section of this section we'll dismantle the architecture step by step and understand how it works.</p>
<p><img src="./Transformers_ Attention is all you need_files/17Un25edDk.png" alt="image.png"></p>
<h2 id="heading-input-embeddings">Input Embeddings</h2>
<p>Let's start from the beginning, what you have with you is a sentence or bunch of sentences. Now you can't exactly feed it to the model as it is so you need a way to convert it to a form more suitable for it. When we used RNNs for text classification or seq2seq we convert our text to <strong>word embeddings</strong>.</p>
<p>These embeddings are nothing but vector representations of words such that words with similar meanings are closer to each other. But what does closer mean? Mathematically, closer means the similarity score is high. This score can be anything but cosine similarity is popular so feel free to think that. Once we have our embeddings to use we are all set! Are we?</p>
<p><img src="./Transformers_ Attention is all you need_files/Qo6JVLncJ.jpeg" alt="4824982 (1).jpg"></p>
<p>Technically, yes we are since these embeddings can be used with RNNs and CNNs because they are able to capture the ordering into consideration. But Transformers use only self-attention, which takes all the words at the same time hence it is not able to take the order of words into consideration. To fix this problem we introduce something called <strong>positional encodings</strong>.</p>
<h2 id="heading-positional-encodings">Positional Encodings</h2>
<p>So as mentioned before, inherently attention model is not able to take the order of words into consideration. In order to compensate for this, we use something called Positional Encodings. Don't be intimidated by the name it's actually much more simple. Let's dive a bit deeper into this.</p>
<h3 id="heading-need-of-positional-encodings">Need of Positional Encodings</h3>
<p>When working with text it's really important to take words order into consideration. Why? The order of words defines the structure of the sentence and this structure helps in providing the context. </p>
<p>RNNs by nature are sequential in nature by taking inputs one by one and updating hidden states accordingly, but the same is not the case with attention since it takes the words all at once. Why this is so, is something we'll understand in the next section but for now, let's say that's how it is.</p>
<p>So we need some extra sources of information in our embeddings to convey this information, something that could tell the position of a word in a sentence. To do so we can add an extra embedding or matrix to our word embeddings to provide info regarding the ordering of the word. But how? </p>
<h3 id="heading-finding-positional-encodings">Finding Positional Encodings</h3>
<p>Well in the most simple case you can use a one-hot encoded form of [0,1,2,3,4,5,..., pos] as positional encodings. You can train an embedding layer to find the <strong>positional embedding</strong> matrix, where each row has the positional embedding for word at pos index, for this and add it to input embeddings.</p>
<p>In the paper, however, the proposed approach uses a combination of sin and cos function to find the encodings. Take a look at the following picture:-</p>
<p><img src="./Transformers_ Attention is all you need_files/lcT9IBipy.png" alt="i (3).png"></p>
<p>As you can see in the above picture the encodings are found by using a combination of sin and cos function. The function is quite self-explanatory, it says that for the embedding of a word at position <strong>pos</strong>:- </p>
<ul>
<li>The value at index <strong>2i</strong> is given using the <strong>sin</strong> function, colored red. </li>
<li>The value at index <strong>2i +1</strong> is given using the <strong>cos</strong> function, colored blue.</li>
</ul>
<p>But why are we using this particular function? Well let's hear from the authors themselves:-</p>
<blockquote>
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions since, for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.</p>
</blockquote>
<p>Well, researchers sure love to make stuff seem more complicated, don't they? It's fine if you don't understand the above sentence, take it as a confirmation that you're still sane. </p>
<p>What the sentence means to say is that given the positional encoding of the word at <strong>pos</strong> you can find the positional encoding of word at <strong>pos + k</strong> by applying some linear transformation on the positional encoding of <strong>pos</strong>. Mathematically, it can be written like this:-</p>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="0" style="font-size: 123.3%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.026em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D438 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.026em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>P</mi><msub><mi>E</mi><mrow data-mjx-texclass="ORD"><mi>p</mi><mo>+</mo><mi>k</mi></mrow></msub><mo>=</mo><mi>T</mi><mo>∗</mo><mi>P</mi><msub><mi>E</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container></p>
<p>Writing <strong>pos</strong> as <strong>p</strong> in the above equation since latex is getting messed up for some reason. The important thing to know is that the matrix T can be used to find PE at pos+k but what is this T? Can we find it? Well yes, that T is a rotational matrix that depends on offset k rather than pos. Wait what! How? </p>
<p>Well, you can derive it, and here is a really good <a target="_blank" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">blog by Amirhossein</a> doing the same. There is another <a target="_blank" href="https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/">blog by Timo Denk</a> that proves the linear relationship between PE at pos and pos+k but the first one explains in a much simpler fashion, but do give both a read.</p>
<h3 id="heading-improving-a-bit-more">Improving a bit more</h3>
<p>Wow so everything is dope, right? Fortunately or unfortunately, No. Even though everything might seem super nice with these encodings, which it is, what these really are doing is finding values for given pos and i values, these are also called <strong>Absolute Positional Encodings</strong>. While the altering of sin and cosine might be able to add relative nature, we might actually have something better i.e. <strong>Relative Positional Encodings</strong>.</p>
<p>Following this paper, there was another paper released i.e. <strong>Self-Attention with Relative Position Representations</strong> by <a target="_blank" href="https://arxiv.org/pdf/1803.02155.pdf">Shaw et. al. 2019</a>. This paper introduces a modified attention mechanism which is mostly the same except this one has 2 matrices that are added to Key and Value Vector and the aim is that they'll be able to induce pairwise relationships in the inputs. In the author's own words:-</p>
<blockquote>
<p>We propose an extension to self-attention to consider the pairwise relationships between input elements. In this sense, we model the input as a labeled, directed, fully connected graph.</p>
</blockquote>
<p>The paper is actually a pretty decent read and you should give it a try. Since this paper doesn't use these embeddings we'll not go into the detail of these. There are many papers that try to come up with better Positional Encodings and try to improve them, but what they are used for and proper intuition is something you should aim to understand.</p>
<h2 id="heading-attention">Attention</h2>
<p>Attention is something that we've been talking about a lot in this blog, after all that's what this paper is about. But what does it actually mean? So before diving into the proposed method for attention calculation in this paper, we'll go ahead and understand what attention is and how it helps our cause.</p>
<p>So when we do Seq2Seq with RNNs we have an encoder and decoder network in which the work of the decoder is to generate a sentence given <strong>context vector</strong>. This context vector is nothing but the last hidden state of the encoder which is RNN based model. So what's the issue? Well, you see:-</p>
<ul>
<li><p>The <strong>context vector</strong> is a vector of fixed length defined by hidden_dims but the sentences are of variable length and so what we are doing is squeezing info into a vector due to which information might be lost.</p>
</li>
<li><p>Another problem is that the generated sentences are based on the context of the whole sentence, however in doing so we might lose out on the local context of parts of the sentence that might've been helpful in tasks like translation.</p>
</li>
</ul>
<p>So in order to improve on this, we introduce <strong>attention</strong> and instead of working on the global context of the sentence by taking all the hidden states in the encoder and passes it to the decoder. The decoder, in this case, doesn't produce the output directly instead it'll calculate the attention score for the states and creates the context vector for that particular time step given the attention score, the encoder hidden states, and the input vector, output of prev. step or last input token.</p>
<p><img src="./Transformers_ Attention is all you need_files/yLRmt7ySf.png" alt="Normal Decoder.png"></p>
<p>But how does this help? Well, what attention does is that it generates a context vector at each time step of the decoder based on attention score and this attention score helps to amplify the more important words to focus on during that time step in order to improve the quality of results. If you calculate attention among the words of the input sequence it's called <strong>Self Attention</strong> and when you calculate attention among the words of the input and output sequence it's called <strong>General Attention</strong>.</p>
<p>Now based on the architecture you have 2 types of attention-based Seq2Seq models i.e. <strong>Bahdanau Attention</strong> and <strong>Luong Attention</strong>. Except the architecture and computation both are basically the same but if you wanna learn more you can go ahead and read the paper, <strong>Effective Approaches to Attention-based Neural Machine Translation</strong> by <a target="_blank" href="https://arxiv.org/abs/1508.04025">Luong et. al. 2015</a>.</p>
<h3 id="heading-scaled-dot-product-attention">Scaled Dot-Product Attention</h3>
<p>We have a basic idea of how attention works in RNN now, but we don't exactly have an RNN in Transformer or a way to sequentially input data. So the proposed method for calculating attention is via something called <strong>Scaled Dot-Product Attention</strong>, which is basically a bunch of matrix multiplication operations. </p>
<p>But before diving in, let's talk about QKV or Query-Key-Value vectors and their dimensions. Throughout the model, the dimension of columns of the resultant matrix from each layer is set to <strong>512</strong> which is denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 123.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>. </p>
<p>So if we have <strong>m</strong> rows in word embedding we'll have the resultant matrix of dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="2" style="font-size: 123.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-msub space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mo>∗</mo><msub><mi>d</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math></mjx-assistive-mml></mjx-container>, this column dimension is consistent for all the layers in the model including the embedding layer. </p>
<p>This is done to facilitate the residual connections, which we'll talk about later. Now that we have output dims cleared let's talk about columns of QKV vectors.</p>
<p>For now, just remember the dimension of QKV vectors being 64, the reasoning is something we'll discuss in Multiheaded attention. The dimensions of Query and Key vector are the same and are denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="3" style="font-size: 123.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>k</mi></msub></math></mjx-assistive-mml></mjx-container>, and even though here it is same the dimensions of value vector can be different and is denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="4" style="font-size: 123.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>v</mi></msub></math></mjx-assistive-mml></mjx-container>. So to sum it all up we'll have the following notation for dimensions:-</p>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="5" style="font-size: 123.3%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>d</mi><mrow data-mjx-texclass="ORD"><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>512</mn></math></mjx-assistive-mml></mjx-container></p>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="6" style="font-size: 123.3%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45E TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>d</mi><mi>q</mi></msub><mo>=</mo><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><mn>64</mn></math></mjx-assistive-mml></mjx-container></p>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="7" style="font-size: 123.3%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>d</mi><mi>v</mi></msub><mo>=</mo><mn>64</mn></math></mjx-assistive-mml></mjx-container></p>
<p>Now that we have the important stuff cleared, let's take a look at the following image that sums up the process of calculating attention:-</p>
<p><img src="./Transformers_ Attention is all you need_files/1NFBKy8H8D.png" alt="S C A L E.png"></p>
<p>It might seem confusing so let's go in step by step on how attention is being calculated and how its equation is constructed:-</p>
<ul>
<li><p>You start by finding <em>Query</em>, <em>Key</em>, and <em>Value</em> vector by multiplying the input matrix with their corresponding weight vector. The dims of these vectors are something we'll discuss in Multi Headed Attention Section. For now, let's just focus on understanding the equation.
<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="8" style="font-size: 123.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></p>
</li>
<li><p>Apply Matrix Multiplication on <em>Query</em> and <em>Key</em> Vector.
<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="9" style="font-size: 123.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.413em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></math></mjx-assistive-mml></mjx-container></p>
</li>
<li><p>Scale the result of above with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="10" style="font-size: 123.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></math></mjx-assistive-mml></mjx-container>. It was observed that for small values of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="11" style="font-size: 123.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>k</mi></msub></math></mjx-assistive-mml></mjx-container> additive attention and dot product attention perform similarly. But for larger values of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="12" style="font-size: 123.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>k</mi></msub></math></mjx-assistive-mml></mjx-container> additive attention outperforms dot product attention without scaling. As mentioned in paper, that for large value the result may explode and may push softmax to smaller gradients. To counteract this <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="13" style="font-size: 123.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac><mn>1</mn><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></math></mjx-assistive-mml></mjx-container> was used for scaling. As to why this occurs, paper mentions:-</p>
</li>
</ul>
<blockquote>
<p>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product has mean 0 and variance d_k.</p>
</blockquote>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="14" style="font-size: 123.3%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></math></mjx-assistive-mml></mjx-container></p>
<ul>
<li><p>Applying softmax on the result of the above.
<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="15" style="font-size: 123.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></p>
</li>
<li><p>Apply Matrix Multiplication on result and <em>Value</em> Vector and get the final result.
<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="16" style="font-size: 123.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type="d"><mjx-num><mjx-nstrut type="d"></mjx-nstrut><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D444 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line type="d"></mjx-line><mjx-row><mjx-den><mjx-dstrut type="d"></mjx-dstrut><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.082em;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></math></mjx-assistive-mml></mjx-container></p>
</li>
</ul>
<p>There you go that's how you find attention using Scaled Dot Product Attention. Just for the info, this is not the absolute way to find attention, there are other ways to calculate attention like additive attention, dot-product(unscaled) attention. While the two are similar in complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p>
<p>Now let's take a look at the newly proposed approach where instead of calculating attention one time we calculate it multiple times and then concatenate the results. Yep, it is indeed <strong>Multi-Headed Attention</strong>.</p>
<h3 id="heading-multi-headed-attention">Multi-Headed Attention</h3>
<p>Let's talk about the beast that the paper proposed and when I say "beast" I mean it. The paper suggested that instead of calculating attention one time we can calculate it  <strong>h</strong> times with d_q, d_k, d_v dimensioned QKV vectors found after passing each of them from a Linear Layer.</p>
<p>We then use them to calculate attention values from each attention head depicted by head<em>i and concatenate these heads forming \( d</em>{model} \) dimensional resultant vector. After that, we'll pass them through another linear layer to get our Multi-Headed Attention output. Architecture wise it looks like this:-</p>
<p><img src="./Transformers_ Attention is all you need_files/RBUNc_MHH.png" alt="Linear.png"></p>
<p>One thing to note is that the value of d_q, d_v, and d_k will be <strong>d_model / h</strong>, that's why after you concatenate, the attention head's output it becomes d_model sized vector. In this paper, the value of <strong>h</strong> for the optimal model was 8. So d_k, d_v, and d_q become 512/8 = 64.</p>
<h4 id="heading-why-does-it-work">Why does it Work?</h4>
<p>But after hearing all this it's only sensible to ask. Why does this even work? To answer this query, we should go and take a look at the paper <strong>"Analyzing Multi-Head Self-Attention"</strong> by <a target="_blank" href="https://arxiv.org/pdf/1905.09418.pdf">Voita et. al. 2019</a>. Where they basically try to find the roles that different attention heads played along with other stuff which we'll talk about later i.e. Attention Head Pruning.</p>
<p>To explain the gist of it the conclusion was, that based on roles attention heads can be divided into 3 parts:-</p>
<ul>
<li><p><strong>Positional Heads:</strong> Heads where the maximum attention weights was assigned to a specific relative position. This was usually +/- 1 in practice signifying attention to adjacent positions</p>
</li>
<li><p><strong>Syntactical Heads: </strong> Head attends to tokens corresponding to any of the major syntactic relations in a sentence. In the paper, they analyze direct relations like nominal subject, direct object, adjectival modifier, and adverbial modifier.</p>
</li>
<li><p><strong>Rare Word Heads</strong>: Heads attend to the least frequent tokens in a sentence. </p>
</li>
</ul>
<p>So different heads try to perform different task cool, but do we need all of them? We have only 3 types right? Does that mean other heads are useless? Yes and No. Let's understand the <strong>Yes</strong> part first. </p>
<p>Yes, attention heads are important and in fact, that's exactly what the blog <strong>"Are Sixteen Heads Really Better than One?"</strong> by <a target="_blank" href="https://blog.ml.cmu.edu/2020/03/20/are-sixteen-heads-really-better-than-one/">Paul Michael</a> tries to find. In the blog, he tried to explain how the performance was affected when attention heads were pruned. After the experiment they found the following result:-</p>
<p><img src="./Transformers_ Attention is all you need_files/s9kG6ry4m.png" alt="image.png"></p>
<p>As you can see the BLEU Score is really affected after pruning the heads however like other metrics the dip is observed after more than 60% heads are pruned. This leads us to the <strong>No</strong> part. I mean kinda No.</p>
<p>Even though we just saw that heads are important we can also prune one or a few of them without seeing a major dip in the performance, something that both the mentioned sources, that I highly recommend you to go through, suggested. Phew that was a lot, now that we have the initial stuff cleared up, let's get back to the architecture. I promise this point forward is a smooth sailing simple life, probably xD. </p>
<h2 id="heading-encoder">Encoder</h2>
<p>If we try to understand the architecture of Encoder it can be broken into 2 parts:-</p>
<ul>
<li><p>Attention Layer</p>
</li>
<li><p>Feed Forward Layer</p>
</li>
</ul>
<p>Yes that's it, you are already familiar with Attention Layer which is nothing but Multi Headed Attention with 8 heads and the Feed Forward Layer is a basic Neural Network with a single hidden layer with 2048 nodes and 512 nodes in input and output layer. Visually it'll look something like this:-</p>
<p><img src="./Transformers_ Attention is all you need_files/RL6ONG63c.png" alt="Multi-Headed Attention.png"></p>
<p>Its flow of Infomation is really simple, or not depending on your knowledge of skip connections. If I have to explain in one line skip connection is basically <strong>adding input to the output</strong>. </p>
<p>I know it sounds weird, but trust me it makes sense. ResNet model was the one that popularised these skip connections which provides an alternate path for gradients to flow. They helped train deeper models without running into vanishing gradient problem. So if x is the input to a block and f(x) is the output of the block then skip connections basically add them both making <strong>x + f(x)</strong> the new output.</p>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="17" style="font-size: 123.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.413em; margin-left: 0.053em;"><mjx-mo class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>f</mi><mo data-mjx-alternate="1">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>+</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></p>
<p>Now that we are all on the same page, let's step by step go through the flow of info in an encoder block:-</p>
<ol>
<li>Encoder block receives an input <strong>X</strong>.</li>
<li>X is supplied to Multi-Headed Attention Layer. QKV all will be the same i.e. <strong>X</strong>.</li>
<li>Multi Headed Attention does the magic and generates an output M(x).</li>
<li>Add X to M(x) because of skip connection.</li>
<li>Apply Layer Normalisation to X + M(x), let's call this N(x).</li>
<li>Pass N(x) to Feed Forward layer to generate an output F(x).</li>
<li>Add N(x) in F(x) because of skip connection again.</li>
<li>Apply Layer Normalisation to F(x) + N(x), let's call this E(x) which is the output of this encoder block.</li>
</ol>
<p>One last thing, the above is flow of info for one encoder block. This encoder block's output will be passed on to the next encoder block and the architecture of all encoder blocks is the same. The paper proposed the architecture with <strong>6 encoder blocks</strong>. I swear I need to learn animation one of these days, would make stuff much much easier.</p>
<p><img src="./Transformers_ Attention is all you need_files/YTnCToXMN.png" alt="image.png"></p>
<p>Yea I know too much info xD, but hey that's all that you need to know about Encoders. The output of the last encoder block is passed to the decoder as KV vector values, which is our next topic of discussion. We are almost near the end, so hang in there 😬.</p>
<h2 id="heading-decoder">Decoder</h2>
<p>Finally, we are at the last step of the architecture, to be honest decoder is not really that different from than encoder except the fact that here were have an extra layer for masked multi-headed attention. We'll get into the details of that in a bit, but let's get an overview of how a decoder block looks:-</p>
<p><img src="./Transformers_ Attention is all you need_files/aVah0DKWI.png" alt="Decoder (1).png"></p>
<p>Before going into the flow of information let's understand exactly what is the input and output of the decoder block. As far as input is concerned the decoder takes 2 inputs:-</p>
<ol>
<li><p><strong>Encoder Output:</strong> This is will be fed as KV Vectors for multi-headed attention in the 2nd layer, this is usually referred to as Encoder-Decoder Attention. All <strong>6 Decoder Blocks</strong> will take the encoder output from the last block as input for encoder-decoder attention.</p>
</li>
<li><p><strong>Output Embedding:</strong> These are the embeddings of the target sentence. But wait!? We don't have a target in testing! So what are we inputting here then? Let's see.</p>
</li>
</ol>
<p>Well usually the first input to the decoder is a special token signifying the start of the output sentence and the output is the token that comes after this. This output is the output of time step 1. </p>
<p>After that, the input to decoder becomes the tokens until a certain timestep i.e. for the 5th timestep input will be all generated tokens till the 4th timestep. After that, it'll keep on giving an output until it throws a special token signifying the end of the output sentence.</p>
<p>Let's understand the flow of information in decoder block to get a better understanding:-</p>
<ol>
<li>Decoder block receives an input <strong>X</strong>.</li>
<li>X is supplied to Masked Multi-Headed Attention as QKV vector, to get an output M(x).</li>
<li>Add X in M(x) because of skip connection.</li>
<li>Apply Layer Normalisation to X + M(x), let's call this N(x).</li>
<li>N(x) is supplied to Multi-Headed Attention as Query and Encoder Output will be supplied as Key-Value..</li>
<li>Multi Headed Attention does the magic and generates an output O(x).</li>
<li>Add N(x) in O(x) because of skip connection.</li>
<li>Apply Layer Normalisation to N(x) + O(x), let's call this P(x).</li>
<li>Pass P(x) to Feed Forward layer to generate an output F(x).</li>
<li>Add P(x) in F(x) because of skip connection again.</li>
<li>Apply Layer Normalisation to P(x) + F(x), let's call this D(x) which is the output of this decoder block.</li>
</ol>
<p>But all that is nice but how does the decoder predict the next token? Actually, the output of the last decoder block is fed to a Linear Layer that gives an output vector of the size same as that of vocabulary. So depending on how many words the model knows that will be the size of the output vector of this Linear Layer.</p>
<p>Now once you have this vector you apply softmax on this to convert and interpret the values of this vector as probabilities. The token at the index of maximum probability is the output of that "time-step". Take a look at the following infographic to understand it better.</p>
<p><img src="./Transformers_ Attention is all you need_files/vAicWO0-Q.png" alt="1.01.png"></p>
<p>Well, that's basically how the decoder works, but we are still missing one key piece in the puzzle i.e. Masked Multi-Headed Attention. It's actually pretty simple and trust me it won't take much long to understand.</p>
<h3 id="heading-masked-multi-headed-attention">Masked Multi-Headed Attention</h3>
<p>If I have to say the difference between Normal Multi-Head Attention and Masked Multi-Head Attention it'll be that in Masked Multi-Head Attention we mask the output of attention score after a point for each token by replacing it with <strong>-inf</strong> and when it applies softmax these -inf become 0.</p>
<p>Well, that's fine but what exactly is going on? Let's understand. The reason we need such a thing in the first place is so that we can speed up training by being able to compute the probabilities of output token for each input token parallelly. The only reason we are doing this stuff is to speed up training.</p>
<p>And just to be clear this passing all at once stuff is something we'll do only in training, during inference we'll still do it step by step by passing previous outputs to get the new ones.</p>
<p>The aim of an autoregressive model is to find the next step given the previous ones but for parallelization, we'll be passing all the stuff together at once hence we'll need to hide the output of the attention score for the future time steps of that token to properly train the model. Let's understand this step by step to get a better understanding.</p>
<h4 id="heading-step-by-step-guide-to-masked-mha">Step-By-Step Guide to Masked MHA</h4>
<p>So let's start by knowing what exactly we need to find:-</p>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="18" style="font-size: 123.3%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="2"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mrow data-mjx-texclass="ORD"><mo stretchy="false">|</mo></mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></p>
<p>What the above equation means is that we'll need to find the <strong>probability of the next word given the previous ones in sequence</strong>. Now let's consider the following sentence and let's assume it to be the expected output:-</p>
<p><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="19" style="font-size: 123.3%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mtext class="mjx-n"><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mtext class="mjx-n"><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mtext class="mjx-n"><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>T</mi><mi>h</mi><mi>e</mi><mtext>&nbsp;</mtext><mi>c</mi><mi>a</mi><mi>k</mi><mi>e</mi><mtext>&nbsp;</mtext><mi>w</mi><mi>a</mi><mi>s</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>o</mi><mi>u</mi><mi>r</mi></math></mjx-assistive-mml></mjx-container></p>
<p>If you pass words one by one you prevent seeing the future steps but for efficiency, you'll need to pass all the words together. When you do this you expose the decoder to future info, in order to prevent that we mask the weights of all j(&gt;i) future steps, i being the current time step.</p>
<p>So for the word <strong>the</strong> in the above sentence, we'll mask the weight for words <strong>best</strong> and <strong>pets</strong>, the same way for the word <strong>best</strong> in the above sentence we'll mask the weight for pets. We can do this by adding a mask matrix to this:-</p>
<p><img src="./Transformers_ Attention is all you need_files/vkXCmdGyw.png" alt="Attention Matrix.png"></p>
<p>Implementation-wise, you apply the mask after doing the scaled matrix multiplication of the QV vector in scaled dot product attention. After this you proceed normally, the only difference is this masking step.</p>
<h1 id="heading-training-and-results">Training and Results</h1>
<p><img src="./Transformers_ Attention is all you need_files/s42ytjQf4.png" alt="image.png"></p>
<p>Done! We are done with the architecture. We are officially out of the deadly stuff and now it's just simple stuff. So let's wrap it up. Let's start by going through some details regarding the training:-</p>
<ul>
<li><p><strong>Data:</strong> The WMT English German Dataset contained 4.7 million sentence pairs, with a vocabulary of about 37k tokens. The WMT English French Dataset contained 36 million sentence pairs, which is so big that my hard drive will die before loading it. </p>
</li>
<li><p><strong>Hardware:</strong> Richie Rich people xD. Trained on 8 Nvidia P100 GPU something which make me go bankrupt 100 times. Trained for 100k steps for 12hrs. The bigger variants were trained for 300k steps for 3.5 days.</p>
</li>
<li><p><strong>Optimizer:</strong> They used Adam optimizer with β1 = 0.9, β2 = 0.98 and epsilon = 10^−9. Increasing the learning rate linearly for the first warmup_steps training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. They used warmup_steps = 4000.</p>
</li>
<li><p><strong>Regularization: </strong> Dropout of 0.1 was applied to the sum of the positional and wor embeddings. Dropout was also applied to the output of the sublayer before the residual step. Label smoothing was also applied with epsilon = 0.1. Label Smoothing is a way to prevent the model from being too confident about it's prediction.</p>
</li>
</ul>
<p>That's all as far as training is concerned. Now the architecture I explained had very specific parameters but that's not the only model they tried. They actually tried a bunch of them and noted the result of each in the table below:-</p>
<p><img src="./Transformers_ Attention is all you need_files/YXSCN_Hzb.png" alt="image.png"></p>
<p>As you can see big model performed the best out of all but between big model and base model with a small performance drop, I'll take my chance with the base one first. </p>
<p>That's all! We have finished the paper and reached the end, take a moment and clap for yourself.</p>
<p><img src="./Transformers_ Attention is all you need_files/RzCFUzOdXI.gif" alt="PlayfulUnknownDuckbillcat-max-1mb.gif"></p>
<h1 id="heading-from-me-to-you">From Me to You...</h1>
<p>Wow, that was a big blog and I won't lie it was tough to write. This is probably one of my longest and most experimental blog ever. I was super casual and explained everything as much as possible. At some point I wanted to just stop but every time I just thought, just a bit more. </p>
<p>Transformers are probably one of those revolutionary concepts in deep learning that changed the way we approached stuff and I hope I was able to dissect the paper that started it all. I hope you had fun reading it.</p>
<p>Forever and Always, Herumb.</p>
</div><div class="css-fhxb3m"><div class="css-12n6jp"><div class="css-37oymn"><a href="https://hashnode.com/n/machine-learning" class="css-h6a8j6"><span>#machine-learning</span></a><a href="https://hashnode.com/n/deep-learning" class="css-h6a8j6"><span>#deep-learning</span></a><a href="https://hashnode.com/n/nlp" class="css-h6a8j6"><span>#nlp</span></a><a href="https://hashnode.com/n/python" class="css-h6a8j6"><span>#python</span></a><a href="https://hashnode.com/n/artificial-intelligence" class="css-h6a8j6"><span>#artificial-intelligence</span></a></div></div></div></div></section><div class="blog-content-sidebar css-g5d1mf"><div style="min-height: 654px;"><div class=" sticky" style="transform: translateZ(0px); top: 0px; width: 160px; position: fixed;"><div class="css-1qz4saa"><div class="css-sm43i1"><div class="css-yn21pq"><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="Here&#39;s my like" title="Here&#39;s my like" data-reaction-id="5c090d96c2a9c2a674d35484" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/nznpsvvJs.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643688456%2FnznpsvvJs.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643688456%2FnznpsvvJs.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643688456%2FnznpsvvJs.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643688456%2FnznpsvvJs.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643688456%2FnznpsvvJs.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="Love it" title="Love it" data-reaction-id="5c090d96c2a9c2a674d35485" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/40RNtU1Zj.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643755644%2F40RNtU1Zj.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643755644%2F40RNtU1Zj.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643755644%2F40RNtU1Zj.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643755644%2F40RNtU1Zj.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643755644%2F40RNtU1Zj.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="You&#39;re a unicorn" title="You&#39;re a unicorn" data-reaction-id="5c090d96c2a9c2a674d35486" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/FYDU5k2kQ.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643772437%2FFYDU5k2kQ.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643772437%2FFYDU5k2kQ.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643772437%2FFYDU5k2kQ.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643772437%2FFYDU5k2kQ.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643772437%2FFYDU5k2kQ.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="Claps claps" title="Claps claps" data-reaction-id="567453d0b73d6a82ac8c5abd" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/Z4fzAt9ln.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643783311%2FZ4fzAt9ln.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643783311%2FZ4fzAt9ln.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643783311%2FZ4fzAt9ln.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643783311%2FZ4fzAt9ln.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643783311%2FZ4fzAt9ln.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="Cheers!" title="Cheers!" data-reaction-id="567453d0b73d6a82ac8c5abc" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/wxrlC2BFn.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643793473%2FwxrlC2BFn.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643793473%2FwxrlC2BFn.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643793473%2FwxrlC2BFn.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643793473%2FwxrlC2BFn.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643793473%2FwxrlC2BFn.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="You deserve a trophy!" title="You deserve a trophy!" data-reaction-id="567453d0b73d6a82ac8c5ab9" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/703oh6Yci.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643804463%2F703oh6Yci.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643804463%2F703oh6Yci.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643804463%2F703oh6Yci.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643804463%2F703oh6Yci.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643804463%2F703oh6Yci.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="I am loving it!" title="I am loving it!" data-reaction-id="5c090d96c2a9c2a674d35488" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/9iXxz71TL.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643814744%2F9iXxz71TL.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643814744%2F9iXxz71TL.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643814744%2F9iXxz71TL.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643814744%2F9iXxz71TL.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643814744%2F9iXxz71TL.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="Take my money" title="Take my money" data-reaction-id="5c090d96c2a9c2a674d3548a" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/ByM5eIxJW.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643825482%2FByM5eIxJW.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643825482%2FByM5eIxJW.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643825482%2FByM5eIxJW.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643825482%2FByM5eIxJW.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643825482%2FByM5eIxJW.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="It&#39;s party time" title="It&#39;s party time" data-reaction-id="5c090d96c2a9c2a674d3548b" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/CvUptN73c.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643839183%2FCvUptN73c.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643839183%2FCvUptN73c.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643839183%2FCvUptN73c.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643839183%2FCvUptN73c.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643839183%2FCvUptN73c.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div><div class="css-18nz7be"><button type="button" variant="transparent" aria-label="Fly high" title="Fly high" data-reaction-id="5d9dd859f31791c942dc5b52" class="btn-reaction-spp button-transparent css-1iibo65"><span class="css-o6765v"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNjQiIGhlaWdodD0iNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+"></span><img alt="Post reaction" src="./Transformers_ Attention is all you need_files/cP8rWFP9I.png" decoding="async" data-nimg="intrinsic" class="css-1082qq3" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643854601%2FcP8rWFP9I.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643854601%2FcP8rWFP9I.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x"><noscript><img alt="Post reaction" srcSet="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643854601%2FcP8rWFP9I.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=64&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643854601%2FcP8rWFP9I.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1594643854601%2FcP8rWFP9I.png%3Fh%3D64%26w%3D64%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=128&amp;q=75" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="css-1082qq3" loading="lazy"/></noscript></span></span><span class="css-16ceglb">1</span></button></div></div></div><div class="css-13n4lde"><div class="css-1nszzz1"><a href="https://krypticmouse.hashnode.dev/attention-is-all-you-need#write-comment" variant="transparent" aria-label="Add a comment" class="css-1xh5maz"><svg class="css-16d2z3r" viewBox="0 0 512 512"><path d="M280 272H136c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8h144c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8zm96-96H136c-4.4 0-8 3.6-8 8v16c0 4.4 3.6 8 8 8h240c4.4 0 8-3.6 8-8v-16c0-4.4-3.6-8-8-8zM256 32C114.6 32 0 125.1 0 240c0 47.6 19.9 91.2 52.9 126.3C38 405.7 7 439.1 6.5 439.5c-6.6 7-8.4 17.2-4.6 26S14.4 480 24 480c61.5 0 110-25.7 139.1-46.3C192 442.8 223.2 448 256 448c141.4 0 256-93.1 256-208S397.4 32 256 32zm0 384c-28.3 0-56.3-4.3-83.2-12.8l-15.2-4.8-13 9.2c-23 16.3-58.5 35.3-102.6 39.6 12-15.1 29.8-40.4 40.8-69.6l7.1-18.7-13.7-14.6C47.3 313.7 32 277.6 32 240c0-97 100.5-176 224-176s224 79 224 176-100.5 176-224 176z"></path></svg><span class="css-erhvri"><span class="css-v3j9lg"><svg class="css-egedf7" viewBox="0 0 384 512"><path d="M368 224H224V80c0-8.84-7.16-16-16-16h-32c-8.84 0-16 7.16-16 16v144H16c-8.84 0-16 7.16-16 16v32c0 8.84 7.16 16 16 16h144v144c0 8.84 7.16 16 16 16h32c8.84 0 16-7.16 16-16V288h144c8.84 0 16-7.16 16-16v-32c0-8.84-7.16-16-16-16z"></path></svg></span></span></a></div><div class="css-1oe7b48"><button type="button" variant="transparent" data-title="Bookmark" title="Bookmark" class="tooltip-handle css-d0b8tw"><svg class="css-16d2z3r" viewBox="0 0 384 512"><path d="M336 0H48C21.49 0 0 21.49 0 48v464l192-112 192 112V48c0-26.51-21.49-48-48-48zm16 456.287l-160-93.333-160 93.333V48c0-8.822 7.178-16 16-16h288c8.822 0 16 7.178 16 16v408.287z"></path></svg></button></div><div class="css-1oe7b48"><a href="https://twitter.com/share?url=https%3A%2F%2Fkrypticmouse.hashnode.dev%2Fattention-is-all-you-need&amp;text=Transformers%3A%20Attention%20is%20all%20you%20need%0D%0A%7B%20by%20Herumb%20Shandilya%20%7D%20from%20%40hashnode%0D%0A%0A%23machinelearning%20%23deeplearning%20%23nlp%20%23python%20%23artificialintelligence" variant="transparent" target="_blank" rel="noopener noreferrer" data-title="Say Thanks!" aria-label="Share on Twitter" class="css-1o50nro"><svg class="css-16d2z3r" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div><div class="css-1oe7b48"><button type="button" variant="transparent" aria-label="Toggle social share widget" class="css-1wt4vg7"><svg class="css-16d2z3r" viewBox="0 0 500 500"><path d="M432.31 135.261h-47.672a17.595 17.595 0 0 0-12.442 30.039c3.3 3.3 7.775 5.154 12.442 5.154h30.075v294.353H86.193V170.454h30.085a17.595 17.595 0 0 0 12.442-30.039 17.595 17.595 0 0 0-12.442-5.154H68.596A17.61 17.61 0 0 0 51 152.858v329.546A17.597 17.597 0 0 0 68.596 500H432.31a17.586 17.586 0 0 0 17.596-17.596V152.858a17.597 17.597 0 0 0-17.596-17.597Z" fill="inherit"></path><path d="M204.521 95.101a17.553 17.553 0 0 0 12.81-5.53l26.083-27.652v206.13a17.595 17.595 0 0 0 30.039 12.442c3.3-3.3 5.154-7.775 5.154-12.442V61.809L304.75 89.59a17.609 17.609 0 0 0 12.332 5.711 17.6 17.6 0 0 0 16.733-10.43 17.61 17.61 0 0 0 .301-13.588 17.603 17.603 0 0 0-3.755-5.825L274.997 6.717a18.147 18.147 0 0 0-1.809-2.011A17.51 17.51 0 0 0 263.223.06h-.503l-.955-.06h-2.283c-.271 0-.543.06-.814.09-.272.03-.593.08-.885.141l-.845.171-.824.221c-.282.08-.553.161-.825.262-.271.1-.543.18-.804.291l-.784.332-.785.382-.744.413-.744.442-.724.503-.663.503c-.252.2-.493.402-.724.613l-.413.352-.17.18c-.232.222-.443.453-.664.695-.221.241-.372.392-.543.593-.171.201-.302.382-.453.583L191.771 65.49a17.59 17.59 0 0 0-3.321 18.98 17.588 17.588 0 0 0 16.071 10.632Z" fill="inherit"></path></svg></button></div></div></div></div></div></div></div><div id="refNode1" style="top:100px;left:100px" class="css-16i1tr9">&nbsp;</div><div id="refNode2" class="css-1yzq4q2"> </div><div class="css-pzj4ej"><div class="css-6oz16o"><span class="css-1gc63p3">Share this</span><a href="https://twitter.com/share?url=https%3A%2F%2Fkrypticmouse.hashnode.dev%2Fattention-is-all-you-need&amp;text=%20%40krypticmouse" variant="transparent" target="_blank" rel="noopener noreferrer" class="css-mok5km"><svg class="css-17mdxl4" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a href="http://www.reddit.com/submit?title=Transformers%3A%20Attention%20is%20all%20you%20need&amp;selftext=true&amp;text=%20https%3A%2F%2Fkrypticmouse.hashnode.dev%2Fattention-is-all-you-need" variant="transparent" target="_blank" rel="noopener noreferrer" class="css-15weny5"><svg class="css-17mdxl4" viewBox="0 0 512 512"><path d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"></path></svg></a><button type="button" variant="transparent" data-clipboard-text=" https://krypticmouse.hashnode.dev/attention-is-all-you-need" id="text-sharer" class="css-197yt2c"><svg class="css-17mdxl4" viewBox="0 0 384 512"><path d="M336 64h-88.6c.4-2.6.6-5.3.6-8 0-30.9-25.1-56-56-56s-56 25.1-56 56c0 2.7.2 5.4.6 8H48C21.5 64 0 85.5 0 112v352c0 26.5 21.5 48 48 48h288c26.5 0 48-21.5 48-48V112c0-26.5-21.5-48-48-48zM192 32c13.3 0 24 10.7 24 24s-10.7 24-24 24-24-10.7-24-24 10.7-24 24-24zm160 432c0 8.8-7.2 16-16 16H48c-8.8 0-16-7.2-16-16V112c0-8.8 7.2-16 16-16h48v20c0 6.6 5.4 12 12 12h168c6.6 0 12-5.4 12-12V96h48c8.8 0 16 7.2 16 16z"></path></svg></button></div></div><div class="blog-more-articles css-qlcyx2"><div class="css-8t4x30"><h3 class="blog-more-articles-title css-17kglv3">Read next 📖</h3><div class="blog-more-articles-wrapper css-kneep3"><div class="blog-similar-article css-14q9iu"><div class="blog-similar-article-wrapper css-109nklk"><div class="blog-similar-author-wrapper css-dvss9y"><div class="css-fhxb3m"><div class="css-15gyty5"><a href="https://hashnode.com/@krypticmouse" class=" css-l9iun9" data-title="false"><span style="box-sizing: border-box; display: inline-block; overflow: hidden; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; position: relative; max-width: 100%;"><span style="box-sizing: border-box; display: block; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; max-width: 100%;"><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNzAiIGhlaWdodD0iNzAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+" style="display: block; max-width: 100%; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px;"></span><img alt="Herumb Shandilya&#39;s photo" src="./Transformers_ Attention is all you need_files/yUO-x7u_t(1).jpg" decoding="async" data-nimg="intrinsic" class="css-1yapcyz" style="position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%;" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D70%26h%3D70%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=96&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D70%26h%3D70%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=256&amp;q=75 2x"><noscript></noscript></span></a></div><a href="https://hashnode.com/@krypticmouse" target="_blank" class="blog-similar-author-name css-1j389vi" rel="noreferrer">Herumb Shandilya</a></div></div><a href="https://krypticmouse.hashnode.dev/pytorch-lightning-datamodules-callbacks-tpu-and-loggers" class="blog-similar-article-cover post-cover css-ecfhcu"><span style="box-sizing: border-box; display: block; overflow: hidden; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; position: relative;"><span style="box-sizing: border-box; display: block; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 52.4% 0px 0px;"></span><img alt="Post cover" src="./Transformers_ Attention is all you need_files/8kC_C7AgJ.jpg" decoding="async" data-nimg="responsive" class="css-zowvxu" style="position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%;" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=640&amp;q=75 640w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=750&amp;q=75 750w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=828&amp;q=75 828w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1080w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1200&amp;q=75 1200w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1920&amp;q=75 1920w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2048w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1623170330358%2F8kC_C7AgJ.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=3840&amp;q=75 3840w"><noscript></noscript></span></a><div class="blog-post-details css-1yc9uo8"><h1 class="css-1fno9fi"><a href="https://krypticmouse.hashnode.dev/pytorch-lightning-datamodules-callbacks-tpu-and-loggers" target="_blank" rel="noreferrer">PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers</a></h1><p class="css-1whhcky"><a href="https://krypticmouse.hashnode.dev/pytorch-lightning-datamodules-callbacks-tpu-and-loggers" target="_blank" rel="noreferrer">When I was a young man,
I had liberty but I didn’t see it, 
I had time but I didn’t know it, 
And I …</a></p></div></div></div><div class="blog-similar-article css-14q9iu"><div class="blog-similar-article-wrapper css-109nklk"><div class="blog-similar-author-wrapper css-dvss9y"><div class="css-fhxb3m"><div class="css-15gyty5"><a href="https://hashnode.com/@krypticmouse" class=" css-l9iun9" data-title="false"><span style="box-sizing: border-box; display: inline-block; overflow: hidden; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; position: relative; max-width: 100%;"><span style="box-sizing: border-box; display: block; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; max-width: 100%;"><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNzAiIGhlaWdodD0iNzAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+" style="display: block; max-width: 100%; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px;"></span><img alt="Herumb Shandilya&#39;s photo" src="./Transformers_ Attention is all you need_files/yUO-x7u_t(1).jpg" decoding="async" data-nimg="intrinsic" class="css-1yapcyz" style="position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%;" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D70%26h%3D70%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=96&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D70%26h%3D70%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=256&amp;q=75 2x"><noscript></noscript></span></a></div><a href="https://hashnode.com/@krypticmouse" target="_blank" class="blog-similar-author-name css-1j389vi" rel="noreferrer">Herumb Shandilya</a></div></div><a href="https://krypticmouse.hashnode.dev/class-imbalance-comes-in-like-a-lion" class="blog-similar-article-cover post-cover css-ecfhcu"><span style="box-sizing: border-box; display: block; overflow: hidden; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; position: relative;"><span style="box-sizing: border-box; display: block; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 52.4% 0px 0px;"></span><img alt="Post cover" src="./Transformers_ Attention is all you need_files/L2hp8MGyr.jpg" decoding="async" data-nimg="responsive" class="css-zowvxu" style="position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%;" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=640&amp;q=75 640w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=750&amp;q=75 750w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=828&amp;q=75 828w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1080w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1200&amp;q=75 1200w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1920&amp;q=75 1920w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2048w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1622912007891%2FL2hp8MGyr.png%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=3840&amp;q=75 3840w"><noscript></noscript></span></a><div class="blog-post-details css-1yc9uo8"><h1 class="css-1fno9fi"><a href="https://krypticmouse.hashnode.dev/class-imbalance-comes-in-like-a-lion" target="_blank" rel="noreferrer">Class Imbalance comes in Like a Lion</a></h1><p class="css-1whhcky"><a href="https://krypticmouse.hashnode.dev/class-imbalance-comes-in-like-a-lion" target="_blank" rel="noreferrer">In a world without class imbalance we might've been heroes. 
- Neural Networks

Keeping aside the fa…</a></p></div></div></div><div class="blog-similar-article css-14q9iu"><div class="blog-similar-article-wrapper css-109nklk"><div class="blog-similar-author-wrapper css-dvss9y"><div class="css-fhxb3m"><div class="css-15gyty5"><a href="https://hashnode.com/@krypticmouse" class=" css-l9iun9" data-title="false"><span style="box-sizing: border-box; display: inline-block; overflow: hidden; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; position: relative; max-width: 100%;"><span style="box-sizing: border-box; display: block; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; max-width: 100%;"><img alt="" aria-hidden="true" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNzAiIGhlaWdodD0iNzAiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgdmVyc2lvbj0iMS4xIi8+" style="display: block; max-width: 100%; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px;"></span><img alt="Herumb Shandilya&#39;s photo" src="./Transformers_ Attention is all you need_files/yUO-x7u_t(1).jpg" decoding="async" data-nimg="intrinsic" class="css-1yapcyz" style="position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%;" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D70%26h%3D70%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=96&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1618422399738%2FyUO-x7u_t.png%3Fw%3D70%26h%3D70%26fit%3Dcrop%26crop%3Dfaces%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=256&amp;q=75 2x"><noscript></noscript></span></a></div><a href="https://hashnode.com/@krypticmouse" target="_blank" class="blog-similar-author-name css-1j389vi" rel="noreferrer">Herumb Shandilya</a></div></div><a href="https://krypticmouse.hashnode.dev/training-svm-over-custom-kernels" class="blog-similar-article-cover post-cover css-ecfhcu"><span style="box-sizing: border-box; display: block; overflow: hidden; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 0px; position: relative;"><span style="box-sizing: border-box; display: block; width: initial; height: initial; background: none; opacity: 1; border: 0px; margin: 0px; padding: 52.4% 0px 0px;"></span><img alt="Post cover" src="./Transformers_ Attention is all you need_files/bHB2Bwj0k.jpg" decoding="async" data-nimg="responsive" class="css-zowvxu" style="position: absolute; inset: 0px; box-sizing: border-box; padding: 0px; border: none; margin: auto; display: block; width: 0px; height: 0px; min-width: 100%; max-width: 100%; min-height: 100%; max-height: 100%;" sizes="100vw" srcset="/_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=640&amp;q=75 640w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=750&amp;q=75 750w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=828&amp;q=75 828w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1080&amp;q=75 1080w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1200&amp;q=75 1200w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=1920&amp;q=75 1920w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=2048&amp;q=75 2048w, /_next/image?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1619654326149%2FbHB2Bwj0k.jpeg%3Fw%3D500%26h%3D262%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp&amp;w=3840&amp;q=75 3840w"><noscript></noscript></span></a><div class="blog-post-details css-1yc9uo8"><h1 class="css-1fno9fi"><a href="https://krypticmouse.hashnode.dev/training-svm-over-custom-kernels" target="_blank" rel="noreferrer">Training SVM over Custom Kernels</a></h1><p class="css-1whhcky"><a href="https://krypticmouse.hashnode.dev/training-svm-over-custom-kernels" target="_blank" rel="noreferrer">One thing that always intrigued me about ML is that the more you learn about it the more you realize…</a></p></div></div></div></div></div></div><div class="css-ec6vv8"><div class="blog-comments-section-wrapper article-width css-xl2uon" style="width:800px"><div id="write-comment" class="css-j3y3q"><div class="css-1y1lnb6"><div class="css-yw2bwq"><h3 class="css-1pf55ej">Comments </h3></div><div class="css-kwmtvj"><button type="button" aria-label="Write a comment" class="css-xvvbvv"><svg class="css-1sq5hd" viewBox="0 0 384 512"><path d="M368 224H224V80c0-8.84-7.16-16-16-16h-32c-8.84 0-16 7.16-16 16v144H16c-8.84 0-16 7.16-16 16v32c0 8.84 7.16 16 16 16h144v144c0 8.84 7.16 16 16 16h32c8.84 0 16-7.16 16-16V288h144c8.84 0 16-7.16 16-16v-32c0-8.84-7.16-16-16-16z"></path></svg><span>Write a comment</span></button></div></div></div></div><div class="css-12orxyd"></div></div></article></main></div><footer class="blog-footer-area css-17deu0r"><div class="blog-footer-credits css-11en623"><p class="css-1f7w4n">©&nbsp;<!-- -->2021<!-- -->&nbsp;<!-- -->krypticmouse<!-- -->.&nbsp;See&nbsp;<a href="https://hashnode.com/privacy?source=blog-footer" target="_blank" rel="noopener" class="css-1wbll7q">privacy policy</a>&nbsp;and&nbsp;<a href="https://hashnode.com/terms?source=blog-footer" target="_blank" rel="noopener" class="css-1wbll7q">terms</a>.</p><div class="css-zigog8"><a aria-label="Publish with Hashnode" href="https://hashnode.com/onboard?unlock-blog=true&amp;source=blog-footer" target="_blank" rel="noopener" class="css-11pvioa"><span class="css-pstqnw"><svg class="css-17mdxl4" viewBox="0 0 200 200" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M13.742 66.824c-18.323 18.323-18.323 48.029 0 66.352l53.082 53.082c18.323 18.323 48.029 18.323 66.352 0l53.082-53.082c18.323-18.323 18.323-48.03 0-66.352l-53.082-53.082c-18.323-18.323-48.03-18.323-66.352 0L13.742 66.824zm109.481 56.399c12.826-12.826 12.826-33.62 0-46.446s-33.62-12.826-46.446 0-12.826 33.62 0 46.446 33.62 12.826 46.446 0z"></path></svg></span><span>Publish with Hashnode</span></a><p class="css-kd754l">Powered by<!-- --> <a aria-label="Hashnode" href="https://hashnode.com/?source=blog-footer" target="_blank" rel="noopener" class="css-1wbll7q">Hashnode</a> <!-- -->- a blogging community for software developers.</p></div></div></footer></div></div><script type="text/javascript">
              var SUPPORTS_PASSIVE = false;
              try {
                var opts = Object.defineProperty({}, 'passive', {
                  get: function() {
                    SUPPORTS_PASSIVE = true;
                  }
                });
                window.addEventListener("testPassive", null, opts);
                window.removeEventListener("testPassive", null, opts);
              } catch (e) {}
            </script><script type="text/javascript">
              // Array.prototype.flat polyfill
              if (!Array.prototype.flat) {
                // eslint-disable-next-line no-extend-native
                Object.defineProperty(Array.prototype, 'flat', {
                  configurable: true,
                  writable: true,
                  value() {
                    // eslint-disable-next-line prefer-rest-params
                    const depth = typeof arguments[0] === 'undefined' ? 1 : Number(arguments[0]) || 0;
                    const result = [];
                    const { forEach } = result;
          
                    // eslint-disable-next-line no-var
                    var flatDeep = function (arr, depth) {
                      forEach.call(arr, (val) => {
                        if (depth > 0 && Array.isArray(val)) {
                          flatDeep(val, depth - 1);
                        } else {
                          result.push(val);
                        }
                      });
                    };
          
                    flatDeep(this, depth);
                    return result;
                  },
                });
              }
            </script><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":"{\"_id\":\"61b288ed1907c16be2ee1209\",\"reward\":{\"type\":\"xlm\"},\"sourcedFromGithub\":false,\"isAnonymous\":false,\"views\":75,\"slugOverridden\":true,\"coverImage\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638987431300/0yWP8XDYJ.png\",\"autoGeneratedCover\":\"\",\"brief\":\"Attention is Transformer’s breath,\\nMulti-Head is Transformer’s release,\\nRNNs thou wert and art,\\nMay thy model reach to greater accuracy.\\nLátom.\\n - Enen No Shouboutai\\n\\nI'm getting tired of butchering anime and video game quotes so I'm thinking I shoul...\",\"content\":\"\u003cblockquote\u003e\\n\u003cp\u003eAttention is Transformer’s breath,\u003c/p\u003e\\n\u003cp\u003eMulti-Head is Transformer’s release,\u003c/p\u003e\\n\u003cp\u003eRNNs thou wert and art,\u003c/p\u003e\\n\u003cp\u003eMay thy model reach to greater accuracy.\u003c/p\u003e\\n\u003cp\u003eLátom.\u003c/p\u003e\\n\u003cp\u003e - Enen No Shouboutai\u003c/p\u003e\\n\u003c/blockquote\u003e\\n\u003cp\u003eI'm getting tired of butchering anime and video game quotes so I'm thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper. \u003c/p\u003e\\n\u003cp\u003eI mean I've been seeing blogs that go by the name of \u003cstrong\u003ePaper Dissected\u003c/strong\u003e so I wanted to try writing one myself. Well this is going to be my first try in this and frankly, I have no idea how it'll be but let's give it a try, shall we?\u003c/p\u003e\\n\u003cp\u003eI guess it's safe to say that Attention Mechanism and Transformers is something that recently took over NLP. Not only did it show improvements over the SOTA models at the time but also overcome the shortcoming of the RNN models like LSTM and GRU.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1631918244839/oF2h1CrF8.jpeg\\\" alt=\\\"1610374526-cheems1 (2).jpg\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eSo let's go ahead and break down the sections of the paper, \u003cstrong\u003eAttention is all you need\u003c/strong\u003e. To give you a gist of what'll be there, it'll be an explanation of each section in the paper like abstract, introduction, model architecture, etc.\u003c/p\u003e\\n\u003ch1 id=\\\"heading-abstract\\\"\u003eAbstract\u003c/h1\u003e\\n\u003cp\u003eThis section pretty much summarizes the whole paper not in terms of working but rather in terms of what it has to offer. It starts with explaining how Seq2Seq models usually use RNN and CNN for encoder and decoder, and how the best ones connect encoder and decoder via attention mechanism.\u003c/p\u003e\\n\u003cp\u003eNow just to be clear, attention itself is not a new concept but to rely completely on attention is something that this paper introduced. It went ahead explaining the results achieved on the machine translation task which were like the following on WMT-2014 dataset:-\u003c/p\u003e\\n\u003cdiv class=\\\"hn-table\\\"\u003e\\n\u003ctable\u003e\\n\u003cthead\u003e\\n\u003ctr\u003e\\n\u003ctd\u003eMT - Task\u003c/td\u003e\u003ctd\u003eBLEU Score\u003c/td\u003e\u003c/tr\u003e\\n\u003c/thead\u003e\\n\u003ctbody\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003cstrong\u003eEnglish to German\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e28.4\u003c/td\u003e\u003c/tr\u003e\\n\u003ctr\u003e\\n\u003ctd\u003e\u003cstrong\u003eEnglish to French\u003c/strong\u003e\u003c/td\u003e\u003ctd\u003e41.8\u003c/td\u003e\u003c/tr\u003e\\n\u003c/tbody\u003e\\n\u003c/table\u003e\\n\u003c/div\u003e\u003cp\u003eIt also took 3.5 days and 8 GPUs to train on, which is certainly out of my budget. But the main thing that shined a bit differently than others in the abstract section was:-\u003c/p\u003e\\n\u003cblockquote\u003e\\n\u003cp\u003eExperiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train.\u003c/p\u003e\\n\u003c/blockquote\u003e\\n\u003cp\u003eWe saw how results were better than traditional models but the thing that separated transformers from LSTM is parallel computation. This can feel a bit tricky to understand at the start so let's break it down. By nature, RNN models aren't parallel but rather Sequential and this is because they compute results one step at a time.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1631732056936/0U5QBxJEa.png\\\" alt=\\\"YjlBt.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eHowever, in the Transformer model, there is no such thing as \\\"time steps\\\" but rather the inputs are passed all together and all this is made possible due to multi-headed attention and other stuff which we'll talk about later.\u003c/p\u003e\\n\u003ch1 id=\\\"heading-introduction\\\"\u003eIntroduction\u003c/h1\u003e\\n\u003cp\u003eThis section mainly talks about the working of RNNs and it's limitations, along with the recent developments to tackle them. Now that we are at it let's understand how does Seq2Seq modeling working in the case of RNNs and to make it easier, take a look a look at the picture below:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1631910532329/yoH-0RbHk.png\\\" alt=\\\"Seq2Seq Blog Img #1.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eSeq2Seq Models consists of 2 models i.e. an Encoder and a Decoder. The Encoder model takes the text as input and generates a Context Vector as an output which is nothing but the last hidden state of RNN. Encoders and Decoders are nothing but RNNs and in few instances CNN 🌚. \u003c/p\u003e\\n\u003cp\u003eAnother thing you'll notice is the crosses above RNN cells except the last one that doesn't mean they aren't produced, they are but we just don't use them and all we'll be doing is to take the context vector from the last time step and feeding it to the decoder as input who'll output each word at a time giving us the resultant text. \u003c/p\u003e\\n\u003cp\u003eIn attention, however, you'll be taking the outputs of all the time step of encoder so you don't discard in that case. I mean you can also do mean pooling on all the states and pass that as context vector, so attention isn't the only way that uses all the states. But how does the decoder predict the words? Let's see, I'll try to be brief.\u003c/p\u003e\\n\u003cp\u003eDuring inference the first token is the special token signifying the start of the sentence along with context vector from the encoder, after that you pass previous output as input to the decoder along with the hidden state to generate new output, this is known as \u003cem\u003e*Teacher Forcing\u003c/em\u003e. \u003c/p\u003e\\n\u003cp\u003eThis way you'll use generated output as input to generate the next token, and it's cool except the output is being constructed on the context of inferred output rather than actual ones giving rise to the problem of \u003cstrong\u003eexposure bias\u003c/strong\u003e. Which basically means our model's output will be messed up if one of the outputs is bad.\u003c/p\u003e\\n\u003cp\u003eBack to the topic, the nature of RNNs is sequential which makes parallelization difficult for training, making RNN models take more time to train. However, there are few developments that tackle it:-\u003c/p\u003e\\n\u003cblockquote\u003e\\n\u003cp\u003e Recent work has achieved significant improvements in computational efficiency through \u003cstrong\u003efactorization tricks\u003c/strong\u003e and \u003cstrong\u003econditional computation\u003c/strong\u003e, while also improving model performance in the case of the latter. The fundamental constraint of sequential computation, however, remains.\u003c/p\u003e\\n\u003c/blockquote\u003e\\n\u003cp\u003eTo understand the factorization trick you can go ahead and read another paper, i.e. \u003cstrong\u003eFactorization Tricks for LSTM Networks\u003c/strong\u003e by Oleksii and Boris, which I recommend but I'll give you the gist of it.\u003c/p\u003e\\n\u003cblockquote\u003e\\n\u003cp\u003e The major part of LSTMP cell computation is in computing affine transform T because it involves multiplication with 4n × 2p matrix W.\u003c/p\u003e\\n\u003c/blockquote\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1631916140101/s9rWIKqZA.png\\\" alt=\\\"image.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eThe aim of the factorization trick is to approximate that W by replacing it with the product of 2 matrices W1(2p*r) and W2(r*4n), this way takes fewer parameters (2p\u003cem\u003er + r\u003c/em\u003e4n) as compared to W (2p*4n) given r \u0026lt; p \u0026lt;=n. Another factorization trick proposes to think parts of inputs and the hidden states as independent of each other and calculate affine transforms of these groups individually and parallelly. \u003c/p\u003e\\n\u003cp\u003eI know it's a lot to grasp so if you don't understand it all at the moment it's fine, important thing to understand is that \u003cstrong\u003emany attempts have been made to improve RNNs but the sequential nature isn't completely removed\u003c/strong\u003e. The attention model did help in capturing long-term dependencies and improve results, however, it was used along with RNNs. \u003c/p\u003e\\n\u003cp\u003eThere is another concept of conditional computation that aims at activating parts of the network to improve computational efficiency, and performance too in this case, but let's not dive much into it. But if you want to you can read this \u003ca target=\\\"_blank\\\" href=\\\"https://trace.tennessee.edu/cgi/viewcontent.cgi?article=5323\u0026amp;context=utk_graddiss\\\"\u003e\u003cstrong\u003eshort book\u003c/strong\u003e\u003c/a\u003e on the same.\u003c/p\u003e\\n\u003cp\u003eThe transformer model on the other hand relies entirely on attention mechanism allowing it to be more parallelizable and give better results with the cost of training being 12 hours on 8 P100 GPUs, something that even Colab Pro users can't get their hands on all the time anymore 🥲.\u003c/p\u003e\\n\u003ch1 id=\\\"heading-model-architecture\\\"\u003eModel Architecture\u003c/h1\u003e\\n\u003cp\u003eFinally, we've come to the real deal. Now while I wanna try and break down the explanation sub-section wise, I know that may or may not end up leaving you kinda confused, and hence instead of explaining each sub-section of this section we'll dismantle the architecture step by step and understand how it works.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1632343488045/17Un25edDk.png\\\" alt=\\\"image.png\\\" /\u003e\u003c/p\u003e\\n\u003ch2 id=\\\"heading-input-embeddings\\\"\u003eInput Embeddings\u003c/h2\u003e\\n\u003cp\u003eLet's start from the beginning, what you have with you is a sentence or bunch of sentences. Now you can't exactly feed it to the model as it is so you need a way to convert it to a form more suitable for it. When we used RNNs for text classification or seq2seq we convert our text to \u003cstrong\u003eword embeddings\u003c/strong\u003e.\u003c/p\u003e\\n\u003cp\u003eThese embeddings are nothing but vector representations of words such that words with similar meanings are closer to each other. But what does closer mean? Mathematically, closer means the similarity score is high. This score can be anything but cosine similarity is popular so feel free to think that. Once we have our embeddings to use we are all set! Are we?\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1632351034260/Qo6JVLncJ.jpeg\\\" alt=\\\"4824982 (1).jpg\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eTechnically, yes we are since these embeddings can be used with RNNs and CNNs because they are able to capture the ordering into consideration. But Transformers use only self-attention, which takes all the words at the same time hence it is not able to take the order of words into consideration. To fix this problem we introduce something called \u003cstrong\u003epositional encodings\u003c/strong\u003e.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-positional-encodings\\\"\u003ePositional Encodings\u003c/h2\u003e\\n\u003cp\u003eSo as mentioned before, inherently attention model is not able to take the order of words into consideration. In order to compensate for this, we use something called Positional Encodings. Don't be intimidated by the name it's actually much more simple. Let's dive a bit deeper into this.\u003c/p\u003e\\n\u003ch3 id=\\\"heading-need-of-positional-encodings\\\"\u003eNeed of Positional Encodings\u003c/h3\u003e\\n\u003cp\u003eWhen working with text it's really important to take words order into consideration. Why? The order of words defines the structure of the sentence and this structure helps in providing the context. \u003c/p\u003e\\n\u003cp\u003eRNNs by nature are sequential in nature by taking inputs one by one and updating hidden states accordingly, but the same is not the case with attention since it takes the words all at once. Why this is so, is something we'll understand in the next section but for now, let's say that's how it is.\u003c/p\u003e\\n\u003cp\u003eSo we need some extra sources of information in our embeddings to convey this information, something that could tell the position of a word in a sentence. To do so we can add an extra embedding or matrix to our word embeddings to provide info regarding the ordering of the word. But how? \u003c/p\u003e\\n\u003ch3 id=\\\"heading-finding-positional-encodings\\\"\u003eFinding Positional Encodings\u003c/h3\u003e\\n\u003cp\u003eWell in the most simple case you can use a one-hot encoded form of [0,1,2,3,4,5,..., pos] as positional encodings. You can train an embedding layer to find the \u003cstrong\u003epositional embedding\u003c/strong\u003e matrix, where each row has the positional embedding for word at pos index, for this and add it to input embeddings.\u003c/p\u003e\\n\u003cp\u003eIn the paper, however, the proposed approach uses a combination of sin and cos function to find the encodings. Take a look at the following picture:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1632693800233/lcT9IBipy.png\\\" alt=\\\"i (3).png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eAs you can see in the above picture the encodings are found by using a combination of sin and cos function. The function is quite self-explanatory, it says that for the embedding of a word at position \u003cstrong\u003epos\u003c/strong\u003e:- \u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003eThe value at index \u003cstrong\u003e2i\u003c/strong\u003e is given using the \u003cstrong\u003esin\u003c/strong\u003e function, colored red. \u003c/li\u003e\\n\u003cli\u003eThe value at index \u003cstrong\u003e2i +1\u003c/strong\u003e is given using the \u003cstrong\u003ecos\u003c/strong\u003e function, colored blue.\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eBut why are we using this particular function? Well let's hear from the authors themselves:-\u003c/p\u003e\\n\u003cblockquote\u003e\\n\u003cp\u003eWe chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions since, for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.\u003c/p\u003e\\n\u003c/blockquote\u003e\\n\u003cp\u003eWell, researchers sure love to make stuff seem more complicated, don't they? It's fine if you don't understand the above sentence, take it as a confirmation that you're still sane. \u003c/p\u003e\\n\u003cp\u003eWhat the sentence means to say is that given the positional encoding of the word at \u003cstrong\u003epos\u003c/strong\u003e you can find the positional encoding of word at \u003cstrong\u003epos + k\u003c/strong\u003e by applying some linear transformation on the positional encoding of \u003cstrong\u003epos\u003c/strong\u003e. Mathematically, it can be written like this:-\u003c/p\u003e\\n\u003cp\u003e\\\\[\\nPE_{p+k} = T * PE_p\\n\\\\]\u003c/p\u003e\\n\u003cp\u003eWriting \u003cstrong\u003epos\u003c/strong\u003e as \u003cstrong\u003ep\u003c/strong\u003e in the above equation since latex is getting messed up for some reason. The important thing to know is that the matrix T can be used to find PE at pos+k but what is this T? Can we find it? Well yes, that T is a rotational matrix that depends on offset k rather than pos. Wait what! How? \u003c/p\u003e\\n\u003cp\u003eWell, you can derive it, and here is a really good \u003ca target=\\\"_blank\\\" href=\\\"https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\\\"\u003eblog by Amirhossein\u003c/a\u003e doing the same. There is another \u003ca target=\\\"_blank\\\" href=\\\"https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/\\\"\u003eblog by Timo Denk\u003c/a\u003e that proves the linear relationship between PE at pos and pos+k but the first one explains in a much simpler fashion, but do give both a read.\u003c/p\u003e\\n\u003ch3 id=\\\"heading-improving-a-bit-more\\\"\u003eImproving a bit more\u003c/h3\u003e\\n\u003cp\u003eWow so everything is dope, right? Fortunately or unfortunately, No. Even though everything might seem super nice with these encodings, which it is, what these really are doing is finding values for given pos and i values, these are also called \u003cstrong\u003eAbsolute Positional Encodings\u003c/strong\u003e. While the altering of sin and cosine might be able to add relative nature, we might actually have something better i.e. \u003cstrong\u003eRelative Positional Encodings\u003c/strong\u003e.\u003c/p\u003e\\n\u003cp\u003eFollowing this paper, there was another paper released i.e. \u003cstrong\u003eSelf-Attention with Relative Position Representations\u003c/strong\u003e by \u003ca target=\\\"_blank\\\" href=\\\"https://arxiv.org/pdf/1803.02155.pdf\\\"\u003eShaw et. al. 2019\u003c/a\u003e. This paper introduces a modified attention mechanism which is mostly the same except this one has 2 matrices that are added to Key and Value Vector and the aim is that they'll be able to induce pairwise relationships in the inputs. In the author's own words:-\u003c/p\u003e\\n\u003cblockquote\u003e\\n\u003cp\u003eWe propose an extension to self-attention to consider the pairwise relationships between input elements. In this sense, we model the input as a labeled, directed, fully connected graph.\u003c/p\u003e\\n\u003c/blockquote\u003e\\n\u003cp\u003eThe paper is actually a pretty decent read and you should give it a try. Since this paper doesn't use these embeddings we'll not go into the detail of these. There are many papers that try to come up with better Positional Encodings and try to improve them, but what they are used for and proper intuition is something you should aim to understand.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-attention\\\"\u003eAttention\u003c/h2\u003e\\n\u003cp\u003eAttention is something that we've been talking about a lot in this blog, after all that's what this paper is about. But what does it actually mean? So before diving into the proposed method for attention calculation in this paper, we'll go ahead and understand what attention is and how it helps our cause.\u003c/p\u003e\\n\u003cp\u003eSo when we do Seq2Seq with RNNs we have an encoder and decoder network in which the work of the decoder is to generate a sentence given \u003cstrong\u003econtext vector\u003c/strong\u003e. This context vector is nothing but the last hidden state of the encoder which is RNN based model. So what's the issue? Well, you see:-\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eThe \u003cstrong\u003econtext vector\u003c/strong\u003e is a vector of fixed length defined by hidden_dims but the sentences are of variable length and so what we are doing is squeezing info into a vector due to which information might be lost.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eAnother problem is that the generated sentences are based on the context of the whole sentence, however in doing so we might lose out on the local context of parts of the sentence that might've been helpful in tasks like translation.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eSo in order to improve on this, we introduce \u003cstrong\u003eattention\u003c/strong\u003e and instead of working on the global context of the sentence by taking all the hidden states in the encoder and passes it to the decoder. The decoder, in this case, doesn't produce the output directly instead it'll calculate the attention score for the states and creates the context vector for that particular time step given the attention score, the encoder hidden states, and the input vector, output of prev. step or last input token.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1633217192689/yLRmt7ySf.png\\\" alt=\\\"Normal Decoder.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eBut how does this help? Well, what attention does is that it generates a context vector at each time step of the decoder based on attention score and this attention score helps to amplify the more important words to focus on during that time step in order to improve the quality of results. If you calculate attention among the words of the input sequence it's called \u003cstrong\u003eSelf Attention\u003c/strong\u003e and when you calculate attention among the words of the input and output sequence it's called \u003cstrong\u003eGeneral Attention\u003c/strong\u003e.\u003c/p\u003e\\n\u003cp\u003eNow based on the architecture you have 2 types of attention-based Seq2Seq models i.e. \u003cstrong\u003eBahdanau Attention\u003c/strong\u003e and \u003cstrong\u003eLuong Attention\u003c/strong\u003e. Except the architecture and computation both are basically the same but if you wanna learn more you can go ahead and read the paper, \u003cstrong\u003eEffective Approaches to Attention-based Neural Machine Translation\u003c/strong\u003e by \u003ca target=\\\"_blank\\\" href=\\\"https://arxiv.org/abs/1508.04025\\\"\u003eLuong et. al. 2015\u003c/a\u003e.\u003c/p\u003e\\n\u003ch3 id=\\\"heading-scaled-dot-product-attention\\\"\u003eScaled Dot-Product Attention\u003c/h3\u003e\\n\u003cp\u003eWe have a basic idea of how attention works in RNN now, but we don't exactly have an RNN in Transformer or a way to sequentially input data. So the proposed method for calculating attention is via something called \u003cstrong\u003eScaled Dot-Product Attention\u003c/strong\u003e, which is basically a bunch of matrix multiplication operations. \u003c/p\u003e\\n\u003cp\u003eBut before diving in, let's talk about QKV or Query-Key-Value vectors and their dimensions. Throughout the model, the dimension of columns of the resultant matrix from each layer is set to \u003cstrong\u003e512\u003c/strong\u003e which is denoted by \\\\( d_{model} \\\\). \u003c/p\u003e\\n\u003cp\u003eSo if we have \u003cstrong\u003em\u003c/strong\u003e rows in word embedding we'll have the resultant matrix of dimension \\\\( m * d_{model} \\\\), this column dimension is consistent for all the layers in the model including the embedding layer. \u003c/p\u003e\\n\u003cp\u003eThis is done to facilitate the residual connections, which we'll talk about later. Now that we have output dims cleared let's talk about columns of QKV vectors.\u003c/p\u003e\\n\u003cp\u003eFor now, just remember the dimension of QKV vectors being 64, the reasoning is something we'll discuss in Multiheaded attention. The dimensions of Query and Key vector are the same and are denoted by \\\\( d_k \\\\), and even though here it is same the dimensions of value vector can be different and is denoted by \\\\( d_v \\\\). So to sum it all up we'll have the following notation for dimensions:-\u003c/p\u003e\\n\u003cp\u003e$$\\nd_{model} = 512\\n$$\u003c/p\u003e\\n\u003cp\u003e$$\\nd_q = d_k = 64\\n$$\u003c/p\u003e\\n\u003cp\u003e$$\\nd_v = 64\\n$$\u003c/p\u003e\\n\u003cp\u003eNow that we have the important stuff cleared, let's take a look at the following image that sums up the process of calculating attention:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1632788371388/1NFBKy8H8D.png\\\" alt=\\\"S C A L E.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eIt might seem confusing so let's go in step by step on how attention is being calculated and how its equation is constructed:-\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eYou start by finding \u003cem\u003eQuery\u003c/em\u003e, \u003cem\u003eKey\u003c/em\u003e, and \u003cem\u003eValue\u003c/em\u003e vector by multiplying the input matrix with their corresponding weight vector. The dims of these vectors are something we'll discuss in Multi Headed Attention Section. For now, let's just focus on understanding the equation.\\n$$\\nAttention(Q, K, V)\\n$$\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eApply Matrix Multiplication on \u003cem\u003eQuery\u003c/em\u003e and \u003cem\u003eKey\u003c/em\u003e Vector.\\n$$\\nAttention(Q, K, V) = QK^T\\n$$\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eScale the result of above with \\\\( \\\\sqrt{d_k} \\\\). It was observed that for small values of \\\\( d_k \\\\) additive attention and dot product attention perform similarly. But for larger values of \\\\( d_k \\\\) additive attention outperforms dot product attention without scaling. As mentioned in paper, that for large value the result may explode and may push softmax to smaller gradients. To counteract this \\\\( \\\\frac{1}{\\\\sqrt{d_k}} \\\\) was used for scaling. As to why this occurs, paper mentions:-\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cblockquote\u003e\\n\u003cp\u003eTo illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product has mean 0 and variance d_k.\u003c/p\u003e\\n\u003c/blockquote\u003e\\n\u003cp\u003e$$\\nAttention(Q, K, V) = \\\\frac{QK^T}{\\\\sqrt{d_k}}\\n$$\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eApplying softmax on the result of the above.\\n$$\\nAttention(Q, K, V) = softmax(\\\\frac{QK^T}{\\\\sqrt{d_k}})\\n$$\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eApply Matrix Multiplication on result and \u003cem\u003eValue\u003c/em\u003e Vector and get the final result.\\n$$\\nAttention(Q, K, V) = softmax(\\\\frac{QK^T}{\\\\sqrt{d_k}})V\\n$$\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eThere you go that's how you find attention using Scaled Dot Product Attention. Just for the info, this is not the absolute way to find attention, there are other ways to calculate attention like additive attention, dot-product(unscaled) attention. While the two are similar in complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\u003c/p\u003e\\n\u003cp\u003eNow let's take a look at the newly proposed approach where instead of calculating attention one time we calculate it multiple times and then concatenate the results. Yep, it is indeed \u003cstrong\u003eMulti-Headed Attention\u003c/strong\u003e.\u003c/p\u003e\\n\u003ch3 id=\\\"heading-multi-headed-attention\\\"\u003eMulti-Headed Attention\u003c/h3\u003e\\n\u003cp\u003eLet's talk about the beast that the paper proposed and when I say \\\"beast\\\" I mean it. The paper suggested that instead of calculating attention one time we can calculate it  \u003cstrong\u003eh\u003c/strong\u003e times with d_q, d_k, d_v dimensioned QKV vectors found after passing each of them from a Linear Layer.\u003c/p\u003e\\n\u003cp\u003eWe then use them to calculate attention values from each attention head depicted by head\u003cem\u003ei and concatenate these heads forming \\\\( d\u003c/em\u003e{model} \\\\) dimensional resultant vector. After that, we'll pass them through another linear layer to get our Multi-Headed Attention output. Architecture wise it looks like this:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638644522204/RBUNc_MHH.png\\\" alt=\\\"Linear.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eOne thing to note is that the value of d_q, d_v, and d_k will be \u003cstrong\u003ed_model / h\u003c/strong\u003e, that's why after you concatenate, the attention head's output it becomes d_model sized vector. In this paper, the value of \u003cstrong\u003eh\u003c/strong\u003e for the optimal model was 8. So d_k, d_v, and d_q become 512/8 = 64.\u003c/p\u003e\\n\u003ch4 id=\\\"heading-why-does-it-work\\\"\u003eWhy does it Work?\u003c/h4\u003e\\n\u003cp\u003eBut after hearing all this it's only sensible to ask. Why does this even work? To answer this query, we should go and take a look at the paper \u003cstrong\u003e\\\"Analyzing Multi-Head Self-Attention\\\"\u003c/strong\u003e by \u003ca target=\\\"_blank\\\" href=\\\"https://arxiv.org/pdf/1905.09418.pdf\\\"\u003eVoita et. al. 2019\u003c/a\u003e. Where they basically try to find the roles that different attention heads played along with other stuff which we'll talk about later i.e. Attention Head Pruning.\u003c/p\u003e\\n\u003cp\u003eTo explain the gist of it the conclusion was, that based on roles attention heads can be divided into 3 parts:-\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003ePositional Heads:\u003c/strong\u003e Heads where the maximum attention weights was assigned to a specific relative position. This was usually +/- 1 in practice signifying attention to adjacent positions\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSyntactical Heads: \u003c/strong\u003e Head attends to tokens corresponding to any of the major syntactic relations in a sentence. In the paper, they analyze direct relations like nominal subject, direct object, adjectival modifier, and adverbial modifier.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eRare Word Heads\u003c/strong\u003e: Heads attend to the least frequent tokens in a sentence. \u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eSo different heads try to perform different task cool, but do we need all of them? We have only 3 types right? Does that mean other heads are useless? Yes and No. Let's understand the \u003cstrong\u003eYes\u003c/strong\u003e part first. \u003c/p\u003e\\n\u003cp\u003eYes, attention heads are important and in fact, that's exactly what the blog \u003cstrong\u003e\\\"Are Sixteen Heads Really Better than One?\\\"\u003c/strong\u003e by \u003ca target=\\\"_blank\\\" href=\\\"https://blog.ml.cmu.edu/2020/03/20/are-sixteen-heads-really-better-than-one/\\\"\u003ePaul Michael\u003c/a\u003e tries to find. In the blog, he tried to explain how the performance was affected when attention heads were pruned. After the experiment they found the following result:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638650058322/s9kG6ry4m.png\\\" alt=\\\"image.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eAs you can see the BLEU Score is really affected after pruning the heads however like other metrics the dip is observed after more than 60% heads are pruned. This leads us to the \u003cstrong\u003eNo\u003c/strong\u003e part. I mean kinda No.\u003c/p\u003e\\n\u003cp\u003eEven though we just saw that heads are important we can also prune one or a few of them without seeing a major dip in the performance, something that both the mentioned sources, that I highly recommend you to go through, suggested. Phew that was a lot, now that we have the initial stuff cleared up, let's get back to the architecture. I promise this point forward is a smooth sailing simple life, probably xD. \u003c/p\u003e\\n\u003ch2 id=\\\"heading-encoder\\\"\u003eEncoder\u003c/h2\u003e\\n\u003cp\u003eIf we try to understand the architecture of Encoder it can be broken into 2 parts:-\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003eAttention Layer\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003eFeed Forward Layer\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eYes that's it, you are already familiar with Attention Layer which is nothing but Multi Headed Attention with 8 heads and the Feed Forward Layer is a basic Neural Network with a single hidden layer with 2048 nodes and 512 nodes in input and output layer. Visually it'll look something like this:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638657930173/RL6ONG63c.png\\\" alt=\\\"Multi-Headed Attention.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eIts flow of Infomation is really simple, or not depending on your knowledge of skip connections. If I have to explain in one line skip connection is basically \u003cstrong\u003eadding input to the output\u003c/strong\u003e. \u003c/p\u003e\\n\u003cp\u003eI know it sounds weird, but trust me it makes sense. ResNet model was the one that popularised these skip connections which provides an alternate path for gradients to flow. They helped train deeper models without running into vanishing gradient problem. So if x is the input to a block and f(x) is the output of the block then skip connections basically add them both making \u003cstrong\u003ex + f(x)\u003c/strong\u003e the new output.\u003c/p\u003e\\n\u003cp\u003e$$\\nf'(x) = x + f(x)\\n$$\u003c/p\u003e\\n\u003cp\u003eNow that we are all on the same page, let's step by step go through the flow of info in an encoder block:-\u003c/p\u003e\\n\u003col\u003e\\n\u003cli\u003eEncoder block receives an input \u003cstrong\u003eX\u003c/strong\u003e.\u003c/li\u003e\\n\u003cli\u003eX is supplied to Multi-Headed Attention Layer. QKV all will be the same i.e. \u003cstrong\u003eX\u003c/strong\u003e.\u003c/li\u003e\\n\u003cli\u003eMulti Headed Attention does the magic and generates an output M(x).\u003c/li\u003e\\n\u003cli\u003eAdd X to M(x) because of skip connection.\u003c/li\u003e\\n\u003cli\u003eApply Layer Normalisation to X + M(x), let's call this N(x).\u003c/li\u003e\\n\u003cli\u003ePass N(x) to Feed Forward layer to generate an output F(x).\u003c/li\u003e\\n\u003cli\u003eAdd N(x) in F(x) because of skip connection again.\u003c/li\u003e\\n\u003cli\u003eApply Layer Normalisation to F(x) + N(x), let's call this E(x) which is the output of this encoder block.\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003cp\u003eOne last thing, the above is flow of info for one encoder block. This encoder block's output will be passed on to the next encoder block and the architecture of all encoder blocks is the same. The paper proposed the architecture with \u003cstrong\u003e6 encoder blocks\u003c/strong\u003e. I swear I need to learn animation one of these days, would make stuff much much easier.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638659972564/YTnCToXMN.png\\\" alt=\\\"image.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eYea I know too much info xD, but hey that's all that you need to know about Encoders. The output of the last encoder block is passed to the decoder as KV vector values, which is our next topic of discussion. We are almost near the end, so hang in there 😬.\u003c/p\u003e\\n\u003ch2 id=\\\"heading-decoder\\\"\u003eDecoder\u003c/h2\u003e\\n\u003cp\u003eFinally, we are at the last step of the architecture, to be honest decoder is not really that different from than encoder except the fact that here were have an extra layer for masked multi-headed attention. We'll get into the details of that in a bit, but let's get an overview of how a decoder block looks:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638738474914/aVah0DKWI.png\\\" alt=\\\"Decoder (1).png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eBefore going into the flow of information let's understand exactly what is the input and output of the decoder block. As far as input is concerned the decoder takes 2 inputs:-\u003c/p\u003e\\n\u003col\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eEncoder Output:\u003c/strong\u003e This is will be fed as KV Vectors for multi-headed attention in the 2nd layer, this is usually referred to as Encoder-Decoder Attention. All \u003cstrong\u003e6 Decoder Blocks\u003c/strong\u003e will take the encoder output from the last block as input for encoder-decoder attention.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eOutput Embedding:\u003c/strong\u003e These are the embeddings of the target sentence. But wait!? We don't have a target in testing! So what are we inputting here then? Let's see.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003cp\u003eWell usually the first input to the decoder is a special token signifying the start of the output sentence and the output is the token that comes after this. This output is the output of time step 1. \u003c/p\u003e\\n\u003cp\u003eAfter that, the input to decoder becomes the tokens until a certain timestep i.e. for the 5th timestep input will be all generated tokens till the 4th timestep. After that, it'll keep on giving an output until it throws a special token signifying the end of the output sentence.\u003c/p\u003e\\n\u003cp\u003eLet's understand the flow of information in decoder block to get a better understanding:-\u003c/p\u003e\\n\u003col\u003e\\n\u003cli\u003eDecoder block receives an input \u003cstrong\u003eX\u003c/strong\u003e.\u003c/li\u003e\\n\u003cli\u003eX is supplied to Masked Multi-Headed Attention as QKV vector, to get an output M(x).\u003c/li\u003e\\n\u003cli\u003eAdd X in M(x) because of skip connection.\u003c/li\u003e\\n\u003cli\u003eApply Layer Normalisation to X + M(x), let's call this N(x).\u003c/li\u003e\\n\u003cli\u003eN(x) is supplied to Multi-Headed Attention as Query and Encoder Output will be supplied as Key-Value..\u003c/li\u003e\\n\u003cli\u003eMulti Headed Attention does the magic and generates an output O(x).\u003c/li\u003e\\n\u003cli\u003eAdd N(x) in O(x) because of skip connection.\u003c/li\u003e\\n\u003cli\u003eApply Layer Normalisation to N(x) + O(x), let's call this P(x).\u003c/li\u003e\\n\u003cli\u003ePass P(x) to Feed Forward layer to generate an output F(x).\u003c/li\u003e\\n\u003cli\u003eAdd P(x) in F(x) because of skip connection again.\u003c/li\u003e\\n\u003cli\u003eApply Layer Normalisation to P(x) + F(x), let's call this D(x) which is the output of this decoder block.\u003c/li\u003e\\n\u003c/ol\u003e\\n\u003cp\u003eBut all that is nice but how does the decoder predict the next token? Actually, the output of the last decoder block is fed to a Linear Layer that gives an output vector of the size same as that of vocabulary. So depending on how many words the model knows that will be the size of the output vector of this Linear Layer.\u003c/p\u003e\\n\u003cp\u003eNow once you have this vector you apply softmax on this to convert and interpret the values of this vector as probabilities. The token at the index of maximum probability is the output of that \\\"time-step\\\". Take a look at the following infographic to understand it better.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638817538390/vAicWO0-Q.png\\\" alt=\\\"1.01.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eWell, that's basically how the decoder works, but we are still missing one key piece in the puzzle i.e. Masked Multi-Headed Attention. It's actually pretty simple and trust me it won't take much long to understand.\u003c/p\u003e\\n\u003ch3 id=\\\"heading-masked-multi-headed-attention\\\"\u003eMasked Multi-Headed Attention\u003c/h3\u003e\\n\u003cp\u003eIf I have to say the difference between Normal Multi-Head Attention and Masked Multi-Head Attention it'll be that in Masked Multi-Head Attention we mask the output of attention score after a point for each token by replacing it with \u003cstrong\u003e-inf\u003c/strong\u003e and when it applies softmax these -inf become 0.\u003c/p\u003e\\n\u003cp\u003eWell, that's fine but what exactly is going on? Let's understand. The reason we need such a thing in the first place is so that we can speed up training by being able to compute the probabilities of output token for each input token parallelly. The only reason we are doing this stuff is to speed up training.\u003c/p\u003e\\n\u003cp\u003eAnd just to be clear this passing all at once stuff is something we'll do only in training, during inference we'll still do it step by step by passing previous outputs to get the new ones.\u003c/p\u003e\\n\u003cp\u003eThe aim of an autoregressive model is to find the next step given the previous ones but for parallelization, we'll be passing all the stuff together at once hence we'll need to hide the output of the attention score for the future time steps of that token to properly train the model. Let's understand this step by step to get a better understanding.\u003c/p\u003e\\n\u003ch4 id=\\\"heading-step-by-step-guide-to-masked-mha\\\"\u003eStep-By-Step Guide to Masked MHA\u003c/h4\u003e\\n\u003cp\u003eSo let's start by knowing what exactly we need to find:-\u003c/p\u003e\\n\u003cp\u003e$$\\nP(x_{i+1}|[x_0, x_1, ..., x_i])\\n$$\u003c/p\u003e\\n\u003cp\u003eWhat the above equation means is that we'll need to find the \u003cstrong\u003eprobability of the next word given the previous ones in sequence\u003c/strong\u003e. Now let's consider the following sentence and let's assume it to be the expected output:-\u003c/p\u003e\\n\u003cp\u003e$$\\nThe \\\\space cake \\\\space was \\\\space sour\\n$$\u003c/p\u003e\\n\u003cp\u003eIf you pass words one by one you prevent seeing the future steps but for efficiency, you'll need to pass all the words together. When you do this you expose the decoder to future info, in order to prevent that we mask the weights of all j(\u0026gt;i) future steps, i being the current time step.\u003c/p\u003e\\n\u003cp\u003eSo for the word \u003cstrong\u003ethe\u003c/strong\u003e in the above sentence, we'll mask the weight for words \u003cstrong\u003ebest\u003c/strong\u003e and \u003cstrong\u003epets\u003c/strong\u003e, the same way for the word \u003cstrong\u003ebest\u003c/strong\u003e in the above sentence we'll mask the weight for pets. We can do this by adding a mask matrix to this:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638824585791/vkXCmdGyw.png\\\" alt=\\\"Attention Matrix.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eImplementation-wise, you apply the mask after doing the scaled matrix multiplication of the QV vector in scaled dot product attention. After this you proceed normally, the only difference is this masking step.\u003c/p\u003e\\n\u003ch1 id=\\\"heading-training-and-results\\\"\u003eTraining and Results\u003c/h1\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638824797235/s42ytjQf4.png\\\" alt=\\\"image.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eDone! We are done with the architecture. We are officially out of the deadly stuff and now it's just simple stuff. So let's wrap it up. Let's start by going through some details regarding the training:-\u003c/p\u003e\\n\u003cul\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eData:\u003c/strong\u003e The WMT English German Dataset contained 4.7 million sentence pairs, with a vocabulary of about 37k tokens. The WMT English French Dataset contained 36 million sentence pairs, which is so big that my hard drive will die before loading it. \u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eHardware:\u003c/strong\u003e Richie Rich people xD. Trained on 8 Nvidia P100 GPU something which make me go bankrupt 100 times. Trained for 100k steps for 12hrs. The bigger variants were trained for 300k steps for 3.5 days.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eOptimizer:\u003c/strong\u003e They used Adam optimizer with β1 = 0.9, β2 = 0.98 and epsilon = 10^−9. Increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. They used warmup_steps = 4000.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eRegularization: \u003c/strong\u003e Dropout of 0.1 was applied to the sum of the positional and wor embeddings. Dropout was also applied to the output of the sublayer before the residual step. Label smoothing was also applied with epsilon = 0.1. Label Smoothing is a way to prevent the model from being too confident about it's prediction.\u003c/p\u003e\\n\u003c/li\u003e\\n\u003c/ul\u003e\\n\u003cp\u003eThat's all as far as training is concerned. Now the architecture I explained had very specific parameters but that's not the only model they tried. They actually tried a bunch of them and noted the result of each in the table below:-\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638742942369/YXSCN_Hzb.png\\\" alt=\\\"image.png\\\" /\u003e\u003c/p\u003e\\n\u003cp\u003eAs you can see big model performed the best out of all but between big model and base model with a small performance drop, I'll take my chance with the base one first. \u003c/p\u003e\\n\u003cp\u003eThat's all! We have finished the paper and reached the end, take a moment and clap for yourself.\u003c/p\u003e\\n\u003cp\u003e\u003cimg src=\\\"https://cdn.hashnode.com/res/hashnode/image/upload/v1638743290287/RzCFUzOdXI.gif\\\" alt=\\\"PlayfulUnknownDuckbillcat-max-1mb.gif\\\" /\u003e\u003c/p\u003e\\n\u003ch1 id=\\\"heading-from-me-to-you\\\"\u003eFrom Me to You...\u003c/h1\u003e\\n\u003cp\u003eWow, that was a big blog and I won't lie it was tough to write. This is probably one of my longest and most experimental blog ever. I was super casual and explained everything as much as possible. At some point I wanted to just stop but every time I just thought, just a bit more. \u003c/p\u003e\\n\u003cp\u003eTransformers are probably one of those revolutionary concepts in deep learning that changed the way we approached stuff and I hope I was able to dissect the paper that started it all. I hope you had fun reading it.\u003c/p\u003e\\n\u003cp\u003eForever and Always, Herumb.\u003c/p\u003e\\n\",\"contentMarkdown\":\"\u003e Attention is Transformer’s breath,\\n\u003e\\n\u003e Multi-Head is Transformer’s release,\\n\u003e\\n\u003e RNNs thou wert and art,\\n\u003e\\n\u003e May thy model reach to greater accuracy.\\n\u003e\\n\u003e Látom.\\n\u003e\\n\u003e  \\\\- Enen No Shouboutai\\n\\nI'm getting tired of butchering anime and video game quotes so I'm thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper. \\n\\nI mean I've been seeing blogs that go by the name of **Paper Dissected** so I wanted to try writing one myself. Well this is going to be my first try in this and frankly, I have no idea how it'll be but let's give it a try, shall we?\\n\\nI guess it's safe to say that Attention Mechanism and Transformers is something that recently took over NLP. Not only did it show improvements over the SOTA models at the time but also overcome the shortcoming of the RNN models like LSTM and GRU.\\n\\n![1610374526-cheems1 (2).jpg](https://cdn.hashnode.com/res/hashnode/image/upload/v1631918244839/oF2h1CrF8.jpeg)\\n\\nSo let's go ahead and break down the sections of the paper, **Attention is all you need**. To give you a gist of what'll be there, it'll be an explanation of each section in the paper like abstract, introduction, model architecture, etc.\\n\\n# Abstract\\n\\nThis section pretty much summarizes the whole paper not in terms of working but rather in terms of what it has to offer. It starts with explaining how Seq2Seq models usually use RNN and CNN for encoder and decoder, and how the best ones connect encoder and decoder via attention mechanism.\\n\\nNow just to be clear, attention itself is not a new concept but to rely completely on attention is something that this paper introduced. It went ahead explaining the results achieved on the machine translation task which were like the following on WMT-2014 dataset:-\\n\\n| MT - Task | BLEU Score |\\n| ----------- | ----------- |\\n| **English to German** | 28.4 |\\n| **English to French** | 41.8 |\\n\\nIt also took 3.5 days and 8 GPUs to train on, which is certainly out of my budget. But the main thing that shined a bit differently than others in the abstract section was:-\\n\\n\u003e Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train.\\n\\nWe saw how results were better than traditional models but the thing that separated transformers from LSTM is parallel computation. This can feel a bit tricky to understand at the start so let's break it down. By nature, RNN models aren't parallel but rather Sequential and this is because they compute results one step at a time.\\n\\n![YjlBt.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1631732056936/0U5QBxJEa.png)\\n\\nHowever, in the Transformer model, there is no such thing as \\\"time steps\\\" but rather the inputs are passed all together and all this is made possible due to multi-headed attention and other stuff which we'll talk about later.\\n\\n# Introduction\\n\\nThis section mainly talks about the working of RNNs and it's limitations, along with the recent developments to tackle them. Now that we are at it let's understand how does Seq2Seq modeling working in the case of RNNs and to make it easier, take a look a look at the picture below:-\\n\\n![Seq2Seq Blog Img #1.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1631910532329/yoH-0RbHk.png)\\n\\nSeq2Seq Models consists of 2 models i.e. an Encoder and a Decoder. The Encoder model takes the text as input and generates a Context Vector as an output which is nothing but the last hidden state of RNN. Encoders and Decoders are nothing but RNNs and in few instances CNN 🌚. \\n\\nAnother thing you'll notice is the crosses above RNN cells except the last one that doesn't mean they aren't produced, they are but we just don't use them and all we'll be doing is to take the context vector from the last time step and feeding it to the decoder as input who'll output each word at a time giving us the resultant text. \\n\\nIn attention, however, you'll be taking the outputs of all the time step of encoder so you don't discard in that case. I mean you can also do mean pooling on all the states and pass that as context vector, so attention isn't the only way that uses all the states. But how does the decoder predict the words? Let's see, I'll try to be brief.\\n\\nDuring inference the first token is the special token signifying the start of the sentence along with context vector from the encoder, after that you pass previous output as input to the decoder along with the hidden state to generate new output, this is known as **Teacher Forcing*. \\n\\nThis way you'll use generated output as input to generate the next token, and it's cool except the output is being constructed on the context of inferred output rather than actual ones giving rise to the problem of **exposure bias**. Which basically means our model's output will be messed up if one of the outputs is bad.\\n\\nBack to the topic, the nature of RNNs is sequential which makes parallelization difficult for training, making RNN models take more time to train. However, there are few developments that tackle it:-\\n\\n\u003e  Recent work has achieved significant improvements in computational efficiency through **factorization tricks** and **conditional computation**, while also improving model performance in the case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nTo understand the factorization trick you can go ahead and read another paper, i.e. **Factorization Tricks for LSTM Networks** by Oleksii and Boris, which I recommend but I'll give you the gist of it.\\n\\n\u003e  The major part of LSTMP cell computation is in computing affine transform T because it involves multiplication with 4n × 2p matrix W.\\n\\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1631916140101/s9rWIKqZA.png)\\n\\nThe aim of the factorization trick is to approximate that W by replacing it with the product of 2 matrices W1(2p\\\\*r) and W2(r\\\\*4n), this way takes fewer parameters (2p*r + r*4n) as compared to W (2p*4n) given r \u003c p \u003c=n. Another factorization trick proposes to think parts of inputs and the hidden states as independent of each other and calculate affine transforms of these groups individually and parallelly. \\n\\nI know it's a lot to grasp so if you don't understand it all at the moment it's fine, important thing to understand is that **many attempts have been made to improve RNNs but the sequential nature isn't completely removed**. The attention model did help in capturing long-term dependencies and improve results, however, it was used along with RNNs. \\n\\nThere is another concept of conditional computation that aims at activating parts of the network to improve computational efficiency, and performance too in this case, but let's not dive much into it. But if you want to you can read this [**short book**](https://trace.tennessee.edu/cgi/viewcontent.cgi?article=5323\u0026context=utk_graddiss) on the same.\\n\\nThe transformer model on the other hand relies entirely on attention mechanism allowing it to be more parallelizable and give better results with the cost of training being 12 hours on 8 P100 GPUs, something that even Colab Pro users can't get their hands on all the time anymore 🥲.\\n\\n# Model Architecture\\n\\nFinally, we've come to the real deal. Now while I wanna try and break down the explanation sub-section wise, I know that may or may not end up leaving you kinda confused, and hence instead of explaining each sub-section of this section we'll dismantle the architecture step by step and understand how it works.\\n\\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1632343488045/17Un25edDk.png)\\n\\n## Input Embeddings\\n\\nLet's start from the beginning, what you have with you is a sentence or bunch of sentences. Now you can't exactly feed it to the model as it is so you need a way to convert it to a form more suitable for it. When we used RNNs for text classification or seq2seq we convert our text to **word embeddings**.\\n\\nThese embeddings are nothing but vector representations of words such that words with similar meanings are closer to each other. But what does closer mean? Mathematically, closer means the similarity score is high. This score can be anything but cosine similarity is popular so feel free to think that. Once we have our embeddings to use we are all set! Are we?\\n\\n![4824982 (1).jpg](https://cdn.hashnode.com/res/hashnode/image/upload/v1632351034260/Qo6JVLncJ.jpeg)\\n\\nTechnically, yes we are since these embeddings can be used with RNNs and CNNs because they are able to capture the ordering into consideration. But Transformers use only self-attention, which takes all the words at the same time hence it is not able to take the order of words into consideration. To fix this problem we introduce something called **positional encodings**.\\n\\n## Positional Encodings\\n\\nSo as mentioned before, inherently attention model is not able to take the order of words into consideration. In order to compensate for this, we use something called Positional Encodings. Don't be intimidated by the name it's actually much more simple. Let's dive a bit deeper into this.\\n\\n### Need of Positional Encodings\\n\\nWhen working with text it's really important to take words order into consideration. Why? The order of words defines the structure of the sentence and this structure helps in providing the context. \\n\\nRNNs by nature are sequential in nature by taking inputs one by one and updating hidden states accordingly, but the same is not the case with attention since it takes the words all at once. Why this is so, is something we'll understand in the next section but for now, let's say that's how it is.\\n\\nSo we need some extra sources of information in our embeddings to convey this information, something that could tell the position of a word in a sentence. To do so we can add an extra embedding or matrix to our word embeddings to provide info regarding the ordering of the word. But how? \\n\\n### Finding Positional Encodings\\n\\nWell in the most simple case you can use a one-hot encoded form of [0,1,2,3,4,5,..., pos] as positional encodings. You can train an embedding layer to find the **positional embedding** matrix, where each row has the positional embedding for word at pos index, for this and add it to input embeddings.\\n\\nIn the paper, however, the proposed approach uses a combination of sin and cos function to find the encodings. Take a look at the following picture:-\\n\\n![i (3).png](https://cdn.hashnode.com/res/hashnode/image/upload/v1632693800233/lcT9IBipy.png)\\n\\nAs you can see in the above picture the encodings are found by using a combination of sin and cos function. The function is quite self-explanatory, it says that for the embedding of a word at position **pos**:- \\n\\n* The value at index **2i** is given using the **sin** function, colored red. \\n* The value at index **2i +1** is given using the **cos** function, colored blue.\\n\\nBut why are we using this particular function? Well let's hear from the authors themselves:-\\n\\n\u003e We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions since, for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.\\n\\nWell, researchers sure love to make stuff seem more complicated, don't they? It's fine if you don't understand the above sentence, take it as a confirmation that you're still sane. \\n\\nWhat the sentence means to say is that given the positional encoding of the word at **pos** you can find the positional encoding of word at **pos + k** by applying some linear transformation on the positional encoding of **pos**. Mathematically, it can be written like this:-\\n\\n\\\\\\\\[\\nPE_{p+k} = T * PE_p\\n\\\\\\\\]\\n\\nWriting **pos** as **p** in the above equation since latex is getting messed up for some reason. The important thing to know is that the matrix T can be used to find PE at pos+k but what is this T? Can we find it? Well yes, that T is a rotational matrix that depends on offset k rather than pos. Wait what! How? \\n\\nWell, you can derive it, and here is a really good [blog by Amirhossein](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) doing the same. There is another [blog by Timo Denk](https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/) that proves the linear relationship between PE at pos and pos+k but the first one explains in a much simpler fashion, but do give both a read.\\n\\n### Improving a bit more\\n\\nWow so everything is dope, right? Fortunately or unfortunately, No. Even though everything might seem super nice with these encodings, which it is, what these really are doing is finding values for given pos and i values, these are also called **Absolute Positional Encodings**. While the altering of sin and cosine might be able to add relative nature, we might actually have something better i.e. **Relative Positional Encodings**.\\n\\nFollowing this paper, there was another paper released i.e. **Self-Attention with Relative Position Representations** by [Shaw et. al. 2019](https://arxiv.org/pdf/1803.02155.pdf). This paper introduces a modified attention mechanism which is mostly the same except this one has 2 matrices that are added to Key and Value Vector and the aim is that they'll be able to induce pairwise relationships in the inputs. In the author's own words:-\\n\\n\u003e We propose an extension to self-attention to consider the pairwise relationships between input elements. In this sense, we model the input as a labeled, directed, fully connected graph.\\n\\nThe paper is actually a pretty decent read and you should give it a try. Since this paper doesn't use these embeddings we'll not go into the detail of these. There are many papers that try to come up with better Positional Encodings and try to improve them, but what they are used for and proper intuition is something you should aim to understand.\\n\\n## Attention\\n\\nAttention is something that we've been talking about a lot in this blog, after all that's what this paper is about. But what does it actually mean? So before diving into the proposed method for attention calculation in this paper, we'll go ahead and understand what attention is and how it helps our cause.\\n\\nSo when we do Seq2Seq with RNNs we have an encoder and decoder network in which the work of the decoder is to generate a sentence given **context vector**. This context vector is nothing but the last hidden state of the encoder which is RNN based model. So what's the issue? Well, you see:-\\n\\n* The **context vector** is a vector of fixed length defined by hidden_dims but the sentences are of variable length and so what we are doing is squeezing info into a vector due to which information might be lost.\\n\\n* Another problem is that the generated sentences are based on the context of the whole sentence, however in doing so we might lose out on the local context of parts of the sentence that might've been helpful in tasks like translation.\\n\\nSo in order to improve on this, we introduce **attention** and instead of working on the global context of the sentence by taking all the hidden states in the encoder and passes it to the decoder. The decoder, in this case, doesn't produce the output directly instead it'll calculate the attention score for the states and creates the context vector for that particular time step given the attention score, the encoder hidden states, and the input vector, output of prev. step or last input token.\\n\\n![Normal Decoder.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1633217192689/yLRmt7ySf.png)\\n\\nBut how does this help? Well, what attention does is that it generates a context vector at each time step of the decoder based on attention score and this attention score helps to amplify the more important words to focus on during that time step in order to improve the quality of results. If you calculate attention among the words of the input sequence it's called **Self Attention** and when you calculate attention among the words of the input and output sequence it's called **General Attention**.\\n\\nNow based on the architecture you have 2 types of attention-based Seq2Seq models i.e. **Bahdanau Attention** and **Luong Attention**. Except the architecture and computation both are basically the same but if you wanna learn more you can go ahead and read the paper, **Effective Approaches to Attention-based Neural Machine Translation** by [Luong et. al. 2015](https://arxiv.org/abs/1508.04025).\\n\\n### Scaled Dot-Product Attention\\n\\nWe have a basic idea of how attention works in RNN now, but we don't exactly have an RNN in Transformer or a way to sequentially input data. So the proposed method for calculating attention is via something called **Scaled Dot-Product Attention**, which is basically a bunch of matrix multiplication operations. \\n\\nBut before diving in, let's talk about QKV or Query-Key-Value vectors and their dimensions. Throughout the model, the dimension of columns of the resultant matrix from each layer is set to **512** which is denoted by \\\\\\\\( d_{model} \\\\\\\\). \\n\\nSo if we have **m** rows in word embedding we'll have the resultant matrix of dimension \\\\\\\\( m * d_{model} \\\\\\\\), this column dimension is consistent for all the layers in the model including the embedding layer. \\n\\nThis is done to facilitate the residual connections, which we'll talk about later. Now that we have output dims cleared let's talk about columns of QKV vectors.\\n\\nFor now, just remember the dimension of QKV vectors being 64, the reasoning is something we'll discuss in Multiheaded attention. The dimensions of Query and Key vector are the same and are denoted by \\\\\\\\( d_k \\\\\\\\), and even though here it is same the dimensions of value vector can be different and is denoted by \\\\\\\\( d_v \\\\\\\\). So to sum it all up we'll have the following notation for dimensions:-\\n\\n$$\\nd_{model} = 512\\n$$\\n\\n$$\\nd_q = d_k = 64\\n$$\\n\\n$$\\nd_v = 64\\n$$\\n\\nNow that we have the important stuff cleared, let's take a look at the following image that sums up the process of calculating attention:-\\n\\n![S C A L E.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1632788371388/1NFBKy8H8D.png)\\n\\nIt might seem confusing so let's go in step by step on how attention is being calculated and how its equation is constructed:-\\n\\n* You start by finding *Query*, *Key*, and *Value* vector by multiplying the input matrix with their corresponding weight vector. The dims of these vectors are something we'll discuss in Multi Headed Attention Section. For now, let's just focus on understanding the equation.\\n$$\\nAttention(Q, K, V)\\n$$\\n\\n* Apply Matrix Multiplication on *Query* and *Key* Vector.\\n$$\\nAttention(Q, K, V) = QK^T\\n$$\\n\\n* Scale the result of above with \\\\\\\\( \\\\sqrt{d_k} \\\\\\\\). It was observed that for small values of \\\\\\\\( d_k \\\\\\\\) additive attention and dot product attention perform similarly. But for larger values of \\\\\\\\( d_k \\\\\\\\) additive attention outperforms dot product attention without scaling. As mentioned in paper, that for large value the result may explode and may push softmax to smaller gradients. To counteract this \\\\\\\\( \\\\frac{1}{\\\\sqrt{d_k}} \\\\\\\\) was used for scaling. As to why this occurs, paper mentions:-\\n\\n\u003e To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product has mean 0 and variance d_k.\\n\\n$$\\nAttention(Q, K, V) = \\\\frac{QK^T}{\\\\sqrt{d_k}}\\n$$\\n\\n* Applying softmax on the result of the above.\\n$$\\nAttention(Q, K, V) = softmax(\\\\frac{QK^T}{\\\\sqrt{d_k}})\\n$$\\n\\n* Apply Matrix Multiplication on result and *Value* Vector and get the final result.\\n$$\\nAttention(Q, K, V) = softmax(\\\\frac{QK^T}{\\\\sqrt{d_k}})V\\n$$\\n\\nThere you go that's how you find attention using Scaled Dot Product Attention. Just for the info, this is not the absolute way to find attention, there are other ways to calculate attention like additive attention, dot-product(unscaled) attention. While the two are similar in complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nNow let's take a look at the newly proposed approach where instead of calculating attention one time we calculate it multiple times and then concatenate the results. Yep, it is indeed **Multi-Headed Attention**.\\n\\n### Multi-Headed Attention\\n\\nLet's talk about the beast that the paper proposed and when I say \\\"beast\\\" I mean it. The paper suggested that instead of calculating attention one time we can calculate it  **h** times with d_q, d_k, d_v dimensioned QKV vectors found after passing each of them from a Linear Layer.\\n\\nWe then use them to calculate attention values from each attention head depicted by head_i and concatenate these heads forming \\\\\\\\( d_{model} \\\\\\\\) dimensional resultant vector. After that, we'll pass them through another linear layer to get our Multi-Headed Attention output. Architecture wise it looks like this:-\\n\\n![Linear.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638644522204/RBUNc_MHH.png)\\n\\nOne thing to note is that the value of d_q, d_v, and d_k will be **d_model / h**, that's why after you concatenate, the attention head's output it becomes d_model sized vector. In this paper, the value of **h** for the optimal model was 8. So d_k, d_v, and d_q become 512/8 = 64.\\n\\n#### Why does it Work?\\n\\nBut after hearing all this it's only sensible to ask. Why does this even work? To answer this query, we should go and take a look at the paper **\\\"Analyzing Multi-Head Self-Attention\\\"** by [Voita et. al. 2019](https://arxiv.org/pdf/1905.09418.pdf). Where they basically try to find the roles that different attention heads played along with other stuff which we'll talk about later i.e. Attention Head Pruning.\\n\\nTo explain the gist of it the conclusion was, that based on roles attention heads can be divided into 3 parts:-\\n\\n* **Positional Heads:** Heads where the maximum attention weights was assigned to a specific relative position. This was usually +/- 1 in practice signifying attention to adjacent positions\\n\\n* **Syntactical Heads: ** Head attends to tokens corresponding to any of the major syntactic relations in a sentence. In the paper, they analyze direct relations like nominal subject, direct object, adjectival modifier, and adverbial modifier.\\n\\n* **Rare Word Heads**: Heads attend to the least frequent tokens in a sentence. \\n\\nSo different heads try to perform different task cool, but do we need all of them? We have only 3 types right? Does that mean other heads are useless? Yes and No. Let's understand the **Yes** part first. \\n\\nYes, attention heads are important and in fact, that's exactly what the blog **\\\"Are Sixteen Heads Really Better than One?\\\"** by [Paul Michael](https://blog.ml.cmu.edu/2020/03/20/are-sixteen-heads-really-better-than-one/) tries to find. In the blog, he tried to explain how the performance was affected when attention heads were pruned. After the experiment they found the following result:-\\n\\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638650058322/s9kG6ry4m.png)\\n\\nAs you can see the BLEU Score is really affected after pruning the heads however like other metrics the dip is observed after more than 60% heads are pruned. This leads us to the **No** part. I mean kinda No.\\n\\nEven though we just saw that heads are important we can also prune one or a few of them without seeing a major dip in the performance, something that both the mentioned sources, that I highly recommend you to go through, suggested. Phew that was a lot, now that we have the initial stuff cleared up, let's get back to the architecture. I promise this point forward is a smooth sailing simple life, probably xD. \\n\\n## Encoder\\n\\nIf we try to understand the architecture of Encoder it can be broken into 2 parts:-\\n\\n* Attention Layer\\n\\n* Feed Forward Layer\\n\\nYes that's it, you are already familiar with Attention Layer which is nothing but Multi Headed Attention with 8 heads and the Feed Forward Layer is a basic Neural Network with a single hidden layer with 2048 nodes and 512 nodes in input and output layer. Visually it'll look something like this:-\\n\\n![Multi-Headed Attention.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638657930173/RL6ONG63c.png)\\n\\nIts flow of Infomation is really simple, or not depending on your knowledge of skip connections. If I have to explain in one line skip connection is basically **adding input to the output**. \\n\\nI know it sounds weird, but trust me it makes sense. ResNet model was the one that popularised these skip connections which provides an alternate path for gradients to flow. They helped train deeper models without running into vanishing gradient problem. So if x is the input to a block and f(x) is the output of the block then skip connections basically add them both making **x + f(x)** the new output.\\n\\n$$\\nf'(x) = x + f(x)\\n$$\\n\\nNow that we are all on the same page, let's step by step go through the flow of info in an encoder block:-\\n\\n1. Encoder block receives an input **X**.\\n2. X is supplied to Multi-Headed Attention Layer. QKV all will be the same i.e. **X**.\\n3. Multi Headed Attention does the magic and generates an output M(x).\\n4. Add X to M(x) because of skip connection.\\n5. Apply Layer Normalisation to X + M(x), let's call this N(x).\\n6. Pass N(x) to Feed Forward layer to generate an output F(x).\\n7. Add N(x) in F(x) because of skip connection again.\\n8. Apply Layer Normalisation to F(x) + N(x), let's call this E(x) which is the output of this encoder block.\\n\\nOne last thing, the above is flow of info for one encoder block. This encoder block's output will be passed on to the next encoder block and the architecture of all encoder blocks is the same. The paper proposed the architecture with **6 encoder blocks**. I swear I need to learn animation one of these days, would make stuff much much easier.\\n\\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638659972564/YTnCToXMN.png)\\n\\nYea I know too much info xD, but hey that's all that you need to know about Encoders. The output of the last encoder block is passed to the decoder as KV vector values, which is our next topic of discussion. We are almost near the end, so hang in there 😬.\\n\\n## Decoder\\n\\nFinally, we are at the last step of the architecture, to be honest decoder is not really that different from than encoder except the fact that here were have an extra layer for masked multi-headed attention. We'll get into the details of that in a bit, but let's get an overview of how a decoder block looks:-\\n\\n![Decoder (1).png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638738474914/aVah0DKWI.png)\\n\\nBefore going into the flow of information let's understand exactly what is the input and output of the decoder block. As far as input is concerned the decoder takes 2 inputs:-\\n\\n1. **Encoder Output:** This is will be fed as KV Vectors for multi-headed attention in the 2nd layer, this is usually referred to as Encoder-Decoder Attention. All **6 Decoder Blocks** will take the encoder output from the last block as input for encoder-decoder attention.\\n\\n2. **Output Embedding:** These are the embeddings of the target sentence. But wait!? We don't have a target in testing! So what are we inputting here then? Let's see.\\n\\nWell usually the first input to the decoder is a special token signifying the start of the output sentence and the output is the token that comes after this. This output is the output of time step 1. \\n\\nAfter that, the input to decoder becomes the tokens until a certain timestep i.e. for the 5th timestep input will be all generated tokens till the 4th timestep. After that, it'll keep on giving an output until it throws a special token signifying the end of the output sentence.\\n\\nLet's understand the flow of information in decoder block to get a better understanding:-\\n\\n1. Decoder block receives an input **X**.\\n2. X is supplied to Masked Multi-Headed Attention as QKV vector, to get an output M(x).\\n3. Add X in M(x) because of skip connection.\\n4. Apply Layer Normalisation to X + M(x), let's call this N(x).\\n5. N(x) is supplied to Multi-Headed Attention as Query and Encoder Output will be supplied as Key-Value..\\n6. Multi Headed Attention does the magic and generates an output O(x).\\n7. Add N(x) in O(x) because of skip connection.\\n8. Apply Layer Normalisation to N(x) + O(x), let's call this P(x).\\n9. Pass P(x) to Feed Forward layer to generate an output F(x).\\n10. Add P(x) in F(x) because of skip connection again.\\n11. Apply Layer Normalisation to P(x) + F(x), let's call this D(x) which is the output of this decoder block.\\n\\nBut all that is nice but how does the decoder predict the next token? Actually, the output of the last decoder block is fed to a Linear Layer that gives an output vector of the size same as that of vocabulary. So depending on how many words the model knows that will be the size of the output vector of this Linear Layer.\\n\\nNow once you have this vector you apply softmax on this to convert and interpret the values of this vector as probabilities. The token at the index of maximum probability is the output of that \\\"time-step\\\". Take a look at the following infographic to understand it better.\\n\\n![1.01.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638817538390/vAicWO0-Q.png)\\n\\nWell, that's basically how the decoder works, but we are still missing one key piece in the puzzle i.e. Masked Multi-Headed Attention. It's actually pretty simple and trust me it won't take much long to understand.\\n\\n### Masked Multi-Headed Attention\\n\\nIf I have to say the difference between Normal Multi-Head Attention and Masked Multi-Head Attention it'll be that in Masked Multi-Head Attention we mask the output of attention score after a point for each token by replacing it with **-inf** and when it applies softmax these -inf become 0.\\n\\nWell, that's fine but what exactly is going on? Let's understand. The reason we need such a thing in the first place is so that we can speed up training by being able to compute the probabilities of output token for each input token parallelly. The only reason we are doing this stuff is to speed up training.\\n\\nAnd just to be clear this passing all at once stuff is something we'll do only in training, during inference we'll still do it step by step by passing previous outputs to get the new ones.\\n\\nThe aim of an autoregressive model is to find the next step given the previous ones but for parallelization, we'll be passing all the stuff together at once hence we'll need to hide the output of the attention score for the future time steps of that token to properly train the model. Let's understand this step by step to get a better understanding.\\n\\n#### Step-By-Step Guide to Masked MHA\\n\\nSo let's start by knowing what exactly we need to find:-\\n\\n$$\\nP(x_{i+1}|[x_0, x_1, ..., x_i])\\n$$\\n\\nWhat the above equation means is that we'll need to find the **probability of the next word given the previous ones in sequence**. Now let's consider the following sentence and let's assume it to be the expected output:-\\n\\n$$\\nThe \\\\space cake \\\\space was \\\\space sour\\n$$\\n\\nIf you pass words one by one you prevent seeing the future steps but for efficiency, you'll need to pass all the words together. When you do this you expose the decoder to future info, in order to prevent that we mask the weights of all j(\u003ei) future steps, i being the current time step.\\n\\nSo for the word **the** in the above sentence, we'll mask the weight for words **best** and **pets**, the same way for the word **best** in the above sentence we'll mask the weight for pets. We can do this by adding a mask matrix to this:-\\n\\n![Attention Matrix.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638824585791/vkXCmdGyw.png)\\n\\nImplementation-wise, you apply the mask after doing the scaled matrix multiplication of the QV vector in scaled dot product attention. After this you proceed normally, the only difference is this masking step.\\n\\n# Training and Results\\n\\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638824797235/s42ytjQf4.png)\\n\\nDone! We are done with the architecture. We are officially out of the deadly stuff and now it's just simple stuff. So let's wrap it up. Let's start by going through some details regarding the training:-\\n\\n* **Data:** The WMT English German Dataset contained 4.7 million sentence pairs, with a vocabulary of about 37k tokens. The WMT English French Dataset contained 36 million sentence pairs, which is so big that my hard drive will die before loading it. \\n\\n* **Hardware:** Richie Rich people xD. Trained on 8 Nvidia P100 GPU something which make me go bankrupt 100 times. Trained for 100k steps for 12hrs. The bigger variants were trained for 300k steps for 3.5 days.\\n\\n* **Optimizer:** They used Adam optimizer with β1 = 0.9, β2 = 0.98 and epsilon = 10^−9. Increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. They used warmup_steps = 4000.\\n\\n* **Regularization: ** Dropout of 0.1 was applied to the sum of the positional and wor embeddings. Dropout was also applied to the output of the sublayer before the residual step. Label smoothing was also applied with epsilon = 0.1. Label Smoothing is a way to prevent the model from being too confident about it's prediction.\\n\\nThat's all as far as training is concerned. Now the architecture I explained had very specific parameters but that's not the only model they tried. They actually tried a bunch of them and noted the result of each in the table below:-\\n\\n![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1638742942369/YXSCN_Hzb.png)\\n\\nAs you can see big model performed the best out of all but between big model and base model with a small performance drop, I'll take my chance with the base one first. \\n\\nThat's all! We have finished the paper and reached the end, take a moment and clap for yourself.\\n\\n![PlayfulUnknownDuckbillcat-max-1mb.gif](https://cdn.hashnode.com/res/hashnode/image/upload/v1638743290287/RzCFUzOdXI.gif)\\n\\n# From Me to You...\\n\\nWow, that was a big blog and I won't lie it was tough to write. This is probably one of my longest and most experimental blog ever. I was super casual and explained everything as much as possible. At some point I wanted to just stop but every time I just thought, just a bit more. \\n\\nTransformers are probably one of those revolutionary concepts in deep learning that changed the way we approached stuff and I hope I was able to dissect the paper that started it all. I hope you had fun reading it.\\n\\nForever and Always, Herumb.\",\"hasPolls\":false,\"totalPollVotes\":0,\"upvotes\":0,\"downvotes\":0,\"tags\":[],\"untaggedFrom\":[],\"popularity\":5112.8714,\"responseCount\":0,\"replyCount\":0,\"followers\":[],\"isActive\":true,\"isFeatured\":false,\"isEngaging\":false,\"answeredByTarget\":false,\"isDelisted\":false,\"isNotified\":false,\"duplicatePosts\":[],\"numCollapsed\":0,\"hasReward\":false,\"bookmarkedIn\":[],\"reactions\":[\"5c090d96c2a9c2a674d35484\",\"5c090d96c2a9c2a674d35485\",\"5c090d96c2a9c2a674d35486\",\"567453d0b73d6a82ac8c5abd\",\"5c090d96c2a9c2a674d35488\",\"567453d0b73d6a82ac8c5abc\",\"5c090d96c2a9c2a674d3548a\",\"567453d0b73d6a82ac8c5ab9\",\"5c090d96c2a9c2a674d3548b\",\"5d9dd859f31791c942dc5b52\"],\"totalReactions\":10,\"reactionsByCurrentUser\":[],\"isPinnedToBlog\":false,\"disableComments\":false,\"commentsPaused\":false,\"hasLatex\":true,\"syncAlgolia\":true,\"enableToc\":false,\"toc\":[],\"numUniqueUsersWhoReacted\":0,\"title\":\"Transformers: Attention is all you need\",\"subtitle\":\"Getting back to where it started\",\"type\":\"story\",\"coverImageAttribution\":\"\",\"coverImagePhotographer\":\"\",\"isCoverAttributionHidden\":false,\"ogImage\":\"\",\"metaTitle\":\"Transformers: Attention is all you need\",\"metaDescription\":\"Let's go ahead and break down the sections of the paper, **Attention is all you need**. To give you a gist of what'll be there, it'll be an explanation of e\",\"isRepublished\":false,\"originalArticleURL\":\"\",\"partOfPublication\":true,\"publication\":{\"_id\":\"60772aad4155da32fc83af3a\",\"betaFeatures\":{\"newsletter\":false,\"embeds\":false,\"csvImport\":false,\"customCSS\":false,\"githubAsSource\":false},\"views\":{\"today\":9,\"thisWeek\":24,\"lastWeek\":13,\"allTime\":668,\"thisMonth\":116},\"numFollowers\":0,\"badgePageEnabled\":true,\"newsletterEnabled\":true,\"audioBlogEnabled\":false,\"textSelectionSharerEnabled\":true,\"newsletterFrequency\":\"asap\",\"staticPages\":[],\"isFeatured\":false,\"dailyViewsForCurrentMonth\":{\"1636502400\":1,\"1636588800\":5,\"1636675200\":3,\"1636761600\":14,\"1636848000\":3,\"1636934400\":20,\"1637020800\":3,\"1637107200\":5,\"1637193600\":5,\"1637280000\":3,\"1637452800\":2,\"1637539200\":2,\"1637625600\":8,\"1637712000\":2,\"1637798400\":3,\"1637884800\":2,\"1638057600\":4,\"1638144000\":1,\"1638230400\":4,\"1638403200\":2,\"1638662400\":9,\"1638748800\":2,\"1638835200\":1,\"1638921600\":3},\"viewCountVisible\":true,\"readTimeHidden\":false,\"customCSSEnabled\":false,\"domainChecklistSkipped\":true,\"author\":{\"_id\":\"60772a7e4155da32fc83af31\",\"socialMedia\":{\"website\":\"\",\"github\":\"\",\"twitter\":\"\",\"facebook\":\"\",\"stackoverflow\":\"\",\"linkedin\":\"\"},\"location\":\"\",\"isDeactivated\":false,\"numFollowers\":2,\"numFollowing\":0,\"storiesCreated\":[],\"totalUpvotesReceived\":0,\"tagManagerOf\":[],\"isEvangelist\":false,\"isAmbassador\":false,\"hasGoldRing\":false,\"name\":\"Herumb Shandilya\",\"username\":\"krypticmouse\",\"tagline\":\"\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1618422399738/yUO-x7u_t.png\",\"dateJoined\":\"2021-04-14T17:46:38.447Z\",\"publicationDomain\":\"\",\"numPosts\":4,\"numReactions\":48},\"isActive\":true,\"username\":\"krypticmouse\",\"domain\":\"\",\"urlPattern\":\"simple\",\"title\":\"krypticmouse\",\"links\":{\"twitter\":\"\",\"github\":\"\",\"website\":\"\",\"hashnode\":\"https://hashnode.com/@krypticmouse\",\"youtube\":\"\",\"dailydev\":\"\",\"linkedin\":\"\"},\"customRules\":[],\"menu\":[{\"priority\":3,\"_id\":\"60d3bcc399fab47c98b6323b\",\"type\":\"link\",\"label\":\"Github\",\"url\":\"https://github.com/krypticmouse\"},{\"priority\":2,\"_id\":\"60d3bcbd99fab47c98b63238\",\"type\":\"link\",\"label\":\"LinkedIn\",\"url\":\"https://www.linkedin.com/in/herumb-s-740163131/\"},{\"priority\":1,\"_id\":\"60d3bcb0f16d577cbe60ce59\",\"type\":\"link\",\"label\":\"Twitter\",\"url\":\"https://twitter.com/krypticmouse\"},{\"priority\":0,\"_id\":\"60d3bc8c99fab47c98b63231\",\"type\":\"page\",\"label\":\"About Me\",\"page\":{\"_id\":\"60d3bc27f16d577cbe60ce52\",\"isHidden\":false,\"priority\":0,\"title\":\"About Me\",\"endpoint\":\"about-me\"}}],\"__v\":0,\"darkModeLogo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1619641455882/WbwWIRzln.png\",\"favicon\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1619641126923/FnNt45KTC.png\",\"headerColor\":\"\",\"layout\":\"grid\",\"logo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1619641458812/RpN_hzmke.png\",\"imprint\":\"\",\"imprintMarkdown\":\"\",\"meta\":\"\",\"metaHTML\":\"\",\"hasBadges\":true},\"slug\":\"attention-is-all-you-need\",\"importedFromMedium\":false,\"dateAdded\":\"2021-12-09T22:53:33.788Z\",\"hasCustomDate\":false,\"stickCoverToBottom\":false,\"pollOptions\":[],\"badges\":[],\"questionReplies\":[],\"contributors\":[],\"cuid\":\"ckwzk41mq0d8hb6s1f64ed2t4\",\"author\":{\"_id\":\"60772a7e4155da32fc83af31\",\"socialMedia\":{\"website\":\"\",\"github\":\"\",\"twitter\":\"\",\"facebook\":\"\",\"stackoverflow\":\"\",\"linkedin\":\"\"},\"location\":\"\",\"isDeactivated\":false,\"numFollowers\":2,\"numFollowing\":0,\"storiesCreated\":[],\"totalUpvotesReceived\":0,\"tagManagerOf\":[],\"isEvangelist\":false,\"isAmbassador\":false,\"hasGoldRing\":false,\"name\":\"Herumb Shandilya\",\"username\":\"krypticmouse\",\"tagline\":\"\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1618422399738/yUO-x7u_t.png\",\"dateJoined\":\"2021-04-14T17:46:38.447Z\",\"publicationDomain\":\"\",\"numPosts\":4,\"numReactions\":48},\"followersCount\":1,\"hasPinnedTag\":false,\"readTime\":26,\"__v\":0,\"sB\":true,\"reactionToCountMap\":{\"reaction_5c090d96c2a9c2a674d35484\":1,\"reaction_5c090d96c2a9c2a674d35485\":1,\"reaction_5c090d96c2a9c2a674d35486\":1,\"reaction_567453d0b73d6a82ac8c5abd\":1,\"reaction_5c090d96c2a9c2a674d35488\":1,\"reaction_567453d0b73d6a82ac8c5abc\":1,\"reaction_5c090d96c2a9c2a674d3548a\":1,\"reaction_567453d0b73d6a82ac8c5ab9\":1,\"reaction_5c090d96c2a9c2a674d3548b\":1,\"reaction_5d9dd859f31791c942dc5b52\":1},\"series\":null,\"dateUpdated\":\"2021-12-10T04:49:08.188Z\",\"pendingPublicationApproval\":false,\"viewsUpdatedOn\":1639112405204,\"hasTags\":true,\"isPartOfSeries\":false,\"responses\":[]}","publication":"{\"_id\":\"60772aad4155da32fc83af3a\",\"betaFeatures\":{\"newsletter\":false,\"embeds\":false,\"csvImport\":false,\"customCSS\":false,\"githubAsSource\":false},\"views\":{\"today\":9,\"thisWeek\":24,\"lastWeek\":13,\"allTime\":668,\"thisMonth\":116},\"numFollowers\":0,\"badgePageEnabled\":true,\"newsletterEnabled\":true,\"audioBlogEnabled\":false,\"textSelectionSharerEnabled\":true,\"newsletterFrequency\":\"asap\",\"staticPages\":[],\"isFeatured\":false,\"dailyViewsForCurrentMonth\":{\"1636502400\":1,\"1636588800\":5,\"1636675200\":3,\"1636761600\":14,\"1636848000\":3,\"1636934400\":20,\"1637020800\":3,\"1637107200\":5,\"1637193600\":5,\"1637280000\":3,\"1637452800\":2,\"1637539200\":2,\"1637625600\":8,\"1637712000\":2,\"1637798400\":3,\"1637884800\":2,\"1638057600\":4,\"1638144000\":1,\"1638230400\":4,\"1638403200\":2,\"1638662400\":9,\"1638748800\":2,\"1638835200\":1,\"1638921600\":3},\"viewCountVisible\":true,\"readTimeHidden\":false,\"customCSSEnabled\":false,\"domainChecklistSkipped\":true,\"author\":{\"_id\":\"60772a7e4155da32fc83af31\",\"socialMedia\":{\"website\":\"\",\"github\":\"\",\"twitter\":\"\",\"facebook\":\"\",\"stackoverflow\":\"\",\"linkedin\":\"\"},\"location\":\"\",\"isDeactivated\":false,\"numFollowers\":2,\"numFollowing\":0,\"storiesCreated\":[],\"totalUpvotesReceived\":0,\"tagManagerOf\":[],\"isEvangelist\":false,\"isAmbassador\":false,\"hasGoldRing\":false,\"name\":\"Herumb Shandilya\",\"username\":\"krypticmouse\",\"tagline\":\"\",\"photo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1618422399738/yUO-x7u_t.png\",\"dateJoined\":\"2021-04-14T17:46:38.447Z\",\"publicationDomain\":\"\",\"numPosts\":4,\"numReactions\":48},\"isActive\":true,\"username\":\"krypticmouse\",\"domain\":\"\",\"urlPattern\":\"simple\",\"title\":\"krypticmouse\",\"links\":{\"twitter\":\"\",\"github\":\"\",\"website\":\"\",\"hashnode\":\"https://hashnode.com/@krypticmouse\",\"youtube\":\"\",\"dailydev\":\"\",\"linkedin\":\"\"},\"customRules\":[],\"menu\":[{\"priority\":3,\"_id\":\"60d3bcc399fab47c98b6323b\",\"type\":\"link\",\"label\":\"Github\",\"url\":\"https://github.com/krypticmouse\"},{\"priority\":2,\"_id\":\"60d3bcbd99fab47c98b63238\",\"type\":\"link\",\"label\":\"LinkedIn\",\"url\":\"https://www.linkedin.com/in/herumb-s-740163131/\"},{\"priority\":1,\"_id\":\"60d3bcb0f16d577cbe60ce59\",\"type\":\"link\",\"label\":\"Twitter\",\"url\":\"https://twitter.com/krypticmouse\"},{\"priority\":0,\"_id\":\"60d3bc8c99fab47c98b63231\",\"type\":\"page\",\"label\":\"About Me\",\"page\":{\"_id\":\"60d3bc27f16d577cbe60ce52\",\"isHidden\":false,\"priority\":0,\"title\":\"About Me\",\"endpoint\":\"about-me\"}}],\"__v\":0,\"darkModeLogo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1619641455882/WbwWIRzln.png\",\"favicon\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1619641126923/FnNt45KTC.png\",\"headerColor\":\"\",\"layout\":\"grid\",\"logo\":\"https://cdn.hashnode.com/res/hashnode/image/upload/v1619641458812/RpN_hzmke.png\",\"imprint\":\"\",\"imprintMarkdown\":\"\",\"meta\":\"\",\"metaHTML\":\"\",\"hasBadges\":true}","isSingleAnswerView":false,"responsesPage":1,"responseStamp":null,"preview":false,"integrations":{"fbPixelID":null,"fathomSiteID":null,"fathomCustomDomainEnabled":null,"fathomCustomDomain":null,"hotjarSiteID":null,"matomoSiteID":null,"matomoURL":null,"plausibleAnalyticsEnabled":null,"domainURL":"krypticmouse.hashnode.dev"},"customHeadItems":{"customFavicon":"https://cdn.hashnode.com/res/hashnode/image/upload/v1619641126923/FnNt45KTC.png?auto=compress,format\u0026format=webp\u0026fm=png","customTheme":null,"customMeta":null},"hnmcMode":false,"defaultThemeDark":false},"__N_SSP":true},"page":"/[...slug]","query":{"slug":["attention-is-all-you-need"]},"buildId":"R2ha-XQcsti25FY5tKHTD","isFallback":false,"dynamicIds":[757],"gssp":true,"scriptLoader":[]}</script><div id="hn-modal"></div><div id="hn-toast"></div><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;"></p></next-route-announcer></body></html>
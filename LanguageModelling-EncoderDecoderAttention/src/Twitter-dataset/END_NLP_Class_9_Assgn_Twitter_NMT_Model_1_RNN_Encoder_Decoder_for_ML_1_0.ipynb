{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END_NLP_Class_9_Assgn_Twitter_NMT_Model_1_RNN_Encoder_Decoder_for_ML_1_0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtxmjHvKjFeo",
        "outputId": "70621407-201a-46c9-9fac-eea10df26ece"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jan  2 05:52:18 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "groncFRQpQEU",
        "outputId": "a63ff037-65b1-49b6-de03-d7d61ae0343d"
      },
      "source": [
        "%%bash\r\n",
        "python -m spacy download en\r\n",
        "# python -m spacy download de"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eWm1sfe2eCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58d2ba1-d316-4b10-e0c8-ac94c8222e44"
      },
      "source": [
        "# download from kaggle\r\n",
        "!pip uninstall -y kaggle\r\n",
        "!pip install --upgrade pip\r\n",
        "!pip install kaggle==1.5.6\r\n",
        "!kaggle -v\r\n",
        "import os"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: kaggle 1.5.6\n",
            "Uninstalling kaggle-1.5.6:\n",
            "  Successfully uninstalled kaggle-1.5.6\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (20.3.3)\n",
            "Collecting kaggle==1.5.6\n",
            "  Using cached kaggle-1.5.6-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2020.12.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.10)\n",
            "Installing collected packages: kaggle\n",
            "Successfully installed kaggle-1.5.6\n",
            "Kaggle API 1.5.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5knmnQOpZ9L"
      },
      "source": [
        "## Data Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMw7SsO2pZcj"
      },
      "source": [
        "# enter your Kaggle credentionals here\r\n",
        "os.environ['KAGGLE_USERNAME']=\"anyltcamitk\"\r\n",
        "os.environ['KAGGLE_KEY']=\"fc7e68b47141c6fe0abcbd70b0a256e8\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrHbQGXZpdgb",
        "outputId": "6c6bb638-79ec-4de4-ac0f-22bf3f080f3d"
      },
      "source": [
        "# !kaggle datasets download twcs\r\n",
        "!kaggle datasets download thoughtvector/customer-support-on-twitter\r\n",
        "!unzip -q -o customer-support-on-twitter.zip -d customer-support-on-twitter\r\n",
        "TWEET_FILE_FOLDER = \"/content/customer-support-on-twitter/twcs/twcs.csv\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading customer-support-on-twitter.zip to /content\n",
            " 95% 160M/169M [00:01<00:00, 97.9MB/s]\n",
            "100% 169M/169M [00:01<00:00, 108MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx6tV9Sh3p7K"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KUPatdbFRbl"
      },
      "source": [
        "import pandas as pd\n",
        "import torchtext\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "from torch import nn, optim"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "cnwI4Qz92wxN",
        "outputId": "ddb106f8-4fe3-4f24-d4d2-3ff15c75ae5a"
      },
      "source": [
        "## Lets understand the data a bit first\n",
        "df = pd.read_csv(TWEET_FILE_FOLDER)\n",
        "df.head()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>author_id</th>\n",
              "      <th>inbound</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>response_tweet_id</th>\n",
              "      <th>in_response_to_tweet_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
              "      <td>@115712 I understand. I would like to assist y...</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
              "      <td>@sprintcare and how do you propose we do that</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
              "      <td>@sprintcare I have sent several private messag...</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>sprintcare</td>\n",
              "      <td>False</td>\n",
              "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
              "      <td>@115712 Please send us a Private Message so th...</td>\n",
              "      <td>3</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>115712</td>\n",
              "      <td>True</td>\n",
              "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
              "      <td>@sprintcare I did.</td>\n",
              "      <td>4</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tweet_id   author_id  ...  response_tweet_id in_response_to_tweet_id\n",
              "0         1  sprintcare  ...                  2                     3.0\n",
              "1         2      115712  ...                NaN                     1.0\n",
              "2         3      115712  ...                  1                     4.0\n",
              "3         4  sprintcare  ...                  3                     5.0\n",
              "4         5      115712  ...                  4                     6.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "1ZR3vuU83pTC",
        "outputId": "4a61fa67-8c33-4927-cfad-368459a634b0"
      },
      "source": [
        "# Get tweets that are first inbound tweets to companies\n",
        "first_inbound = df[pd.isnull(df.in_response_to_tweet_id) & df.inbound]\n",
        "\n",
        "# Get replies for first inbound tweets\n",
        "inbound_and_replies = pd.merge(first_inbound, df, left_on='tweet_id', right_on='in_response_to_tweet_id')\n",
        "\n",
        "# Remove replies that aren't from companies\n",
        "inbound_and_replies = inbound_and_replies[inbound_and_replies.inbound_y ^ True]\n",
        "\n",
        "# Replace carriage returns\n",
        "inbound_and_replies = inbound_and_replies.replace('\\r', '', regex=True)\n",
        "\n",
        "# Extract inbound tweets and responses\n",
        "tweets = inbound_and_replies[['text_x', 'text_y']]\n",
        "tweets.columns = ['tweet', 'response']\n",
        "tweets.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@sprintcare is the worst customer service</td>\n",
              "      <td>@115712 Can you please send us a private messa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@sprintcare is the worst customer service</td>\n",
              "      <td>@115712 I would love the chance to review the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@sprintcare is the worst customer service</td>\n",
              "      <td>@115712 Hello! We never like our customers to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@115714 y’all lie about your “great” connectio...</td>\n",
              "      <td>@115713 H there! We'd definitely like to work ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@115714 whenever I contact customer support, t...</td>\n",
              "      <td>@115715 Please send me a private message so th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet                                           response\n",
              "0          @sprintcare is the worst customer service  @115712 Can you please send us a private messa...\n",
              "1          @sprintcare is the worst customer service  @115712 I would love the chance to review the ...\n",
              "2          @sprintcare is the worst customer service  @115712 Hello! We never like our customers to ...\n",
              "3  @115714 y’all lie about your “great” connectio...  @115713 H there! We'd definitely like to work ...\n",
              "4  @115714 whenever I contact customer support, t...  @115715 Please send me a private message so th..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY--A8I3JDOn",
        "outputId": "58752a6c-4a92-455f-fa74-1efa1005fb30"
      },
      "source": [
        "print(tweets.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(794299, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Bv_InoH7fS"
      },
      "source": [
        "tweets.head(100_000).to_csv('tweets.tsv', index=False, sep='\\t')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd66cHvABpb-"
      },
      "source": [
        "# Use same field for both columns since they have a shared vocabulary\n",
        "TWEET = torchtext.data.Field(\n",
        "    tokenize='spacy',\n",
        "    lower=True,\n",
        "    init_token='<sos>', \n",
        "    eos_token='<eos>'\n",
        ")\n",
        "\n",
        "fields = [('tweet', TWEET), ('response', TWEET)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJzjcXEkFOyV",
        "outputId": "36872b14-379c-405a-c581-e0ea52ad8ee3"
      },
      "source": [
        "# Create dataset\n",
        "start = time.time()\n",
        "dataset = torchtext.data.TabularDataset(\n",
        "    path='tweets.tsv',\n",
        "    format='TSV',\n",
        "    fields=fields,\n",
        "    skip_header=True\n",
        ")\n",
        "end = time.time()\n",
        "print(f'Duration: {end - start}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Duration: 35.86033844947815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKV9qi_UKZPN",
        "outputId": "27cf71ec-37f0-4a60-a448-595039af2f67"
      },
      "source": [
        "# Train/val split\n",
        "(train, valid) = dataset.split(split_ratio=[0.85, 0.15])\n",
        "print(len(train), len(valid))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85000 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REXlZ7OsKmxd",
        "outputId": "14e19a41-95c8-4905-e906-20226e0bbb2a"
      },
      "source": [
        "MAX_VOCAB_SIZE = 10_000\n",
        "\n",
        "TWEET.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
        "\n",
        "print(f'Size of vocab: {len(TWEET.vocab)}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocab: 10004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhcOZigaDTZ",
        "outputId": "41869968-3c26-432f-ead5-e1f22e299605"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-eVUYh6a8mA"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator = torchtext.data.BucketIterator.splits(\n",
        "    (train, valid),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.tweet),\n",
        "    device = device\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUqqAeIjbJlL"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPAVqUNMbQ7z"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y716L56bbasV"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24Jvg0XkbceL"
      },
      "source": [
        "INPUT_DIM = len(TWEET.vocab)\n",
        "OUTPUT_DIM = len(TWEET.vocab)\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvW61Ob6bedG",
        "outputId": "3383fab4-3478-4fb3-fc1c-9035a3cac165"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(10004, 128)\n",
              "    (rnn): GRU(128, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10004, 128)\n",
              "    (rnn): GRU(384, 256)\n",
              "    (fc_out): Linear(in_features=640, out_features=10004, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhQwmnOibhsa",
        "outputId": "cc623559-aa94-46c0-de1b-f316daf7c2c3"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 9,763,092 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvBW8KA5bktH"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95U4u9DHbmNF"
      },
      "source": [
        "TRG_PAD_IDX = TWEET.vocab.stoi[TWEET.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej2Erb9pbt1l"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, epoch):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.tweet\n",
        "        trg = batch.response\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        if(not i % 80):\n",
        "          print(f\"Epoch:{epoch}, Batch:{i}, Batch Loss:{loss.item()}\") \n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woBzibKVbvCl"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.tweet\n",
        "            trg = batch.response\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1VrTr7Ibwnl"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVtCqJYObx9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dbe9b6-8b20-4d49-8e5c-6149961b8bee"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP,epoch)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0, Batch:0, Batch Loss:4.725342273712158\n",
            "Epoch:0, Batch:80, Batch Loss:4.62764835357666\n",
            "Epoch:0, Batch:160, Batch Loss:4.399036884307861\n",
            "Epoch:0, Batch:240, Batch Loss:4.864499092102051\n",
            "Epoch:0, Batch:320, Batch Loss:4.345799446105957\n",
            "Epoch:0, Batch:400, Batch Loss:4.2641777992248535\n",
            "Epoch:0, Batch:480, Batch Loss:4.041617393493652\n",
            "Epoch:0, Batch:560, Batch Loss:4.023436069488525\n",
            "Epoch:0, Batch:640, Batch Loss:3.9685771465301514\n",
            "Epoch: 01 | Time: 4m 54s\n",
            "\tTrain Loss: 4.400 | Train PPL:  81.452\n",
            "\t Val. Loss: 5.862 |  Val. PPL: 351.273\n",
            "Epoch:1, Batch:0, Batch Loss:3.978214740753174\n",
            "Epoch:1, Batch:80, Batch Loss:3.8067259788513184\n",
            "Epoch:1, Batch:160, Batch Loss:4.127160549163818\n",
            "Epoch:1, Batch:240, Batch Loss:4.076448917388916\n",
            "Epoch:1, Batch:320, Batch Loss:3.905701160430908\n",
            "Epoch:1, Batch:400, Batch Loss:3.8517212867736816\n",
            "Epoch:1, Batch:480, Batch Loss:3.789311408996582\n",
            "Epoch:1, Batch:560, Batch Loss:3.8706648349761963\n",
            "Epoch:1, Batch:640, Batch Loss:4.3266377449035645\n",
            "Epoch: 02 | Time: 4m 55s\n",
            "\tTrain Loss: 4.024 | Train PPL:  55.911\n",
            "\t Val. Loss: 5.681 |  Val. PPL: 293.136\n",
            "Epoch:2, Batch:0, Batch Loss:3.568803548812866\n",
            "Epoch:2, Batch:80, Batch Loss:3.641017436981201\n",
            "Epoch:2, Batch:160, Batch Loss:3.74377179145813\n",
            "Epoch:2, Batch:240, Batch Loss:3.352635145187378\n",
            "Epoch:2, Batch:320, Batch Loss:3.926273822784424\n",
            "Epoch:2, Batch:400, Batch Loss:3.703125238418579\n",
            "Epoch:2, Batch:480, Batch Loss:4.202154159545898\n",
            "Epoch:2, Batch:560, Batch Loss:4.225384712219238\n",
            "Epoch:2, Batch:640, Batch Loss:3.8514199256896973\n",
            "Epoch: 03 | Time: 4m 59s\n",
            "\tTrain Loss: 3.776 | Train PPL:  43.662\n",
            "\t Val. Loss: 5.657 |  Val. PPL: 286.334\n",
            "Epoch:3, Batch:0, Batch Loss:3.7394979000091553\n",
            "Epoch:3, Batch:80, Batch Loss:3.5772900581359863\n",
            "Epoch:3, Batch:160, Batch Loss:3.8532655239105225\n",
            "Epoch:3, Batch:240, Batch Loss:3.472353219985962\n",
            "Epoch:3, Batch:320, Batch Loss:3.314514398574829\n",
            "Epoch:3, Batch:400, Batch Loss:3.781459093093872\n",
            "Epoch:3, Batch:480, Batch Loss:3.5329084396362305\n",
            "Epoch:3, Batch:560, Batch Loss:3.8095288276672363\n",
            "Epoch:3, Batch:640, Batch Loss:3.5966479778289795\n",
            "Epoch: 04 | Time: 4m 55s\n",
            "\tTrain Loss: 3.637 | Train PPL:  37.995\n",
            "\t Val. Loss: 5.698 |  Val. PPL: 298.307\n",
            "Epoch:4, Batch:0, Batch Loss:3.5820209980010986\n",
            "Epoch:4, Batch:80, Batch Loss:3.280839443206787\n",
            "Epoch:4, Batch:160, Batch Loss:3.395580530166626\n",
            "Epoch:4, Batch:240, Batch Loss:3.436155319213867\n",
            "Epoch:4, Batch:320, Batch Loss:3.278549909591675\n",
            "Epoch:4, Batch:400, Batch Loss:3.2893500328063965\n",
            "Epoch:4, Batch:480, Batch Loss:3.6623756885528564\n",
            "Epoch:4, Batch:560, Batch Loss:3.584721803665161\n",
            "Epoch:4, Batch:640, Batch Loss:3.7906110286712646\n",
            "Epoch: 05 | Time: 4m 52s\n",
            "\tTrain Loss: 3.540 | Train PPL:  34.473\n",
            "\t Val. Loss: 5.681 |  Val. PPL: 293.295\n",
            "Epoch:5, Batch:0, Batch Loss:3.4966330528259277\n",
            "Epoch:5, Batch:80, Batch Loss:3.466008424758911\n",
            "Epoch:5, Batch:160, Batch Loss:3.3320322036743164\n",
            "Epoch:5, Batch:240, Batch Loss:3.498567581176758\n",
            "Epoch:5, Batch:320, Batch Loss:3.3325891494750977\n",
            "Epoch:5, Batch:400, Batch Loss:3.205151081085205\n",
            "Epoch:5, Batch:480, Batch Loss:3.139066457748413\n",
            "Epoch:5, Batch:560, Batch Loss:3.5258193016052246\n",
            "Epoch:5, Batch:640, Batch Loss:3.440101385116577\n",
            "Epoch: 06 | Time: 4m 55s\n",
            "\tTrain Loss: 3.466 | Train PPL:  32.006\n",
            "\t Val. Loss: 5.687 |  Val. PPL: 294.995\n",
            "Epoch:6, Batch:0, Batch Loss:3.1002557277679443\n",
            "Epoch:6, Batch:80, Batch Loss:3.5772736072540283\n",
            "Epoch:6, Batch:160, Batch Loss:3.4842641353607178\n",
            "Epoch:6, Batch:240, Batch Loss:3.334102153778076\n",
            "Epoch:6, Batch:320, Batch Loss:3.273094415664673\n",
            "Epoch:6, Batch:400, Batch Loss:3.0585527420043945\n",
            "Epoch:6, Batch:480, Batch Loss:3.584012985229492\n",
            "Epoch:6, Batch:560, Batch Loss:3.5132336616516113\n",
            "Epoch:6, Batch:640, Batch Loss:3.4032092094421387\n",
            "Epoch: 07 | Time: 4m 59s\n",
            "\tTrain Loss: 3.410 | Train PPL:  30.251\n",
            "\t Val. Loss: 5.745 |  Val. PPL: 312.529\n",
            "Epoch:7, Batch:0, Batch Loss:3.530346632003784\n",
            "Epoch:7, Batch:80, Batch Loss:3.36381196975708\n",
            "Epoch:7, Batch:160, Batch Loss:3.176259994506836\n",
            "Epoch:7, Batch:240, Batch Loss:3.382678747177124\n",
            "Epoch:7, Batch:320, Batch Loss:3.26006817817688\n",
            "Epoch:7, Batch:400, Batch Loss:3.4378490447998047\n",
            "Epoch:7, Batch:480, Batch Loss:3.599073648452759\n",
            "Epoch:7, Batch:560, Batch Loss:3.322866678237915\n",
            "Epoch:7, Batch:640, Batch Loss:3.482238292694092\n",
            "Epoch: 08 | Time: 4m 59s\n",
            "\tTrain Loss: 3.363 | Train PPL:  28.862\n",
            "\t Val. Loss: 5.699 |  Val. PPL: 298.586\n",
            "Epoch:8, Batch:0, Batch Loss:3.069021463394165\n",
            "Epoch:8, Batch:80, Batch Loss:3.2638330459594727\n",
            "Epoch:8, Batch:160, Batch Loss:3.5727508068084717\n",
            "Epoch:8, Batch:240, Batch Loss:3.5707244873046875\n",
            "Epoch:8, Batch:320, Batch Loss:3.739116668701172\n",
            "Epoch:8, Batch:400, Batch Loss:3.485154867172241\n",
            "Epoch:8, Batch:480, Batch Loss:3.429231643676758\n",
            "Epoch:8, Batch:560, Batch Loss:3.0489449501037598\n",
            "Epoch:8, Batch:640, Batch Loss:3.772697925567627\n",
            "Epoch: 09 | Time: 4m 59s\n",
            "\tTrain Loss: 3.325 | Train PPL:  27.809\n",
            "\t Val. Loss: 5.735 |  Val. PPL: 309.628\n",
            "Epoch:9, Batch:0, Batch Loss:3.145784378051758\n",
            "Epoch:9, Batch:80, Batch Loss:3.9000608921051025\n",
            "Epoch:9, Batch:160, Batch Loss:3.1685261726379395\n",
            "Epoch:9, Batch:240, Batch Loss:3.1591074466705322\n",
            "Epoch:9, Batch:320, Batch Loss:3.2547171115875244\n",
            "Epoch:9, Batch:400, Batch Loss:3.493823766708374\n",
            "Epoch:9, Batch:480, Batch Loss:3.4150495529174805\n",
            "Epoch:9, Batch:560, Batch Loss:3.1975550651550293\n",
            "Epoch:9, Batch:640, Batch Loss:3.0226340293884277\n",
            "Epoch: 10 | Time: 5m 0s\n",
            "\tTrain Loss: 3.292 | Train PPL:  26.902\n",
            "\t Val. Loss: 5.752 |  Val. PPL: 314.870\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END_NLP_Assignment_7_StanfordSentimentAnalysis_LSTM_RNN_FineGrained_2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WVjCuKK_LVEF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKSTi9GepUmJ",
        "outputId": "6254007f-1619-4df0-facc-2342ef5e3ea2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhrCTg2Qphmm"
      },
      "source": [
        "!cp '/content/gdrive/My Drive/EVA/stanfordSentimentTreebank.zip' stanfordSentimentTreebank.zip\n",
        "!unzip -q -o stanfordSentimentTreebank.zip -d stanfordSentimentTreebank"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1-Yz-5RRFYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d7245357-252c-4b64-ee2d-79b9a6c67ccf"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/stanfordSentimentTreebank/stanfordSentimentTreebank/datasetSentences.txt',sep='\\t')\n",
        "df.tail()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11850</th>\n",
              "      <td>11851</td>\n",
              "      <td>A real snooze .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11851</th>\n",
              "      <td>11852</td>\n",
              "      <td>No surprises .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11852</th>\n",
              "      <td>11853</td>\n",
              "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11853</th>\n",
              "      <td>11854</td>\n",
              "      <td>Her fans walked out muttering words like `` ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11854</th>\n",
              "      <td>11855</td>\n",
              "      <td>In this case zero .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index                                           sentence\n",
              "11850           11851                                    A real snooze .\n",
              "11851           11852                                     No surprises .\n",
              "11852           11853  We 've seen the hippie-turned-yuppie plot befo...\n",
              "11853           11854  Her fans walked out muttering words like `` ho...\n",
              "11854           11855                                In this case zero ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JdpCW-YbAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65fca234-a13f-4515-e28d-f7626a776326"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqRsoF6xYdgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0fa545-3572-4dbe-cb07-8f39c33a7c80"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11855 entries, 0 to 11854\n",
            "Data columns (total 2 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   sentence_index  11855 non-null  int64 \n",
            " 1   sentence        11855 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 185.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgaTy-qJ_ATn"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoEXPawa_3tN"
      },
      "source": [
        "class StanfordDatasetReader():\n",
        "  def __init__(self, sst_dir, split_idx):\n",
        "    \n",
        "    merged_dataset = self.get_merged_dataset(sst_dir)\n",
        "    merged_dataset['sentiment values'] = merged_dataset['sentiment values'].astype(float)\n",
        "    self.dataset = merged_dataset[merged_dataset[\"splitset_label\"] == split_idx]\n",
        "    # self.dataset[\"Revised_Sentiment\"] = self.discretize_label(self.dataset.iloc[5])\n",
        "    self.dataset['Revised_sentiment values'] = self.dataset.apply(lambda x: labelfunc(x[\"sentiment values\"]), axis=1)\n",
        "    # train_st_data['Revised_sentiment values'] = train_st_data.apply(lambda x: myfunc(x[\"sentiment values\"]), axis=1)\n",
        "  # https://github.com/iamsimha/conv-sentiment-analysis/blob/master/code/dataset_reader.py\n",
        "  def get_merged_dataset(self, sst_dir):\n",
        "\n",
        "    sentiment_labels = pd.read_csv(os.path.join(sst_dir, \"sentiment_labels.txt\"), sep=\"|\")\n",
        "    sentence_ids = pd.read_csv(os.path.join(sst_dir, \"datasetSentences.txt\"), sep=\"\\t\")\n",
        "    dictionary = pd.read_csv(os.path.join(sst_dir, \"dictionary.txt\"), sep=\"|\", names=['phrase', 'phrase ids'])\n",
        "    train_test_split = pd.read_csv(os.path.join(sst_dir, \"datasetSplit.txt\"))\n",
        "    sentence_phrase_merge = pd.merge(sentence_ids, dictionary, left_on='sentence', right_on='phrase')\n",
        "    sentence_phrase_split = pd.merge(sentence_phrase_merge, train_test_split, on='sentence_index')\n",
        "    return pd.merge(sentence_phrase_split, sentiment_labels, on='phrase ids').sample(frac=1)\n",
        "\n",
        "  def discretize_label(self, label):\n",
        "    print(type(label))\n",
        "    if label <= 0.2: return 0\n",
        "    if label <= 0.4: return 1\n",
        "    if label <= 0.6: return 2\n",
        "    if label <= 0.8: return 3\n",
        "    return 4\n",
        "\n",
        "  def word_to_index(self, word):\n",
        "    if word in self.w2i:\n",
        "      return self.w2i[word]\n",
        "    else:\n",
        "      return self.w2i[\"<OOV>\"]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.dataset.shape[0]\n",
        "    \n",
        "  # def __getitem__(self, idx):\n",
        "  #   return {\"sentence\": [self.word_to_index(x) for x in self.dataset.iloc[idx, 1].split()],\n",
        "  #           \"label\": self.discretize_label(self.dataset.iloc[idx, 5])}\n",
        "  def labelfunc(label):\n",
        "    if label <= 0.5: return 0\n",
        "    if label <= 0.4: return 1\n",
        "    if label <= 0.6: return 2\n",
        "    if label <= 0.8: return 3\n",
        "    return 4\n",
        "\n",
        "  def get_data(self):\n",
        "    return self.dataset\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\"sentence\": [x for x in self.dataset.iloc[idx, 1].split()],\n",
        "            \"label\": self.discretize_label(self.dataset.iloc[idx, 5])}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1BWSC8jd7AZ"
      },
      "source": [
        "def labelfunc(label):\n",
        "  if label <= 0.4: return 0\n",
        "  if label <= 0.6: return 1\n",
        "  if label <= 0.6: return 2\n",
        "  if label <= 0.8: return 3\n",
        "  return 4"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBxRHuT5Fhyb"
      },
      "source": [
        "import os\n",
        "def load_data(sst_dir=\"/content/stanfordSentimentTreebank/stanfordSentimentTreebank/\"):\n",
        "  train_st_data_cl = StanfordDatasetReader(sst_dir, 1).get_data()\n",
        "  # train_st_data_cl['Revised_sentiment values'] = train_st_data.apply(lambda x: labelfunc(x[\"sentiment values\"]), axis=1)\n",
        "  test_st_data_cl = StanfordDatasetReader(sst_dir, 2).get_data()\n",
        "  # test_st_data_cl['Revised_sentiment values'] = test_st_data_cl.apply(lambda x: labelfunc(x[\"sentiment values\"]), axis=1)\n",
        "  validation_st_data_cl = StanfordDatasetReader(sst_dir, 3).get_data()\n",
        "  # validation_st_data_cl['Revised_sentiment values'] = validation_st_data_cl.apply(lambda x: labelfunc(x[\"sentiment values\"]), axis=1)\n",
        "  return train_st_data_cl,test_st_data_cl,validation_st_data_cl"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABfKELBlJf2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa328c2f-1b73-4991-c9ec-163d11a5e382"
      },
      "source": [
        "train_st_data,test_st_data,validation_st_data = load_data()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c10qU_CBoOZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "6bfa5161-9cf6-476d-dbb1-ef73619b7e95"
      },
      "source": [
        "train_st_data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>Revised_sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8925</th>\n",
              "      <td>9349</td>\n",
              "      <td>Kung Pow seems like some futile concoction tha...</td>\n",
              "      <td>Kung Pow seems like some futile concoction tha...</td>\n",
              "      <td>185644</td>\n",
              "      <td>1</td>\n",
              "      <td>0.19444</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6668</th>\n",
              "      <td>6976</td>\n",
              "      <td>Jaglom offers the none-too-original premise th...</td>\n",
              "      <td>Jaglom offers the none-too-original premise th...</td>\n",
              "      <td>146820</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26389</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>7322</td>\n",
              "      <td>is the kind of movie that 's critic-proof , si...</td>\n",
              "      <td>is the kind of movie that 's critic-proof , si...</td>\n",
              "      <td>163462</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26389</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4918</th>\n",
              "      <td>5145</td>\n",
              "      <td>Waiting for Godard can be fruitful : ` In Prai...</td>\n",
              "      <td>Waiting for Godard can be fruitful : ` In Prai...</td>\n",
              "      <td>110911</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>10492</td>\n",
              "      <td>But , no , we get another scene , and then ano...</td>\n",
              "      <td>But , no , we get another scene , and then ano...</td>\n",
              "      <td>222713</td>\n",
              "      <td>1</td>\n",
              "      <td>0.41667</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ... Revised_sentiment values\n",
              "8925             9349  ...                        0\n",
              "6668             6976  ...                        0\n",
              "6998             7322  ...                        0\n",
              "4918             5145  ...                        3\n",
              "10001           10492  ...                        1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "fwFAQsWogYg-",
        "outputId": "37478898-2b51-4f36-a9e3-420f6704ab4f"
      },
      "source": [
        "test_st_data.tail()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>Revised_sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7604</th>\n",
              "      <td>7963</td>\n",
              "      <td>The worst kind of independent ; the one where ...</td>\n",
              "      <td>The worst kind of independent ; the one where ...</td>\n",
              "      <td>149995</td>\n",
              "      <td>2</td>\n",
              "      <td>0.416670</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>682</th>\n",
              "      <td>708</td>\n",
              "      <td>Smith profiles five extraordinary American hom...</td>\n",
              "      <td>Smith profiles five extraordinary American hom...</td>\n",
              "      <td>26603</td>\n",
              "      <td>2</td>\n",
              "      <td>0.739580</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>806</td>\n",
              "      <td>The perfect film for those who like sick comed...</td>\n",
              "      <td>The perfect film for those who like sick comed...</td>\n",
              "      <td>26882</td>\n",
              "      <td>2</td>\n",
              "      <td>0.638890</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>405</td>\n",
              "      <td>Although it bangs a very cliched drum at times...</td>\n",
              "      <td>Although it bangs a very cliched drum at times...</td>\n",
              "      <td>18632</td>\n",
              "      <td>2</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7981</th>\n",
              "      <td>8360</td>\n",
              "      <td>Extremely dumb .</td>\n",
              "      <td>Extremely dumb .</td>\n",
              "      <td>223315</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentence_index  ... Revised_sentiment values\n",
              "7604            7963  ...                        1\n",
              "682              708  ...                        3\n",
              "776              806  ...                        3\n",
              "395              405  ...                        3\n",
              "7981            8360  ...                        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "HzM5HDWkgcip",
        "outputId": "97e62388-32fb-49a8-8312-521477f54761"
      },
      "source": [
        "validation_st_data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>Revised_sentiment values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7215</th>\n",
              "      <td>7554</td>\n",
              "      <td>A coarse and stupid gross-out .</td>\n",
              "      <td>A coarse and stupid gross-out .</td>\n",
              "      <td>143051</td>\n",
              "      <td>3</td>\n",
              "      <td>0.29167</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7286</th>\n",
              "      <td>7633</td>\n",
              "      <td>Characters still need to function according to...</td>\n",
              "      <td>Characters still need to function according to...</td>\n",
              "      <td>144414</td>\n",
              "      <td>3</td>\n",
              "      <td>0.33333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7331</th>\n",
              "      <td>7681</td>\n",
              "      <td>This is the sort of burly action flick where o...</td>\n",
              "      <td>This is the sort of burly action flick where o...</td>\n",
              "      <td>150256</td>\n",
              "      <td>3</td>\n",
              "      <td>0.27778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7344</th>\n",
              "      <td>7697</td>\n",
              "      <td>By the miserable standards to which the slashe...</td>\n",
              "      <td>By the miserable standards to which the slashe...</td>\n",
              "      <td>222853</td>\n",
              "      <td>3</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7105</th>\n",
              "      <td>7435</td>\n",
              "      <td>For starters , the story is just too slim .</td>\n",
              "      <td>For starters , the story is just too slim .</td>\n",
              "      <td>223402</td>\n",
              "      <td>3</td>\n",
              "      <td>0.33333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentence_index  ... Revised_sentiment values\n",
              "7215            7554  ...                        0\n",
              "7286            7633  ...                        0\n",
              "7331            7681  ...                        0\n",
              "7344            7697  ...                        1\n",
              "7105            7435  ...                        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFCfiRpdGnQD"
      },
      "source": [
        "### Further NLP Augemnattion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ubb2QEEG0in",
        "outputId": "fb925c40-e58e-4cfa-dcab-74fd3af19505"
      },
      "source": [
        "!pip install nlpaug"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nlpaug\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/40/18941536abc63578010e87808089eb3184a8d027df03bfc226894698f491/nlpaug-1.1.0-py3-none-any.whl (380kB)\n",
            "\r\u001b[K     |▉                               | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 18.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 51kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 81kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 92kB 10.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 102kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 122kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 133kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 143kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 163kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 174kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 184kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 194kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 204kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 215kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 235kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 245kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 256kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 266kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 276kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 286kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 296kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 307kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 317kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 327kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 337kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 348kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 358kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 368kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 378kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 389kB 10.7MB/s \n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMI8CXdrIMGy"
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGXDQghJEqmV"
      },
      "source": [
        "## Lets do the NLP data augmentation\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "\n",
        "from nlpaug.util import Action"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed8FaP3MG-kc"
      },
      "source": [
        "##### Some basic examples for understanding and then further data augmentation by these\n",
        "- Substitute word by WordNet's synonym\n",
        "- Swap word randomly\n",
        "- Delete a set of contunous word will be removed randomly\n",
        "- Delete word randomly augemnattion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9gledVX9KtEj",
        "outputId": "59d4e3e2-91d0-4771-9fa1-66c95753b638"
      },
      "source": [
        "# validation_st_data.head()\n",
        "train_st_data['sentence'].iloc[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Kung Pow seems like some futile concoction that was developed hastily after Oedekerk and his fellow moviemakers got through crashing a college keg party .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0gu3UXtXqH4",
        "outputId": "f551a15c-3f55-48d0-f4ff-4ea76bc68991"
      },
      "source": [
        "aug = naw.SynonymAug(aug_src='wordnet') ## Substitute word by WordNet's synonym¶\n",
        "\n",
        "augmented_text = aug.augment(train_st_data['sentence'].iloc[0])\n",
        "print(\"Original:\")\n",
        "print(train_st_data['sentence'].iloc[0])\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)\n",
        "train_st_data_SynonymAug_aug = train_st_data\n",
        "train_st_data_SynonymAug_aug['sentence_aug'] = train_st_data_SynonymAug_aug.apply(lambda x: aug.augment(x['sentence']),axis=1)  ## Swap word randomly¶"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Original:\n",
            "Kung Pow seems like some futile concoction that was developed hastily after Oedekerk and his fellow moviemakers got through crashing a college keg party .\n",
            "Augmented Text:\n",
            "Kung Pow appear comparable some futile concoction that was developed hastily after Oedekerk and his fellow moviemakers buzz off through crashing a college keg party.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBoPakkKX3ly",
        "outputId": "68c71194-16ea-4c83-8794-1ba178945160"
      },
      "source": [
        "aug = naw.RandomWordAug(action=\"swap\") # Swap word randomly¶\n",
        "\n",
        "augmented_text = aug.augment(train_st_data['sentence'].iloc[0])\n",
        "print(\"Original:\")\n",
        "print(train_st_data['sentence'].iloc[0])\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)\n",
        "train_st_data_swap_aug = train_st_data\n",
        "train_st_data_swap_aug['sentence_aug'] = train_st_data_swap_aug.apply(lambda x: aug.augment(x['sentence']),axis=1)  ## Swap word randomly¶"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "Kung Pow seems like some futile concoction that was developed hastily after Oedekerk and his fellow moviemakers got through crashing a college keg party .\n",
            "Augmented Text:\n",
            "Kung Pow seems like futile some concoction that was developed hastily after and Oedekerk his fellow moviemakers got through a crashing keg party college.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n24qk3VIYIR3"
      },
      "source": [
        "# aug = naw.RandomWordAug(action='crop',aug_p=0.5, aug_min=0)\n",
        "# augmented_text = aug.augment(train_st_data['sentence'].iloc[0])  ## Delete a set of contunous word will be removed randomly¶\n",
        "\n",
        "# print(\"Original:\")\n",
        "# print(train_st_data['sentence'].iloc[0])\n",
        "# print(\"Augmented Text:\")\n",
        "# print(augmented_text)\n",
        "# train_st_data_crop_aug = train_st_data\n",
        "# train_st_data_crop_aug['sentence_aug'] = train_st_data_crop_aug.apply(lambda x: aug.augment(x['sentence']),axis=1)  ## Delete a set of contunous word will be removed randomly¶"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcDwOTqxNXkc"
      },
      "source": [
        "text = 'The quick brown fox jumps over the lazy dog .'\n",
        "# Augmenter that apply random word operation to textual input.Augmenter that apply randomly behavior for augmentation.\n",
        "aug = naw.RandomWordAug()\n",
        "augmented_data = aug.augment(text)\n",
        "augmented_data\n",
        "\n",
        "train_st_data_delete_aug = train_st_data\n",
        "# train_st_data_aug[sentence_aug] = aug.augment(train_st_data_aug.loc[\"sentence\"] )\n",
        "#--Using position to slice Email using a lambda function\n",
        "train_st_data_delete_aug['sentence_aug'] = train_st_data_delete_aug.apply(lambda x: aug.augment(x['sentence']),axis=1)  ## Delete word randomly augemnattion\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFRlJ07oZnwX",
        "outputId": "658fb2f7-3f40-4b6a-9cb5-07a57c78e3a0"
      },
      "source": [
        "print(\"Original:\")\n",
        "print(train_st_data_delete_aug['sentence'].iloc[0])\n",
        "print(\"Augmented Text:\")\n",
        "print(train_st_data_delete_aug['sentence_aug'].iloc[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "Kung Pow seems like some futile concoction that was developed hastily after Oedekerk and his fellow moviemakers got through crashing a college keg party .\n",
            "Augmented Text:\n",
            "Pow seems futile concoction was developed hastily after Oedekerk and his fellow moviemakers crashing a college keg.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "2zMubaWhVP2w",
        "outputId": "c7d8eb4f-45e8-4d92-fb80-8cac528617b2"
      },
      "source": [
        "train_st_data_delete_aug.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>Revised_sentiment values</th>\n",
              "      <th>sentence_aug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8925</th>\n",
              "      <td>9349</td>\n",
              "      <td>Kung Pow seems like some futile concoction tha...</td>\n",
              "      <td>Kung Pow seems like some futile concoction tha...</td>\n",
              "      <td>185644</td>\n",
              "      <td>1</td>\n",
              "      <td>0.19444</td>\n",
              "      <td>0</td>\n",
              "      <td>Pow seems futile concoction was developed hast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6668</th>\n",
              "      <td>6976</td>\n",
              "      <td>Jaglom offers the none-too-original premise th...</td>\n",
              "      <td>Jaglom offers the none-too-original premise th...</td>\n",
              "      <td>146820</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26389</td>\n",
              "      <td>0</td>\n",
              "      <td>Jaglom offers - too - premise that involved mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>7322</td>\n",
              "      <td>is the kind of movie that 's critic-proof , si...</td>\n",
              "      <td>is the kind of movie that 's critic-proof , si...</td>\n",
              "      <td>163462</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26389</td>\n",
              "      <td>0</td>\n",
              "      <td>is the kind of ' critic -, simply because aims...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4918</th>\n",
              "      <td>5145</td>\n",
              "      <td>Waiting for Godard can be fruitful : ` In Prai...</td>\n",
              "      <td>Waiting for Godard can be fruitful : ` In Prai...</td>\n",
              "      <td>110911</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75000</td>\n",
              "      <td>3</td>\n",
              "      <td>Waiting for can: ` In of Love ' is ' s epitaph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>10492</td>\n",
              "      <td>But , no , we get another scene , and then ano...</td>\n",
              "      <td>But , no , we get another scene , and then ano...</td>\n",
              "      <td>222713</td>\n",
              "      <td>1</td>\n",
              "      <td>0.41667</td>\n",
              "      <td>1</td>\n",
              "      <td>, no, we get another, and another.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ...                                       sentence_aug\n",
              "8925             9349  ...  Pow seems futile concoction was developed hast...\n",
              "6668             6976  ...  Jaglom offers - too - premise that involved mo...\n",
              "6998             7322  ...  is the kind of ' critic -, simply because aims...\n",
              "4918             5145  ...  Waiting for can: ` In of Love ' is ' s epitaph...\n",
              "10001           10492  ...                 , no, we get another, and another.\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw_pqEU9csk0"
      },
      "source": [
        "## Now I need to add all these data frames\n",
        "combined_data_aug = pd.concat([train_st_data_delete_aug, train_st_data_swap_aug, train_st_data_SynonymAug_aug], axis=0)\n",
        "## after this, now I need to drop the sentence column and rename sentence_aug to sentence\n",
        "combined_data_aug.drop('sentence', axis=1, inplace=True)\n",
        "combined_data_aug.rename(columns = {'sentence_aug':'sentence'}, inplace = True) "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "quNy-dDMzfeL",
        "outputId": "75f65226-ab82-4038-b792-9a26522defe1"
      },
      "source": [
        "combined_data_aug.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>phrase</th>\n",
              "      <th>phrase ids</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>Revised_sentiment values</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8925</th>\n",
              "      <td>9349</td>\n",
              "      <td>Kung Pow seems like some futile concoction tha...</td>\n",
              "      <td>185644</td>\n",
              "      <td>1</td>\n",
              "      <td>0.19444</td>\n",
              "      <td>0</td>\n",
              "      <td>Pow seems futile concoction was developed hast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6668</th>\n",
              "      <td>6976</td>\n",
              "      <td>Jaglom offers the none-too-original premise th...</td>\n",
              "      <td>146820</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26389</td>\n",
              "      <td>0</td>\n",
              "      <td>Jaglom offers - too - premise that involved mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>7322</td>\n",
              "      <td>is the kind of movie that 's critic-proof , si...</td>\n",
              "      <td>163462</td>\n",
              "      <td>1</td>\n",
              "      <td>0.26389</td>\n",
              "      <td>0</td>\n",
              "      <td>is the kind of ' critic -, simply because aims...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4918</th>\n",
              "      <td>5145</td>\n",
              "      <td>Waiting for Godard can be fruitful : ` In Prai...</td>\n",
              "      <td>110911</td>\n",
              "      <td>1</td>\n",
              "      <td>0.75000</td>\n",
              "      <td>3</td>\n",
              "      <td>Waiting for can: ` In of Love ' is ' s epitaph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>10492</td>\n",
              "      <td>But , no , we get another scene , and then ano...</td>\n",
              "      <td>222713</td>\n",
              "      <td>1</td>\n",
              "      <td>0.41667</td>\n",
              "      <td>1</td>\n",
              "      <td>, no, we get another, and another.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_index  ...                                           sentence\n",
              "8925             9349  ...  Pow seems futile concoction was developed hast...\n",
              "6668             6976  ...  Jaglom offers - too - premise that involved mo...\n",
              "6998             7322  ...  is the kind of ' critic -, simply because aims...\n",
              "4918             5145  ...  Waiting for can: ` In of Love ' is ' s epitaph...\n",
              "10001           10492  ...                 , no, we get another, and another.\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZkxFvYtGs7K"
      },
      "source": [
        "### Final Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arFy0s73nImt"
      },
      "source": [
        "def get_final_data(train_st_data,test_st_data,validation_st_data,combined_data_aug):\n",
        "  train_st_data_final = train_st_data.drop(['sentence_index','phrase','phrase ids','splitset_label','sentiment values'],axis=1)\n",
        "  train_st_data_final.rename(columns = {'Revised_sentiment values': 'sentiment'}, inplace = True)\n",
        "\n",
        "  combined_data_aug.drop(['sentence_index','phrase','phrase ids','splitset_label','sentiment values'],axis=1,inplace=True)\n",
        "  combined_data_aug.rename(columns = {'Revised_sentiment values': 'sentiment'}, inplace = True)\n",
        "\n",
        "  train_st_data_final_mixed = pd.concat([combined_data_aug, train_st_data_final], axis=0)\n",
        "\n",
        "  train_st_data_final_mixed = train_st_data_final_mixed.reset_index(drop=True) ## This is being done because data.Example.fromlist was failing\n",
        "\n",
        "  test_st_data_final = test_st_data.drop(['sentence_index','phrase','phrase ids','splitset_label','sentiment values'],axis=1)\n",
        "  test_st_data_final.rename(columns = {'Revised_sentiment values': 'sentiment'}, inplace = True)\n",
        "  test_st_data_final = test_st_data_final.reset_index(drop=True) ## This is being done because data.Example.fromlist was failing\n",
        "\n",
        "  validation_st_data_final = validation_st_data.drop(['sentence_index','phrase','phrase ids','splitset_label','sentiment values'],axis=1)\n",
        "  validation_st_data_final.rename(columns = {'Revised_sentiment values': 'sentiment'} , inplace = True)\n",
        "  validation_st_data_final = validation_st_data_final.reset_index(drop=True) ## This is being done because data.Example.fromlist was failing\n",
        "\n",
        "  return train_st_data_final_mixed, test_st_data_final, validation_st_data_final"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjmsNd8wJiIj"
      },
      "source": [
        "train_st_data_final, test_st_data_final, validation_st_data_final = get_final_data(train_st_data,test_st_data,validation_st_data,combined_data_aug)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "iWZytpY6pE2S",
        "outputId": "8d58b9e6-f020-4456-ae2f-affeda8636c4"
      },
      "source": [
        "train_st_data_final.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_aug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Pow seems futile concoction was developed hast...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Jaglom offers - too - premise that involved mo...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>is the kind of ' critic -, simply because aims...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Waiting for can: ` In of Love ' is ' s epitaph...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>, no, we get another, and another.</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                           sentence sentence_aug\n",
              "0          0  Pow seems futile concoction was developed hast...          NaN\n",
              "1          0  Jaglom offers - too - premise that involved mo...          NaN\n",
              "2          0  is the kind of ' critic -, simply because aims...          NaN\n",
              "3          3  Waiting for can: ` In of Love ' is ' s epitaph...          NaN\n",
              "4          1                 , no, we get another, and another.          NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0p26-wQLbV4"
      },
      "source": [
        "# train_st_data_final.to_csv(r'train_st_data_final.csv', index = False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Z1M-vssT03",
        "outputId": "48c56ad7-0e63-425a-c9c3-8ae14cfdea91"
      },
      "source": [
        "train_st_data_final.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32468, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCq6t89LsXvI",
        "outputId": "c4ce3325-7ee4-4b34-ea88-e5fbe02e4201"
      },
      "source": [
        "train_st_data_final.sentiment.value_counts()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    12488\n",
              "3     8864\n",
              "1     6196\n",
              "4     4920\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dQTLWRqVxME",
        "outputId": "6d1d9fd7-e222-4e66-864f-b72a6a6e8a7e"
      },
      "source": [
        "validation_st_data_final.sentiment.value_counts()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    408\n",
              "3    259\n",
              "1    219\n",
              "4    158\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk8IP4SK1Lrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3430377-c6b1-4c88-a7a1-37dff614cd06"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb1cd2b0cd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U"
      },
      "source": [
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Sentiment = data.LabelField(tokenize ='spacy',is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('sentence', Sentence),('sentiment',Sentiment)]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0UOsNaNO-hp"
      },
      "source": [
        "# saving the dataframe \n",
        "train_st_data_final.to_csv('train_st_data_final.csv', index=False) \n",
        "test_st_data_final.to_csv('test_st_data_final.csv', index=False) \n",
        "validation_st_data_final.to_csv('validation_st_data_final.csv', index=False) "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgH1g8jTMPuH"
      },
      "source": [
        "# df = pd.read_csv('train_st_data_final.csv')\n",
        "# df.head()\n",
        "# example_trng = [data.Example.fromlist([df.sentence[i],df.sentiment[i]], fields) for i in range(df.shape[0])] "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIlOGo2HvOHi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OLcJ5B7rHz"
      },
      "source": [
        "example_trng = [data.Example.fromlist([train_st_data_final.sentence[i],train_st_data_final.sentiment[i]], fields) for i in range(train_st_data_final.shape[0])] \n",
        "example_val = [data.Example.fromlist([validation_st_data_final.sentence[i],validation_st_data_final.sentiment[i]], fields) for i in range(validation_st_data_final.shape[0])] "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd"
      },
      "source": [
        "# Creating dataset\n",
        "#twitterDataset = data.TabularDataset(path=\"tweets.csv\", format=\"CSV\", fields=fields, skip_header=True)\n",
        "\n",
        "# twitterDataset = data.Dataset(example, fields)\n",
        "train = data.Dataset(example_trng, fields)\n",
        "valid = data.Dataset(example_val, fields)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZnyCPaR08F"
      },
      "source": [
        "Finally, we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPYXyuKhRpBk"
      },
      "source": [
        "# (train, valid) = twitterDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykvsCGQMR6UD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "7643fec3-c174-48cb-e2a1-6a5a81ba9fdd"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-b4b8914499ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'function' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpEOQruR9JL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "26e6cf05-ab65-4e62-8564-80b63d9afebd"
      },
      "source": [
        "vars(train.examples[10])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-641da66dfddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'examples'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGH3R6IYcY_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "e4ba124d-dde4-4dcd-aff9-d11787d1d557"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "Sentence.build_vocab(train, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-07a43b07ff88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_VOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.100d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                  unk_init = torch.Tensor.normal_)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9CVBX6IhZtL"
      },
      "source": [
        "Sentiment.build_vocab(train)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "# Sentence.build_vocab(train)\n",
        "# Sentiment.build_vocab(train)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6eeb40-c681-476a-af37-f2d5dc1a08d1"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Sentiment.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Sentiment.vocab.stoi)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17052\n",
            "Size of label vocab :  4\n",
            "Top 10 words appreared repeatedly : [('.', 31935), (',', 26833), ('the', 17342), ('-', 13429), (\"'\", 13351), ('of', 12687), ('and', 12678), ('a', 12226), ('to', 8571), ('is', 7050)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7fb15864db70>, {0: 0, 3: 1, 1: 2, 4: 3})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOgXGAD3fqQs",
        "outputId": "154207de-1c6f-433a-b9af-2f613b53eda7"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Sentiment.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Sentiment.vocab.stoi)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  17052\n",
            "Size of label vocab :  4\n",
            "Top 10 words appreared repeatedly : [('.', 31935), (',', 26833), ('the', 17342), ('-', 13429), (\"'\", 13351), ('of', 12687), ('and', 12678), ('a', 12226), ('to', 8571), ('is', 7050)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7fb15864db70>, {0: 0, 3: 1, 1: 2, 4: 3})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.sentence),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Sentence.vocab.stoi, tokens)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, bidirectional,pad_idx):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim,padding_idx = pad_idx)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           bidirectional = bidirectional,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.dropout(self.fc(hidden))  \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "        # output = F.softmax(dense_outputs, dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Sentence.vocab)\n",
        "embedding_dim = 100\n",
        "num_hidden_nodes = 256\n",
        "num_output_nodes = 5\n",
        "num_layers = 3\n",
        "dropout = 0.2\n",
        "bidirectional = True\n",
        "PAD_IDX = Sentence.vocab.stoi[Sentence.pad_token]\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout, bidirectional,PAD_IDX)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-pOMqzJ3eTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e878d8e-0ff2-4156-f4fe-f32640108967"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(17052, 100, padding_idx=1)\n",
            "  (encoder): LSTM(100, 256, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "The model has 5,593,589 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt_e2IkGdRW8",
        "outputId": "eaa7dbd6-c130-4ef4-b1bb-6becdd90aa60"
      },
      "source": [
        "pretrained_embeddings = Sentence.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([17052, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1bHOjdUdY2a",
        "outputId": "876e22c6-9c28-46ff-f50f-ba679bd08d04"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0166, -0.4668,  2.0909,  ..., -1.4692,  0.4476, -0.7223],\n",
              "        [-0.0791, -0.2089, -0.3442,  ...,  0.4657,  0.6297, -1.7395],\n",
              "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
              "        ...,\n",
              "        [-0.2805,  0.1506,  0.3955,  ...,  0.6393,  0.0779,  0.7722],\n",
              "        [ 0.5732, -1.0756, -0.1600,  ...,  0.4548,  0.2344,  0.0364],\n",
              "        [-0.3997, -0.2994, -0.3571,  ...,  0.1802, -1.3936, -1.6659]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASWyq5tgiyLt",
        "outputId": "623e55b1-2080-4c34-bed4-a7ecd2c9621e"
      },
      "source": [
        "UNK_IDX = Sentence.vocab.stoi[Sentence.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "        ...,\n",
            "        [-0.2805,  0.1506,  0.3955,  ...,  0.6393,  0.0779,  0.7722],\n",
            "        [ 0.5732, -1.0756, -0.1600,  ...,  0.4548,  0.2344,  0.0364],\n",
            "        [-0.3997, -0.2994, -0.3571,  ...,  0.1802, -1.3936, -1.6659]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiVj8tajdTTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5c3147-e3e1-430e-863a-83db31d11706"
      },
      "source": [
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399950/400000 [00:40<00:00, 16545.52it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.sentence   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.sentiment)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.sentiment)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.sentence\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.sentiment)\n",
        "            acc = binary_accuracy(predictions, batch.sentiment)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq330XlnaEU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c7609f-797b-4923-8d97-24c6aa3b9fc4"
      },
      "source": [
        "N_EPOCHS = 15\n",
        "best_valid_loss = float('inf')\n",
        "#freeze embeddings\n",
        "model.embedding.weight.requires_grad = unfrozen = False\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.500 | Train Acc: 39.46%\n",
            "\t Val. Loss: 1.421 |  Val. Acc: 46.14% \n",
            "\n",
            "\tTrain Loss: 1.411 | Train Acc: 49.95%\n",
            "\t Val. Loss: 1.399 |  Val. Acc: 49.79% \n",
            "\n",
            "\tTrain Loss: 1.358 | Train Acc: 55.63%\n",
            "\t Val. Loss: 1.412 |  Val. Acc: 48.54% \n",
            "\n",
            "\tTrain Loss: 1.316 | Train Acc: 60.25%\n",
            "\t Val. Loss: 1.404 |  Val. Acc: 49.89% \n",
            "\n",
            "\tTrain Loss: 1.280 | Train Acc: 63.87%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 50.19% \n",
            "\n",
            "\tTrain Loss: 1.254 | Train Acc: 66.45%\n",
            "\t Val. Loss: 1.377 |  Val. Acc: 52.41% \n",
            "\n",
            "\tTrain Loss: 1.232 | Train Acc: 68.61%\n",
            "\t Val. Loss: 1.389 |  Val. Acc: 51.27% \n",
            "\n",
            "\tTrain Loss: 1.213 | Train Acc: 70.70%\n",
            "\t Val. Loss: 1.389 |  Val. Acc: 51.08% \n",
            "\n",
            "\tTrain Loss: 1.193 | Train Acc: 72.76%\n",
            "\t Val. Loss: 1.403 |  Val. Acc: 49.68% \n",
            "\n",
            "\tTrain Loss: 1.181 | Train Acc: 73.88%\n",
            "\t Val. Loss: 1.400 |  Val. Acc: 49.85% \n",
            "\n",
            "\tTrain Loss: 1.172 | Train Acc: 74.87%\n",
            "\t Val. Loss: 1.431 |  Val. Acc: 46.29% \n",
            "\n",
            "\tTrain Loss: 1.161 | Train Acc: 75.86%\n",
            "\t Val. Loss: 1.418 |  Val. Acc: 48.18% \n",
            "\n",
            "\tTrain Loss: 1.152 | Train Acc: 77.01%\n",
            "\t Val. Loss: 1.396 |  Val. Acc: 49.85% \n",
            "\n",
            "\tTrain Loss: 1.143 | Train Acc: 77.84%\n",
            "\t Val. Loss: 1.393 |  Val. Acc: 50.70% \n",
            "\n",
            "\tTrain Loss: 1.136 | Train Acc: 78.51%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 49.09% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs9COVJlW_JI"
      },
      "source": [
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6Z7TKg_W6s4",
        "outputId": "daae17f4-ab37-45e2-a781-038ac36e093d"
      },
      "source": [
        "# N_EPOCHS = 15\n",
        "# best_valid_loss = float('inf')\n",
        "# #freeze embeddings\n",
        "# model.embedding.weight.requires_grad = unfrozen = True\n",
        "\n",
        "# for epoch in range(N_EPOCHS):\n",
        "     \n",
        "#     # train the model\n",
        "#     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "#     # evaluate the model\n",
        "#     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "#     # save the best model\n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.213 | Train Acc: 70.59%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 49.00% \n",
            "\n",
            "\tTrain Loss: 1.154 | Train Acc: 76.56%\n",
            "\t Val. Loss: 1.387 |  Val. Acc: 51.65% \n",
            "\n",
            "\tTrain Loss: 1.114 | Train Acc: 80.64%\n",
            "\t Val. Loss: 1.430 |  Val. Acc: 46.69% \n",
            "\n",
            "\tTrain Loss: 1.090 | Train Acc: 83.12%\n",
            "\t Val. Loss: 1.449 |  Val. Acc: 44.64% \n",
            "\n",
            "\tTrain Loss: 1.073 | Train Acc: 84.69%\n",
            "\t Val. Loss: 1.427 |  Val. Acc: 47.03% \n",
            "\n",
            "\tTrain Loss: 1.059 | Train Acc: 86.40%\n",
            "\t Val. Loss: 1.407 |  Val. Acc: 49.20% \n",
            "\n",
            "\tTrain Loss: 1.051 | Train Acc: 87.06%\n",
            "\t Val. Loss: 1.466 |  Val. Acc: 43.22% \n",
            "\n",
            "\tTrain Loss: 1.045 | Train Acc: 87.53%\n",
            "\t Val. Loss: 1.440 |  Val. Acc: 45.70% \n",
            "\n",
            "\tTrain Loss: 1.038 | Train Acc: 88.33%\n",
            "\t Val. Loss: 1.446 |  Val. Acc: 45.00% \n",
            "\n",
            "\tTrain Loss: 1.033 | Train Acc: 88.86%\n",
            "\t Val. Loss: 1.461 |  Val. Acc: 43.66% \n",
            "\n",
            "\tTrain Loss: 1.026 | Train Acc: 89.44%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 46.27% \n",
            "\n",
            "\tTrain Loss: 1.023 | Train Acc: 89.99%\n",
            "\t Val. Loss: 1.448 |  Val. Acc: 45.45% \n",
            "\n",
            "\tTrain Loss: 1.020 | Train Acc: 90.36%\n",
            "\t Val. Loss: 1.444 |  Val. Acc: 45.57% \n",
            "\n",
            "\tTrain Loss: 1.017 | Train Acc: 90.57%\n",
            "\t Val. Loss: 1.437 |  Val. Acc: 46.55% \n",
            "\n",
            "\tTrain Loss: 1.014 | Train Acc: 90.73%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 46.31% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZgzB0ZkHVTI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTkHLEipIlM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da80d345-e226-4d45-fde0-b95d9821d8da"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVjCuKK_LVEF"
      },
      "source": [
        "## Discussion on Data Augmentation Techniques \n",
        "\n",
        "You might wonder exactly how you can augment text data. After all, you can’t really flip it horizontally as you can an image! :D \n",
        "\n",
        "In contrast to data augmentation in images, augmentation techniques on data is very specific to final product you are building. As its general usage on any type of textual data doesn't provides a significant performance boost, that's why unlike torchvision, torchtext doesn’t offer a augmentation pipeline. Due to powerful models as transformers, augmentation tecnhiques are not so preferred now-a-days. But its better to know about some techniques with text that will provide your model with a little more information for training. \n",
        "\n",
        "### Synonym Replacement\n",
        "\n",
        "First, you could replace words in the sentence with synonyms, like so:\n",
        "\n",
        "    The dog slept on the mat\n",
        "\n",
        "could become\n",
        "\n",
        "    The dog slept on the rug\n",
        "\n",
        "Aside from the dog's insistence that a rug is much softer than a mat, the meaning of the sentence hasn’t changed. But mat and rug will be mapped to different indices in the vocabulary, so the model will learn that the two sentences map to the same label, and hopefully that there’s a connection between those two words, as everything else in the sentences is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_uEfWJpL6Nq"
      },
      "source": [
        "### Random Insertion\n",
        "A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alm5D7WIvAC"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLWzwJ3Mm8h"
      },
      "source": [
        "## Random Deletion\n",
        "As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Dz7JJfMqyC"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIbi5WzO5OU"
      },
      "source": [
        "### Random Swap\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599NpwfMR5Vm"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aeKuNCRGip"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2a5FZkoDSyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9269405b-950c-4377-d333-f33ce2d323b8"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/3c/cdeaf9ab0404853e77c45d9e8021d0d2c01f70a1bb26e460090926fe2a5e/hstspreload-2020.11.21-py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.11.8)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
            "\u001b[?25hCollecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: googletrans, contextvars\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp36-none-any.whl size=16369 sha256=8a754f7952c21a6c71e0c349e65883dc83f6a1241d99d6d48ef486f89c5cfc01\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=78084eef51a489f26515c4b31f82e531404cb1a872edf3691bb11e868b213298\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built googletrans contextvars\n",
            "Installing collected packages: hpack, hyperframe, h2, immutables, contextvars, sniffio, h11, httpcore, hstspreload, rfc3986, httpx, googletrans\n",
            "Successfully installed contextvars-2.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.11.21 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.14 rfc3986-1.4.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b97e6f2-836d-4590-ced6-85c7ddc435a1"
      },
      "source": [
        "import random\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "# import googletrans.Translator\n",
        "\n",
        "translator = Translator()\n",
        "sentence = ['The dog slept on the rug']\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "t_text = [t.text for t in translations]\n",
        "print(t_text)\n",
        "\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "print(en_text)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to malay\n",
            "['Anjing itu tidur di atas permaidani']\n",
            "['The dog slept on the carpet']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}